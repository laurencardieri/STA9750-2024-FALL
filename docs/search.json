[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Website",
    "section": "",
    "text": "My name is Lauren and I am 22 years old.\nI am currently a student in Baruch’s MS Business Analytics Program.\n\nMy Dogs!\nI have two dogs! Their names are:\n\nMacaroni\n\nMacaroni (Mack) is technically my brother’s dog, but we babysit him while he’s at work.\nHe is a Golden Retreiver and is 2 years old!\n\n\nRusty\n\nRusty is extremely loving and the smartest dog I have ever met.\nHe is a Pitbull/Lab mix and is 13 years old (we think, he’s a rescue)!"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini Project 01: Fiscal Characterisitics of Major US Public Transit Systems",
    "section": "",
    "text": "This project was taken as inspiration from a YouTuber City Nerd, who created a video about the “10 Transit Services That Do Huge Numbers at the Farebox”. Farebox Recovery is the fraction of revenues raised from fares instead of taxes.\nFor this project, we will use data from the National Transit Database as our primary source. The following tables/reports will be used:\n\nThe 2022 Fare Revenue table, in which we will mostly explore revenue.\nThe lastest Monthly Ridership tables, in which we will mostly explore the Unlinked Passenger Trips (UPT) and Vehicle Revenue Miles (VRM).\nThe 2022 Operating Expenses reports, in which we will mostly explore expenses.\n\nWe will use the 2022 verson of all reports, as up-to-date data and newer reports are often uploaded on a lag.\n\n\n\nFirst, we must download, clean, and join the tables.\nUnfortunately, the code that allowed the immediate download of the data sets did not work for me. So instead, I had to download the data and import it into R, which is seen in my code below.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readxl)\nlibrary(readr)\n\nFARES &lt;- read_excel(\"Miniproject001/2022 Fare Revenue (1).xlsx\") |&gt;\n  select(-`State/Parent NTD ID`, \n         -`Reporter Type`,\n         -`Reporting Module`,\n         -`TOS`,\n         -`Passenger Paid Fares`,\n         -`Organization Paid Fares`) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`) |&gt;\n  group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n           `Agency Name`,  # These are direct operated and sub-contracted \n           `Mode`) |&gt;      # of the same transit modality\n  # Not a big effect in most munis (significant DO\n  # tends to get rid of sub-contractors), but we'll sum\n  # to unify different passenger experiences\n  summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency Name'. You can override\nusing the `.groups` argument.\n\nEXPENSES &lt;- read_csv(\"Miniproject001/2022_NTD_Annual_Data_-_Operating_Expenses__by_Function__20231102.csv\") |&gt;\n  select(`NTD ID`, \n         `Agency`,\n         `Total`, \n         `Mode`) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()\n\nRows: 3744 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (9): Agency, City, State, Organization Type, Reporter Type, UZA Name, M...\ndbl (13): NTD ID, Report Year, UACE Code, Primary UZA Population, Agency VOM...\nlgl  (7): Vehicle Operations Questionable, Vehicle Maintenance Questionable,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'NTD ID'. You can override using the `.groups` argument.\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\nNow, we must extract the monthly transit numbers. My computer struggled to read the whole file for ridership, so I split it into two respective files, one for UPT and one for VRM. This is seen in my code below.\n\nTRIPS &lt;- read_excel(\"Miniproject001/Ridership_UPT.xlsx\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"UPT\") |&gt;\n  drop_na() |&gt;\n  mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\n\nMILES &lt;- read_excel(\"Miniproject001/Ridership_VRN.xlsx\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"VRM\") |&gt;\n  drop_na() |&gt;\n  group_by(`NTD ID`, `Agency`, `UZA Name`, \n           `Mode`, `3 Mode`, month) |&gt;\n  summarize(VRM = sum(VRM)) |&gt;\n  ungroup() |&gt;\n  mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'UZA Name', 'Mode', '3\nMode'. You can override using the `.groups` argument.\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`))\n\nJoining with `by = join_by(`NTD ID`, Agency, `UZA Name`, Mode, `3 Mode`,\nmonth)`\n\n\nNow, this creates the table as follows:\n\nif(!require(\"DT\")) install.packages(\"DT\")\n\nLoading required package: DT\n\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()\n\n\n\n\n\n\n\n\nHere, I decided to rename three of the columns in my table, one to remove spaces so it is easier to manipulate in code, and the other to have more common names for easier understanding. Thus, I wanted:\n\n“UZA Name” to become “Metro_Area”\n“UPT” to become “Unlinked_Passenger_Trips”\n“VRM” to become “Vehicle Revenue Miles”\n\nSo, I did this with the following code:\n\ncolnames(USAGE)[colnames(USAGE) == \"UZA Name\"] &lt;- \"Metro_Area\"\ncolnames(USAGE)[colnames(USAGE) == \"UPT\"] &lt;- \"Unlinked_Passenger_Trips\"\ncolnames(USAGE)[colnames(USAGE) == \"VRM\"] &lt;- \"Vehicle_Revenue_Miles\"\n\nAnd then I reloaded my table to see if the columns changed/looked good. They did!\n\nlibrary(DT)\n\nsample_n(USAGE, 1) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()\n\n\n\n\n\n\n\n\nFirst, I needed to know what the unique codes were used in the Mode column in our data set. To do this, I ran the following command, which produced the following 18 codes:\n\ndistinct(USAGE, Mode)\n\n# A tibble: 18 × 1\n   Mode \n   &lt;chr&gt;\n 1 DR   \n 2 FB   \n 3 MB   \n 4 SR   \n 5 TB   \n 6 VP   \n 7 CB   \n 8 RB   \n 9 LR   \n10 YR   \n11 MG   \n12 CR   \n13 AR   \n14 TR   \n15 HR   \n16 IP   \n17 PB   \n18 CC   \n\n\nUsing the Glossary on the National Transit Database website, I forced search each of these codes to find their corresponding name. Then, I recoded my table to display these names.\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode=case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\", \n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))\n\nThus, my cleaned-up table became:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()\n\n\n\n\n\n\n\n\n\nWhat transit agency had the most total VRM in our data set?\n\n\ntotal_vrm_agency &lt;- USAGE |&gt; \n  group_by(`Agency`) |&gt; \n  summarize(VRM = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) |&gt;\n  arrange(desc(VRM))\n  tvrm &lt;- total_vrm_agency |&gt;\n    slice_max(VRM, n = 1)\n  print(tvrm)\n\n# A tibble: 1 × 2\n  Agency                            VRM\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 MTA New York City Transit 10832855350\n\n\nThus, the transit agency that had the most total VRM in our data set was MTA New York City Transit, with a Vehicle Revenue Miles of 10832855350. This makes sense, as public transit easily spans the entirety of New York City, whether it be the subway, buses, the ferry, or more. Since public transit is prevalent in our lives as New Yorkers, with many not even owning their own vehicle and solely relying on public transit, it is understandable that NYC is top of the list.\n\nWhat transit mode had the most total VRM in our data set?\n\n\n  total_vrm_mode &lt;- USAGE |&gt; \n    group_by(`Mode`) |&gt; \n    summarize(M = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) |&gt;\n    arrange(desc(M))\n    topmode &lt;- total_vrm_mode |&gt;\n      slice_max(M, n = 1)\n    print(topmode)\n\n# A tibble: 1 × 2\n  Mode            M\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Bus   49444494088\n\n\nThe transit mode that had the most total VRM in our data set was the BUS, with a Vehicle Revenue Miles of 49444494088.\n\nHow many trips were taken on the NYC subway (Heavy Rail) in May 2024?\n\n\n    May24Subway &lt;- USAGE |&gt;\n      filter(Mode == \"Heavy Rail\", month &gt;= \"2024-05-01\", month &lt;= \"2024-05-31\") |&gt;\n      summarize(may_sub = sum(Unlinked_Passenger_Trips, na.rm = TRUE))\n    \n    print(May24Subway)\n\n# A tibble: 1 × 1\n    may_sub\n      &lt;dbl&gt;\n1 237383777\n\n\nSo, in May 2024, 237383777 unlinked passenger trips were taken on the NYC subway (Heavy Rail). Again, I believe that this checks out, especially with the way many New Yorkers rely on the subway to get around Manhatten during the work day.\n\nHow much did NYC subway ridership fall between April 2019 and April 2020?\n\n\n  april_19_to_20 &lt;- USAGE |&gt;\n      filter(Mode == \"Heavy Rail\", \n             (month &gt;= \"2019-04-01\" & month &lt;= \"2019-04-30\") |\n             (month &gt;= \"2020-04-01\" & month &lt;= \"2020-04-30\"))|&gt;\n      group_by(month) |&gt;\n      summarize(aprupt = sum(Unlinked_Passenger_Trips, na.rm = TRUE)) |&gt;\n      summarize(april_difference = \n            sum(aprupt[month &gt;= \"2020-04-01\" & month &lt;= \"2020-04-30\"]) - \n            sum(aprupt[month &gt;= \"2019-04-01\" & month &lt;= \"2019-04-30\"]))\n    \n    print(april_19_to_20)\n\n# A tibble: 1 × 1\n  april_difference\n             &lt;dbl&gt;\n1       -296416858\n\n\nHere, we can see that 296416858 less riders used the NYC Subway in April 2020 than April 2019. This makes a lot of sense, as April 2020 was the true start of lock down we faced because of the Covid-19 Pandemic. Whether riders were terrified to leave their homes in fear of contracting the virus or they were listening to the mandate to stay inside unless it is for an emergency, it is not a shock that the MTA had almost 300 million less riders in comparison.\nTo further prove this point, let’s consider the same comparison, but for May 2019 and May 2020.\n\n  may_19_to_20 &lt;- USAGE |&gt;\n      filter(Mode == \"Heavy Rail\", \n             (month &gt;= \"2019-05-01\" & month &lt;= \"2019-05-31\") |\n             (month &gt;= \"2020-05-01\" & month &lt;= \"2020-05-31\"))|&gt;\n      group_by(month) |&gt;\n      summarize(mayupt = sum(Unlinked_Passenger_Trips, na.rm = TRUE)) |&gt;\n      summarize(may_difference = \n            sum(mayupt[month &gt;= \"2020-05-01\" & month &lt;= \"2020-05-31\"]) - \n            sum(mayupt[month &gt;= \"2019-05-01\" & month &lt;= \"2019-05-31\"]))\n    \n    print(may_19_to_20)\n\n# A tibble: 1 × 1\n  may_difference\n           &lt;dbl&gt;\n1     -295050652\n\n\nThe month of May also experienced close to 300 million less riders in May 2020 than May 2019. I am sure if we were to investigate the following months as well, we would come to a similar conclusion. It is truly interesting to see the major impact that the pandemic caused in such a short period of time!\n\n\n\n\nWhat Metro Area has the most total amount of Unlinked Passenger Trips? Do they also have the most total Vehicle Revenue Miles?\n\n\n    Total_UPT &lt;- USAGE |&gt; \n    group_by(`Metro_Area`) |&gt; \n    summarize(most_trips = sum(`Unlinked_Passenger_Trips`, na.rm = TRUE)) |&gt;\n    arrange(desc(most_trips))\n  topupt &lt;- Total_UPT |&gt;\n    slice_max(most_trips, n = 1)\nprint(topupt)\n\n# A tibble: 1 × 2\n  Metro_Area                             most_trips\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 New York--Jersey City--Newark, NY--NJ 84020935224\n\nTotal_VRM_MA &lt;- USAGE |&gt; \n  group_by(`Metro_Area`) |&gt; \n  summarize(most_miles = sum(`Vehicle_Revenue_Miles`, na.rm = TRUE)) |&gt;\n  arrange(desc(most_miles))\ntop_vrm_ma &lt;- Total_VRM_MA |&gt;\n  slice_max(most_miles, n = 1)\nprint(top_vrm_ma)\n\n# A tibble: 1 × 2\n  Metro_Area                             most_miles\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 New York--Jersey City--Newark, NY--NJ 21190345637\n\n\nThe Metro Area that had the most total amount of Unlinked Passenger Trips was New York–Jersey City–Newark, NY–NJ, with 84020935224 trips. This metro area also has the most total Vehicle Revenue Miles, with 21190345637 miles. It is interesting to see that this metro area is dominant on both lists, especially with the NYC MTA also leading other lists prior.\n\nWhat Date had the most total amount of Vehicle Revenue Miles? What date had the least?\n\n\n  Total_date_T &lt;- USAGE |&gt; \n  group_by(`month`) |&gt; \n  summarize(most_date = sum(`Vehicle_Revenue_Miles`, na.rm = TRUE)) |&gt;\n  arrange(desc(most_date))\ntopdate &lt;- Total_date_T |&gt;\n  slice_max(most_date, n = 1)\nprint(topdate)\n\n# A tibble: 1 × 2\n  month      most_date\n  &lt;date&gt;         &lt;dbl&gt;\n1 2019-10-01 449683378\n\nTotal_date_L &lt;- USAGE |&gt; \n  group_by(`month`) |&gt; \n  summarize(least_date = sum(`Vehicle_Revenue_Miles`, na.rm = TRUE)) |&gt;\n  arrange(desc(least_date))\nlowdate &lt;- Total_date_L |&gt;\n  slice_min(least_date, n = 1)\nprint(lowdate)\n\n# A tibble: 1 × 2\n  month      least_date\n  &lt;date&gt;          &lt;dbl&gt;\n1 2020-04-01  255564356\n\n\nThe date with the most total Vehicle Revenue Miles is October 1, 2019. The date with the least is April 1, 2020. Though I am unsure why October 1, 2019 was a popular date to travel, as there was no holidays or world events happen that day, it makes sense why April 1, 2020 was the lowest, due to COVID.\n\nWhich Metro Area contributed the most to the busiest day of the year in regard to Unlinked Passenger Trips? ? Which contributed the least?\n\n\n Transport_Metro &lt;- USAGE |&gt;\n  filter(month == \"2019-10-1\") |&gt;\n  group_by(Metro_Area) |&gt;\n  summarize(most_metro = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) \n  topmetro &lt;- Transport_Metro |&gt; \n  slice_max(most_metro, n=1)\nprint(topmetro)\n\n# A tibble: 1 × 2\n  Metro_Area                            most_metro\n  &lt;chr&gt;                                      &lt;dbl&gt;\n1 New York--Jersey City--Newark, NY--NJ   87400676\n\ntopmetro &lt;- Transport_Metro |&gt; \n  slice_min(most_metro, n=1)\nprint(topmetro) \n\n# A tibble: 1 × 2\n  Metro_Area  most_metro\n  &lt;chr&gt;            &lt;dbl&gt;\n1 Decatur, AL      25944\n\n\nOn the busiest day of the year for Vehicle Revenue Miles, New York–Jersey City–Newark, NY–NJ contributed the most with 87400676 miles. However, Decatur, AL contributed the least, with only 25944 miles. The New York–Jersey City–Newark, NY–NJ area providing the most to this list is not shocking, as it is also the leader on the list for total VRM.\n\n\n\nNow, we will create a new table from USAGE that also has annual total UPT and VRM for 2022.\n\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == 2022) |&gt;\n  group_by(`NTD ID`, Agency, Metro_Area, Mode, Unlinked_Passenger_Trips, Vehicle_Revenue_Miles) |&gt;\n  summarize(UPT = sum(Unlinked_Passenger_Trips, na.rm = TRUE),\n           VRM = sum(Vehicle_Revenue_Miles, na.rm = TRUE)\n  ) |&gt; \n  ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'Metro_Area', 'Mode',\n'Unlinked_Passenger_Trips'. You can override using the `.groups` argument.\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode=case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\", \n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n                                  FINANCIALS,\n                                  join_by(`NTD ID`, Mode)) |&gt;\n  drop_na()\n\nWhen I attempted to run the given code the first time, I discovered that the tables were not joining together with all of their values. I quickly realized that this was because the Modes in the Financial table was still known as their short names. For example, the “Heavy Rail” was still referred to as “HR”. Thus, I ran the same code that we did on the USAGE table earlier to manipulate these codes to reflect their proper names. After this, the table formed with no issue.\n\nsample_n(USAGE_AND_FINANCIALS, 1000) |&gt; \n    DT::datatable()\n\n\n\n\n\n\n\n\n\nWhich transit system (agency and mode) had the most UPT in 2022?\n\n\nMTSUPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(mosttsupt = sum(UPT, na.rm = TRUE)) |&gt;\n  arrange(desc(mosttsupt))\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(MTSUPT)\n\n# A tibble: 1,129 × 3\n# Groups:   Agency [525]\n   Agency                                                   Mode       mosttsupt\n   &lt;chr&gt;                                                    &lt;chr&gt;          &lt;dbl&gt;\n 1 MTA New York City Transit                                Heavy Rail    1.79e9\n 2 MTA New York City Transit                                Bus           4.59e8\n 3 Los Angeles County Metropolitan Transportation Authority Bus           1.94e8\n 4 Chicago Transit Authority                                Bus           1.40e8\n 5 New Jersey Transit Corporation                           Bus           1.13e8\n 6 Chicago Transit Authority                                Heavy Rail    1.04e8\n 7 MTA Bus Company                                          Bus           1.00e8\n 8 Washington Metropolitan Area Transit Authority           Heavy Rail    9.84e7\n 9 Southeastern Pennsylvania Transportation Authority       Bus           9.66e7\n10 Washington Metropolitan Area Transit Authority           Bus           8.99e7\n# ℹ 1,119 more rows\n\n\nThe transit system, agency and mode, that had the most UPT in 2022 was the MTA New York City Transit with their Heavy Rail, with a total UPT of 1793073801. This number is way larger than the following on the list, which interestly enough also belongs to the MTA New York City transit and their Bus system.\n\nWhich transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\n\n\ncolnames(USAGE_AND_FINANCIALS)[colnames(USAGE_AND_FINANCIALS) == \"Total Fares\"] &lt;- \"Total_Fares\"\n\nhighest_farebox &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter( UPT &gt;= 400000) |&gt;\n  mutate(farebox_rate = Total_Fares/Expenses) |&gt; \n  group_by(Agency, Mode) |&gt;\n  summarize(highest_ratio = max (farebox_rate)) |&gt;\n  arrange(desc(highest_ratio))\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(highest_farebox)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                    Mode  highest_ratio\n   &lt;chr&gt;                                                     &lt;chr&gt;         &lt;dbl&gt;\n 1 Anaheim Transportation Network                            Bus           0.865\n 2 City of Gainesville, FL                                   Bus           0.548\n 3 MTA New York City Transit                                 Heav…         0.435\n 4 Massachusetts Bay Transportation Authority                Heav…         0.375\n 5 Woods Hole, Martha's Vineyard and Nantucket Steamship Au… Ferr…         0.335\n 6 Metro-North Commuter Railroad Company, dba: MTA Metro-No… Comm…         0.331\n 7 Centre Area Transportation Authority                      Bus           0.324\n 8 MTA Long Island Rail Road                                 Comm…         0.286\n 9 Southeastern Pennsylvania Transportation Authority        Heav…         0.253\n10 Regional Transportation Commission of Southern Nevada     Bus           0.252\n# ℹ 136 more rows\n\n\nHere, we can clearly see that the transit system with the highest farebox recovery is Anaheim Transportation Network with their bus transportation. The fairbox recovery for each transit system is found by dividing their Total Fares by their Expenses. Interestingly enough, Anaheim Transportation Network and their buses have a ratio of .865, which is about a 57% greater than the second place spot, which is the City of Gainesville, FL with their buses!\n\nWhich transit system (agency and mode) has the lowest expenses per UPT?\n\n\nlow_expense_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter( UPT &gt;= 400000) |&gt;\n  mutate(exp_p_upt = Expenses/UPT) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(lowest_exp_p_upt = min(exp_p_upt)) |&gt;\n  arrange(lowest_exp_p_upt)\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(low_expense_UPT)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                 Mode  lowest_exp_p_upt\n   &lt;chr&gt;                                                  &lt;chr&gt;            &lt;dbl&gt;\n 1 Anaheim Transportation Network                         Bus               12.8\n 2 University of Georgia                                  Bus               14.9\n 3 University of Michigan Parking and Transportation Ser… Bus               16.2\n 4 Town of Blacksburg                                     Bus               17.5\n 5 Ames Transit Agency                                    Bus               21.5\n 6 Centre Area Transportation Authority                   Bus               23.5\n 7 MTA New York City Transit                              Heav…             31.1\n 8 Greater Lafayette Public Transportation Corporation    Bus               31.5\n 9 San Diego Metropolitan Transit System                  Ligh…             31.6\n10 Champaign-Urbana Mass Transit District                 Bus               39.8\n# ℹ 136 more rows\n\n\nSimilar to the last inquiry , the transit system that has the lowest expenses per UPT is Anaheim Transportation Network with their bus transportation. Their ratio of expenses per UPT is 12.8, which is drastically less than the highest. The highest expense per UPT is Northeast Illinois Regional Commuter Railroad Corporation and their Cable Cars, with a ratio of 600, which is an extreme difference between the two.\n\nWhich transit system (agency and mode) has the highest total fares per UPT?\n\n\nhigh_fare_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000) |&gt;\n  mutate(totalfares_p_upt = (Total_Fares/UPT)) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(high_fare_p_upt = max(totalfares_p_upt)) |&gt;\n  arrange(desc(high_fare_p_upt))\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(high_fare_UPT)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                  Mode  high_fare_p_upt\n   &lt;chr&gt;                                                   &lt;chr&gt;           &lt;dbl&gt;\n 1 New Jersey Transit Corporation                          Bus             536. \n 2 Northeast Illinois Regional Commuter Railroad Corporat… Comm…           265. \n 3 Metro-North Commuter Railroad Company, dba: MTA Metro-… Comm…           161. \n 4 New Jersey Transit Corporation                          Comm…           119. \n 5 MTA New York City Transit                               Comm…           112. \n 6 Massachusetts Bay Transportation Authority              Bus             107. \n 7 Massachusetts Bay Transportation Authority              Comm…            84.4\n 8 Woods Hole, Martha's Vineyard and Nantucket Steamship … Ferr…            78.0\n 9 MTA Long Island Rail Road                               Comm…            73.9\n10 Peninsula Corridor Joint Powers Board                   Comm…            67.5\n# ℹ 136 more rows\n\n\nThe transit system with the highest total fares per UPT is New Jersey Transit Corporation and their buses, with a total fare of 536 to one Unlinked Passenger Trip. This is more than double the second place spot, which goes to Northeast Illinois Regional Commuter Railroad Corporation and their Commuter Bus, with a total fare of 265 to one Unlinked Passenger Trip.\n\nWhich transit system (agency and mode) has the lowest expenses per VRM?\n\nInterestingly enough, I had to change my “Expenses” column name to “expense” for my code to recognize the column instead of the table we established prior, hence the reasoning for that code below.\n\ncolnames(USAGE_AND_FINANCIALS)[colnames(USAGE_AND_FINANCIALS) == \"Expenses\"] &lt;- \"expense\"\n\nlow_exp_vrm&lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000) |&gt;\n  mutate(exp_p_vrm = (expense/VRM)) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(low_exp_p_vrm = min(exp_p_vrm)) |&gt; \n  arrange(low_exp_p_vrm) \n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(low_exp_vrm)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                              Mode       low_exp_p_vrm\n   &lt;chr&gt;                                               &lt;chr&gt;              &lt;dbl&gt;\n 1 Interurban Transit Partnership                      Bus                 77.2\n 2 City of El Paso                                     Bus                 85.8\n 3 Des Moines Area Regional Transit Authority          Bus                 86.3\n 4 Central Florida Regional Transportation Authority   Bus                 87.5\n 5 San Francisco Bay Area Rapid Transit District       Heavy Rail          88.8\n 6 Transportation District Commission of Hampton Roads Bus                 88.9\n 7 City of Gainesville, FL                             Bus                 90.1\n 8 Greater Lafayette Public Transportation Corporation Bus                 90.5\n 9 Ames Transit Agency                                 Bus                 92.0\n10 Delaware Transit Corporation                        Bus                 92.4\n# ℹ 136 more rows\n\n\nThe transit system with the lowest expenses per Vehicle Revenue Miles was Interurban Transit Partnership and their Bus transportation, with an expense of 77.2 per Vehicle Revenue Mile. This is far cheaper than the leader of this category, which is the New York City Department of Transportation and their Ferryboat, with an expense of 771 per vehicle revenue mile. Interestingly enough, out of the ten highest expenses on the list, nine of the most expensive Modes of transportation is the Ferryboat, with only the Cable Car in San Francisco coming in at fourth. This shows that a Ferryboat is most-likely the most expensive transportation mode for transit agencies to run in comparison to its vehicle revenue miles.\n\nWhich transit system (agency and mode) has the highest total fares per VRM?\n\n\nhigh_fare_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000) |&gt;\n  mutate(totalfares_p_vrm = (Total_Fares/VRM)) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(high_fare_p_vrm = max(totalfares_p_vrm)) |&gt;\n  arrange(desc(high_fare_p_vrm))\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(high_fare_VRM)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                  Mode  high_fare_p_vrm\n   &lt;chr&gt;                                                   &lt;chr&gt;           &lt;dbl&gt;\n 1 Washington State Ferries                                Ferr…          1120. \n 2 Woods Hole, Martha's Vineyard and Nantucket Steamship … Ferr…           829. \n 3 New York City Economic Development Corporation          Ferr…           188. \n 4 Anaheim Transportation Network                          Bus             170. \n 5 Massachusetts Bay Transportation Authority              Ligh…           131. \n 6 Port Authority Trans-Hudson Corporation                 Heav…           119. \n 7 Massachusetts Bay Transportation Authority              Heav…           101. \n 8 MTA Long Island Rail Road                               Comm…            98.6\n 9 Metro-North Commuter Railroad Company, dba: MTA Metro-… Comm…            94.4\n10 MTA New York City Transit                               Heav…            91.5\n# ℹ 136 more rows\n\n\nThe transit system with the highest total fares per vehicle revenue miles is Washington State Ferries, with a ratio of 1120 total fares to one vehicle revenue mile. An interesting correlation to the last inquiry is that the top three highest total fares per VRM belong to ferryboats. Furthermore, the highest value on the list is almost 500% greater than the third value, which is 188, and that percentage only grows as that list goes further down.\n\n\n\nIn regard to what transit system in the country is the most effective, it depends on which data points you use to weigh your opinion. If you are considering which system has the highest values in URM and UPT, meaning it is the most frequently used, the answer would be the major transit system of NYC’s MTA Transit. Not only is its heavy rail system extremely prevalent in these numbers, which makes sense since numerous rails span the lengths of New York City and are used daily by many, but the NYC MTA Bus also holds rank compared to others.\nHowever, if you are considering which transit system is the most cost effective, it would have to be the Anaheim Transportation Network. Since a high farebox recovery ratio indicates that a transit system is profitable, Anaheim Transportation Network bus’ numbers alone show that this system is highly more profitable than the rest. This agency also leads margin of lowest expenses per unlinked passenger trip, which shows that not only is this system great at making money, it also great at retaining it and not spending the majority of it on expenses. However, while this is great from a business standpoint, as a passenger this transportation agency ranks high on the total fares per vehicle revenue mile scale, as it is in 4th place. This means that this transportation is fairly expensive to ride compared to the others.\nIn my opinion, I believe that the MTA New York City Transit is the most effective in the country, due to the enormous scale it can run on at a moderate price compared to other agencies."
  },
  {
    "objectID": "mp01.html#this-code-needs-to-be-modified",
    "href": "mp01.html#this-code-needs-to-be-modified",
    "title": "STA 9750 2024 Submission Material",
    "section": "This code needs to be modified",
    "text": "This code needs to be modified\nUSAGE &lt;- USAGE |&gt; mutate(Mode=case_when( Mode == “HR” ~ “Heavy Rail”, Mode == “DR” ~ “Demand Response”, Mode == “FB” ~ “Ferryboat”, Mode == “MB” ~ “Bus”, Mode == “SR” ~ “Streetcar Rail”, Mode == “TB” ~ “Trolleybus”, Mode == “VP” ~ “Vanpool”, Mode == “CB” ~ “Commuter Bus”, Mode == “RB” ~ “Bus Rapid Transit”, Mode == “LR” ~ “Light Rail”, Mode == “YR” ~ “Hybrid Rail”, Mode == “MG” ~ “Monorail and Automated Guideway”, Mode == “CR” ~ “Commuter Rail”, Mode == “AR” ~ “Alaska Railroad”, Mode == “TR” ~ “Aerial Tramways”, Mode == “IP” ~ “Inclined Plane”, Mode == “PB” ~ “Publico”, Mode == “CC” ~ “Cable Car”, TRUE ~ “Unknown”))\n##changing column names (optional) colnames(USAGE)[colnames(USAGE) == “UPT”] &lt;- “Unlinked_Passenger_Trips” colnames(USAGE)[colnames(USAGE) == “VRM”] &lt;- “Vehicle_Revenue_Miles”\nsample_n(USAGE, 1000) |&gt; mutate(month=as.character(month)) |&gt; DT::datatable()\n#Task 3"
  },
  {
    "objectID": "mp01.html#what-transit-agency-had-the-most-total-vrm-in-our-data-set",
    "href": "mp01.html#what-transit-agency-had-the-most-total-vrm-in-our-data-set",
    "title": "STA 9750 2024 Submission Material",
    "section": "1. What transit agency had the most total VRM in our data set?",
    "text": "1. What transit agency had the most total VRM in our data set?\ntotal_vrm_agency &lt;- USAGE |&gt; group_by(Agency) |&gt; summarize(VRM = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) |&gt; arrange(desc(VRM)) view(total_vrm_agency)\n###The transit agency that had the most total VRM in our data set was MTA New York City Transit, with a Vehicle Revenue Miles of 10832855350."
  },
  {
    "objectID": "mp01.html#what-transit-mode-had-the-most-total-vrm-in-our-data-set",
    "href": "mp01.html#what-transit-mode-had-the-most-total-vrm-in-our-data-set",
    "title": "STA 9750 2024 Submission Material",
    "section": "2. What transit mode had the most total VRM in our data set?",
    "text": "2. What transit mode had the most total VRM in our data set?\ntotal_vrm_mode &lt;- USAGE |&gt; group_by(Mode) |&gt; summarize(M = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) |&gt; arrange(desc(M)) topmode &lt;- total_vrm_mode |&gt; slice_max(M, n = 1) print(topmode)\n\nThe tranist mode that had the most total VRM in our data set was the BUS, with a Vehicle Revenue Miles of 49444494088."
  },
  {
    "objectID": "mp01.html#how-many-trips-were-taken-on-the-nyc-subway-heavy-rail-in-may-2024",
    "href": "mp01.html#how-many-trips-were-taken-on-the-nyc-subway-heavy-rail-in-may-2024",
    "title": "STA 9750 2024 Submission Material",
    "section": "3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?",
    "text": "3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\nNYC_Subway &lt;- USAGE |&gt; month &lt;- as.Date(month)\nfilter(Mode == “Heavy Rail”, month == (“2024-05-01”) & (“2024-05-31”))\nfilter(month %in% c(2024-05-01,2024-05-31))"
  },
  {
    "objectID": "mp01.html#how-much-did-nyc-subway-ridership-fall-between-april-2019-and-april-2020",
    "href": "mp01.html#how-much-did-nyc-subway-ridership-fall-between-april-2019-and-april-2020",
    "title": "STA 9750 2024 Submission Material",
    "section": "5. How much did NYC subway ridership fall between April 2019 and April 2020?",
    "text": "5. How much did NYC subway ridership fall between April 2019 and April 2020?\napril_19 &lt;- USAGE$month |&gt; filter(month &gt;= as.Date(“2019-04-01”) & month &lt;= as.Date(“2029-04-30”))\nUSAGE\\(month &lt;- as.Date(USAGE\\)month)\napril_19 &lt;- USAGE$month |&gt; filter(Date &gt;= as.Date(“2019-04-01”) & Date &lt;= as.Date(“2029-04-30”))\n#Task 4"
  },
  {
    "objectID": "mp01.html#what-metro-area-has-the-most-total-amount-of-unlinked-passenger-trips-do-they-also-have-the-most-total-vehicle-revenue-miles",
    "href": "mp01.html#what-metro-area-has-the-most-total-amount-of-unlinked-passenger-trips-do-they-also-have-the-most-total-vehicle-revenue-miles",
    "title": "STA 9750 2024 Submission Material",
    "section": "1. What Metro Area has the most total amount of Unlinked Passenger Trips? Do they also have the most total Vehicle Revenue Miles?",
    "text": "1. What Metro Area has the most total amount of Unlinked Passenger Trips? Do they also have the most total Vehicle Revenue Miles?\nTotal_UPT &lt;- USAGE |&gt; group_by(Metro_Area) |&gt; summarize(most_trips = sum(Unlinked_Passenger_Trips, na.rm = TRUE)) |&gt; arrange(desc(most_trips)) topupt &lt;- Total_UPT |&gt; slice_max(most_trips, n = 1) print(topupt)\nTotal_VRM_MA &lt;- USAGE |&gt; group_by(Metro_Area) |&gt; summarize(most_miles = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) |&gt; arrange(desc(most_miles)) top_vrm_ma &lt;- Total_VRM_MA |&gt; slice_max(most_miles, n = 1) print(top_vrm_ma)\n\nThe Metro Area that had the most total amount of Unlinked Passenger Trips was New York–Jersey City–Newwark, NY–NJ, with 84020935224 trips.\n\n\nThis metro area also has the most total Vehicle Revenue Miles, with 21190345637 miles."
  },
  {
    "objectID": "mp01.html#what-date-had-the-most-total-amount-of-vehicle-revenue-miles-what-date-has-the-least",
    "href": "mp01.html#what-date-had-the-most-total-amount-of-vehicle-revenue-miles-what-date-has-the-least",
    "title": "STA 9750 2024 Submission Material",
    "section": "2. What Date had the most total amount of Vehicle Revenue Miles? What date has the least?",
    "text": "2. What Date had the most total amount of Vehicle Revenue Miles? What date has the least?\nTotal_date_T &lt;- USAGE |&gt; group_by(month) |&gt; summarize(most_date = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) |&gt; arrange(desc(most_date)) topdate &lt;- Total_date_T |&gt; slice_max(most_date, n = 1) print(topdate)\nTotal_date_L &lt;- USAGE |&gt; group_by(month) |&gt; summarize(least_date = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) |&gt; arrange(desc(least_date)) lowdate &lt;- Total_date_L |&gt; slice_min(least_date, n = 1) print(lowdate)\n\nThe date with the most total Vehicle Revenue Miles is October 1, 2019. The date with the least is April 1, 2020."
  },
  {
    "objectID": "mp01.html#which-metro-area-contributed-the-most-to-the-busiest-day-of-the-year-which-contributed-the-least",
    "href": "mp01.html#which-metro-area-contributed-the-most-to-the-busiest-day-of-the-year-which-contributed-the-least",
    "title": "STA 9750 2024 Submission Material",
    "section": "3. Which Metro Area contributed the most to the busiest day of the year? Which contributed the least?",
    "text": "3. Which Metro Area contributed the most to the busiest day of the year? Which contributed the least?\nTransport_Metro &lt;- USAGE |&gt; filter(month == “2019-10-1”) |&gt; group_by(Metro_Area) |&gt; summarize(most_metro = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) topmetro &lt;- Transport_Metro |&gt; slice_max(most_metro, n=1) print(topmetro)\ntopmetro &lt;- Transport_Metro |&gt; slice_min(most_metro, n=1) print(topmetro)\n\nOn the busiest day of the year for Vehicle Revenue Miles, New York–Jersey City–Newark, NY–NJ contributed the most with 87400676 miles.\n\n\nHowever, Decatur, AL contributed the least, with only 25944 miles.\n#Task 5"
  },
  {
    "objectID": "mp01.html#introduction",
    "href": "mp01.html#introduction",
    "title": "Mini Project 01: Fiscal Characterisitics of Major US Public Transit Systems",
    "section": "",
    "text": "This project was taken as inspiration from a YouTuber City Nerd, who created a video about the “10 Transit Services That Do Huge Numbers at the Farebox”. Farebox Recovery is the fraction of revenues raised from fares instead of taxes.\nFor this project, we will use data from the National Transit Database as our primary source. The following tables/reports will be used:\n\nThe 2022 Fare Revenue table, in which we will mostly explore revenue.\nThe lastest Monthly Ridership tables, in which we will mostly explore the Unlinked Passenger Trips (UPT) and Vehicle Revenue Miles (VRM).\nThe 2022 Operating Expenses reports, in which we will mostly explore expenses.\n\nWe will use the 2022 verson of all reports, as up-to-date data and newer reports are often uploaded on a lag."
  },
  {
    "objectID": "index.html#my-dogs",
    "href": "index.html#my-dogs",
    "title": "My Website",
    "section": "My Dogs!",
    "text": "My Dogs!\nI have two dogs! Their names are:\n\nMacaroni\n\nMacaroni (Mack) is technically my brother’s dog, but we babysit him while he’s at work.\nHe is a Golden Retreiver and is 2 years old!\n\n\nRusty\n\nRusty is extremely loving and the smartest dog I have ever met.\nHe is a Pitbull/Lab mix and is 13 years old (we think, he’s a rescue)!"
  },
  {
    "objectID": "mp01.html#getting-started",
    "href": "mp01.html#getting-started",
    "title": "Mini Project 01: Fiscal Characterisitics of Major US Public Transit Systems",
    "section": "",
    "text": "First, we must download, clean, and join the tables.\nUnfortunately, the code that allowed the immediate download of the data sets did not work for me. So instead, I had to download the data and import it into R, which is seen in my code below.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readxl)\nlibrary(readr)\n\nFARES &lt;- read_excel(\"Miniproject001/2022 Fare Revenue (1).xlsx\") |&gt;\n  select(-`State/Parent NTD ID`, \n         -`Reporter Type`,\n         -`Reporting Module`,\n         -`TOS`,\n         -`Passenger Paid Fares`,\n         -`Organization Paid Fares`) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`) |&gt;\n  group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n           `Agency Name`,  # These are direct operated and sub-contracted \n           `Mode`) |&gt;      # of the same transit modality\n  # Not a big effect in most munis (significant DO\n  # tends to get rid of sub-contractors), but we'll sum\n  # to unify different passenger experiences\n  summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency Name'. You can override\nusing the `.groups` argument.\n\nEXPENSES &lt;- read_csv(\"Miniproject001/2022_NTD_Annual_Data_-_Operating_Expenses__by_Function__20231102.csv\") |&gt;\n  select(`NTD ID`, \n         `Agency`,\n         `Total`, \n         `Mode`) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()\n\nRows: 3744 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (9): Agency, City, State, Organization Type, Reporter Type, UZA Name, M...\ndbl (13): NTD ID, Report Year, UACE Code, Primary UZA Population, Agency VOM...\nlgl  (7): Vehicle Operations Questionable, Vehicle Maintenance Questionable,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'NTD ID'. You can override using the `.groups` argument.\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\nNow, we must extract the monthly transit numbers. My computer struggled to read the whole file for ridership, so I split it into two respective files, one for UPT and one for VRM. This is seen in my code below.\n\nTRIPS &lt;- read_excel(\"Miniproject001/Ridership_UPT.xlsx\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"UPT\") |&gt;\n  drop_na() |&gt;\n  mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\n\nMILES &lt;- read_excel(\"Miniproject001/Ridership_VRN.xlsx\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"VRM\") |&gt;\n  drop_na() |&gt;\n  group_by(`NTD ID`, `Agency`, `UZA Name`, \n           `Mode`, `3 Mode`, month) |&gt;\n  summarize(VRM = sum(VRM)) |&gt;\n  ungroup() |&gt;\n  mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'UZA Name', 'Mode', '3\nMode'. You can override using the `.groups` argument.\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`))\n\nJoining with `by = join_by(`NTD ID`, Agency, `UZA Name`, Mode, `3 Mode`,\nmonth)`\n\n\nNow, this creates the table as follows:\n\nif(!require(\"DT\")) install.packages(\"DT\")\n\nLoading required package: DT\n\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()"
  },
  {
    "objectID": "mp01.html#task-1--creating-syntatic-names",
    "href": "mp01.html#task-1--creating-syntatic-names",
    "title": "Mini Project 01: Fiscal Characterisitics of Major US Public Transit Systems",
    "section": "",
    "text": "Here, I decided to rename three of the columns in my table, one to remove spaces so it is easier to manipulate in code, and the other to have more common names for easier understanding. Thus, I wanted:\n\n“UZA Name” to become “Metro_Area”\n“UPT” to become “Unlinked_Passenger_Trips”\n“VRM” to become “Vehicle Revenue Miles”\n\nSo, I did this with the following code:\n\ncolnames(USAGE)[colnames(USAGE) == \"UZA Name\"] &lt;- \"Metro_Area\"\ncolnames(USAGE)[colnames(USAGE) == \"UPT\"] &lt;- \"Unlinked_Passenger_Trips\"\ncolnames(USAGE)[colnames(USAGE) == \"VRM\"] &lt;- \"Vehicle_Revenue_Miles\"\n\nAnd then I reloaded my table to see if the columns changed/looked good. They did!\n\nlibrary(DT)\n\nsample_n(USAGE, 1) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()"
  },
  {
    "objectID": "mp01.html#task-1-creating-syntatic-names",
    "href": "mp01.html#task-1-creating-syntatic-names",
    "title": "Mini Project 01: Fiscal Characterisitics of Major US Public Transit Systems",
    "section": "",
    "text": "Here, I decided to rename three of the columns in my table, one to remove spaces so it is easier to manipulate in code, and the other to have more common names for easier understanding. Thus, I wanted:\n\n“UZA Name” to become “Metro_Area”\n“UPT” to become “Unlinked_Passenger_Trips”\n“VRM” to become “Vehicle Revenue Miles”\n\nSo, I did this with the following code:\n\ncolnames(USAGE)[colnames(USAGE) == \"UZA Name\"] &lt;- \"Metro_Area\"\ncolnames(USAGE)[colnames(USAGE) == \"UPT\"] &lt;- \"Unlinked_Passenger_Trips\"\ncolnames(USAGE)[colnames(USAGE) == \"VRM\"] &lt;- \"Vehicle_Revenue_Miles\"\n\nAnd then I reloaded my table to see if the columns changed/looked good. They did!\n\nlibrary(DT)\n\nsample_n(USAGE, 1) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()"
  },
  {
    "objectID": "mp01.html#task-2-recording-the-mode-column",
    "href": "mp01.html#task-2-recording-the-mode-column",
    "title": "Mini Project 01: Fiscal Characterisitics of Major US Public Transit Systems",
    "section": "",
    "text": "First, I needed to know what the unique codes were used in the Mode column in our data set. To do this, I ran the following command, which produced the following 18 codes:\n\ndistinct(USAGE, Mode)\n\n# A tibble: 18 × 1\n   Mode \n   &lt;chr&gt;\n 1 DR   \n 2 FB   \n 3 MB   \n 4 SR   \n 5 TB   \n 6 VP   \n 7 CB   \n 8 RB   \n 9 LR   \n10 YR   \n11 MG   \n12 CR   \n13 AR   \n14 TR   \n15 HR   \n16 IP   \n17 PB   \n18 CC   \n\n\nUsing the Glossary on the National Transit Database website, I forced search each of these codes to find their corresponding name. Then, I recoded my table to display these names.\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode=case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\", \n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))\n\nThus, my cleaned-up table became:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()"
  },
  {
    "objectID": "mp01.html#task-3-answering-the-intructor-specified-questions-with-dplyr.",
    "href": "mp01.html#task-3-answering-the-intructor-specified-questions-with-dplyr.",
    "title": "Mini Project 01: Fiscal Characterisitics of Major US Public Transit Systems",
    "section": "",
    "text": "What transit agency had the most total VRM in our data set?\n\n\ntotal_vrm_agency &lt;- USAGE |&gt; \n  group_by(`Agency`) |&gt; \n  summarize(VRM = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) |&gt;\n  arrange(desc(VRM))\n  tvrm &lt;- total_vrm_agency |&gt;\n    slice_max(VRM, n = 1)\n  print(tvrm)\n\n# A tibble: 1 × 2\n  Agency                            VRM\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 MTA New York City Transit 10832855350\n\n\nThus, the transit agency that had the most total VRM in our data set was MTA New York City Transit, with a Vehicle Revenue Miles of 10832855350. This makes sense, as public transit easily spans the entirety of New York City, whether it be the subway, buses, the ferry, or more. Since public transit is prevalent in our lives as New Yorkers, with many not even owning their own vehicle and solely relying on public transit, it is understandable that NYC is top of the list.\n\nWhat transit mode had the most total VRM in our data set?\n\n\n  total_vrm_mode &lt;- USAGE |&gt; \n    group_by(`Mode`) |&gt; \n    summarize(M = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) |&gt;\n    arrange(desc(M))\n    topmode &lt;- total_vrm_mode |&gt;\n      slice_max(M, n = 1)\n    print(topmode)\n\n# A tibble: 1 × 2\n  Mode            M\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Bus   49444494088\n\n\nThe transit mode that had the most total VRM in our data set was the BUS, with a Vehicle Revenue Miles of 49444494088.\n\nHow many trips were taken on the NYC subway (Heavy Rail) in May 2024?\n\n\n    May24Subway &lt;- USAGE |&gt;\n      filter(Mode == \"Heavy Rail\", month &gt;= \"2024-05-01\", month &lt;= \"2024-05-31\") |&gt;\n      summarize(may_sub = sum(Unlinked_Passenger_Trips, na.rm = TRUE))\n    \n    print(May24Subway)\n\n# A tibble: 1 × 1\n    may_sub\n      &lt;dbl&gt;\n1 237383777\n\n\nSo, in May 2024, 237383777 unlinked passenger trips were taken on the NYC subway (Heavy Rail). Again, I believe that this checks out, especially with the way many New Yorkers rely on the subway to get around Manhatten during the work day.\n\nHow much did NYC subway ridership fall between April 2019 and April 2020?\n\n\n  april_19_to_20 &lt;- USAGE |&gt;\n      filter(Mode == \"Heavy Rail\", \n             (month &gt;= \"2019-04-01\" & month &lt;= \"2019-04-30\") |\n             (month &gt;= \"2020-04-01\" & month &lt;= \"2020-04-30\"))|&gt;\n      group_by(month) |&gt;\n      summarize(aprupt = sum(Unlinked_Passenger_Trips, na.rm = TRUE)) |&gt;\n      summarize(april_difference = \n            sum(aprupt[month &gt;= \"2020-04-01\" & month &lt;= \"2020-04-30\"]) - \n            sum(aprupt[month &gt;= \"2019-04-01\" & month &lt;= \"2019-04-30\"]))\n    \n    print(april_19_to_20)\n\n# A tibble: 1 × 1\n  april_difference\n             &lt;dbl&gt;\n1       -296416858\n\n\nHere, we can see that 296416858 less riders used the NYC Subway in April 2020 than April 2019. This makes a lot of sense, as April 2020 was the true start of lock down we faced because of the Covid-19 Pandemic. Whether riders were terrified to leave their homes in fear of contracting the virus or they were listening to the mandate to stay inside unless it is for an emergency, it is not a shock that the MTA had almost 300 million less riders in comparison.\nTo further prove this point, let’s consider the same comparison, but for May 2019 and May 2020.\n\n  may_19_to_20 &lt;- USAGE |&gt;\n      filter(Mode == \"Heavy Rail\", \n             (month &gt;= \"2019-05-01\" & month &lt;= \"2019-05-31\") |\n             (month &gt;= \"2020-05-01\" & month &lt;= \"2020-05-31\"))|&gt;\n      group_by(month) |&gt;\n      summarize(mayupt = sum(Unlinked_Passenger_Trips, na.rm = TRUE)) |&gt;\n      summarize(may_difference = \n            sum(mayupt[month &gt;= \"2020-05-01\" & month &lt;= \"2020-05-31\"]) - \n            sum(mayupt[month &gt;= \"2019-05-01\" & month &lt;= \"2019-05-31\"]))\n    \n    print(may_19_to_20)\n\n# A tibble: 1 × 1\n  may_difference\n           &lt;dbl&gt;\n1     -295050652\n\n\nThe month of May also experienced close to 300 million less riders in May 2020 than May 2019. I am sure if we were to investigate the following months as well, we would come to a similar conclusion. It is truly interesting to see the major impact that the pandemic caused in such a short period of time!"
  },
  {
    "objectID": "mp01.html#task-4-explore-and-analyze",
    "href": "mp01.html#task-4-explore-and-analyze",
    "title": "Mini Project 01: Fiscal Characterisitics of Major US Public Transit Systems",
    "section": "",
    "text": "What Metro Area has the most total amount of Unlinked Passenger Trips? Do they also have the most total Vehicle Revenue Miles?\n\n\n    Total_UPT &lt;- USAGE |&gt; \n    group_by(`Metro_Area`) |&gt; \n    summarize(most_trips = sum(`Unlinked_Passenger_Trips`, na.rm = TRUE)) |&gt;\n    arrange(desc(most_trips))\n  topupt &lt;- Total_UPT |&gt;\n    slice_max(most_trips, n = 1)\nprint(topupt)\n\n# A tibble: 1 × 2\n  Metro_Area                             most_trips\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 New York--Jersey City--Newark, NY--NJ 84020935224\n\nTotal_VRM_MA &lt;- USAGE |&gt; \n  group_by(`Metro_Area`) |&gt; \n  summarize(most_miles = sum(`Vehicle_Revenue_Miles`, na.rm = TRUE)) |&gt;\n  arrange(desc(most_miles))\ntop_vrm_ma &lt;- Total_VRM_MA |&gt;\n  slice_max(most_miles, n = 1)\nprint(top_vrm_ma)\n\n# A tibble: 1 × 2\n  Metro_Area                             most_miles\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 New York--Jersey City--Newark, NY--NJ 21190345637\n\n\nThe Metro Area that had the most total amount of Unlinked Passenger Trips was New York–Jersey City–Newark, NY–NJ, with 84020935224 trips. This metro area also has the most total Vehicle Revenue Miles, with 21190345637 miles. It is interesting to see that this metro area is dominant on both lists, especially with the NYC MTA also leading other lists prior.\n\nWhat Date had the most total amount of Vehicle Revenue Miles? What date had the least?\n\n\n  Total_date_T &lt;- USAGE |&gt; \n  group_by(`month`) |&gt; \n  summarize(most_date = sum(`Vehicle_Revenue_Miles`, na.rm = TRUE)) |&gt;\n  arrange(desc(most_date))\ntopdate &lt;- Total_date_T |&gt;\n  slice_max(most_date, n = 1)\nprint(topdate)\n\n# A tibble: 1 × 2\n  month      most_date\n  &lt;date&gt;         &lt;dbl&gt;\n1 2019-10-01 449683378\n\nTotal_date_L &lt;- USAGE |&gt; \n  group_by(`month`) |&gt; \n  summarize(least_date = sum(`Vehicle_Revenue_Miles`, na.rm = TRUE)) |&gt;\n  arrange(desc(least_date))\nlowdate &lt;- Total_date_L |&gt;\n  slice_min(least_date, n = 1)\nprint(lowdate)\n\n# A tibble: 1 × 2\n  month      least_date\n  &lt;date&gt;          &lt;dbl&gt;\n1 2020-04-01  255564356\n\n\nThe date with the most total Vehicle Revenue Miles is October 1, 2019. The date with the least is April 1, 2020. Though I am unsure why October 1, 2019 was a popular date to travel, as there was no holidays or world events happen that day, it makes sense why April 1, 2020 was the lowest, due to COVID.\n\nWhich Metro Area contributed the most to the busiest day of the year in regard to Unlinked Passenger Trips? ? Which contributed the least?\n\n\n Transport_Metro &lt;- USAGE |&gt;\n  filter(month == \"2019-10-1\") |&gt;\n  group_by(Metro_Area) |&gt;\n  summarize(most_metro = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) \n  topmetro &lt;- Transport_Metro |&gt; \n  slice_max(most_metro, n=1)\nprint(topmetro)\n\n# A tibble: 1 × 2\n  Metro_Area                            most_metro\n  &lt;chr&gt;                                      &lt;dbl&gt;\n1 New York--Jersey City--Newark, NY--NJ   87400676\n\ntopmetro &lt;- Transport_Metro |&gt; \n  slice_min(most_metro, n=1)\nprint(topmetro) \n\n# A tibble: 1 × 2\n  Metro_Area  most_metro\n  &lt;chr&gt;            &lt;dbl&gt;\n1 Decatur, AL      25944\n\n\nOn the busiest day of the year for Vehicle Revenue Miles, New York–Jersey City–Newark, NY–NJ contributed the most with 87400676 miles. However, Decatur, AL contributed the least, with only 25944 miles. The New York–Jersey City–Newark, NY–NJ area providing the most to this list is not shocking, as it is also the leader on the list for total VRM."
  },
  {
    "objectID": "mp01.html#task-5-table-summarization",
    "href": "mp01.html#task-5-table-summarization",
    "title": "Mini Project 01: Fiscal Characterisitics of Major US Public Transit Systems",
    "section": "",
    "text": "Now, we will create a new table from USAGE that also has annual total UPT and VRM for 2022.\n\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == 2022) |&gt;\n  group_by(`NTD ID`, Agency, Metro_Area, Mode, Unlinked_Passenger_Trips, Vehicle_Revenue_Miles) |&gt;\n  summarize(UPT = sum(Unlinked_Passenger_Trips, na.rm = TRUE),\n           VRM = sum(Vehicle_Revenue_Miles, na.rm = TRUE)\n  ) |&gt; \n  ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'Metro_Area', 'Mode',\n'Unlinked_Passenger_Trips'. You can override using the `.groups` argument.\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode=case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\", \n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n                                  FINANCIALS,\n                                  join_by(`NTD ID`, Mode)) |&gt;\n  drop_na()\n\nWhen I attempted to run the given code the first time, I discovered that the tables were not joining together with all of their values. I quickly realized that this was because the Modes in the Financial table was still known as their short names. For example, the “Heavy Rail” was still referred to as “HR”. Thus, I ran the same code that we did on the USAGE table earlier to manipulate these codes to reflect their proper names. After this, the table formed with no issue.\n\nsample_n(USAGE_AND_FINANCIALS, 1000) |&gt; \n    DT::datatable()"
  },
  {
    "objectID": "mp01.html#task-6-farebox-recovery-among-major-systems",
    "href": "mp01.html#task-6-farebox-recovery-among-major-systems",
    "title": "Mini Project 01: Fiscal Characterisitics of Major US Public Transit Systems",
    "section": "",
    "text": "Which transit system (agency and mode) had the most UPT in 2022?\n\n\nMTSUPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(mosttsupt = sum(UPT, na.rm = TRUE)) |&gt;\n  arrange(desc(mosttsupt))\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(MTSUPT)\n\n# A tibble: 1,129 × 3\n# Groups:   Agency [525]\n   Agency                                                   Mode       mosttsupt\n   &lt;chr&gt;                                                    &lt;chr&gt;          &lt;dbl&gt;\n 1 MTA New York City Transit                                Heavy Rail    1.79e9\n 2 MTA New York City Transit                                Bus           4.59e8\n 3 Los Angeles County Metropolitan Transportation Authority Bus           1.94e8\n 4 Chicago Transit Authority                                Bus           1.40e8\n 5 New Jersey Transit Corporation                           Bus           1.13e8\n 6 Chicago Transit Authority                                Heavy Rail    1.04e8\n 7 MTA Bus Company                                          Bus           1.00e8\n 8 Washington Metropolitan Area Transit Authority           Heavy Rail    9.84e7\n 9 Southeastern Pennsylvania Transportation Authority       Bus           9.66e7\n10 Washington Metropolitan Area Transit Authority           Bus           8.99e7\n# ℹ 1,119 more rows\n\n\nThe transit system, agency and mode, that had the most UPT in 2022 was the MTA New York City Transit with their Heavy Rail, with a total UPT of 1793073801. This number is way larger than the following on the list, which interestly enough also belongs to the MTA New York City transit and their Bus system.\n\nWhich transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\n\n\ncolnames(USAGE_AND_FINANCIALS)[colnames(USAGE_AND_FINANCIALS) == \"Total Fares\"] &lt;- \"Total_Fares\"\n\nhighest_farebox &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter( UPT &gt;= 400000) |&gt;\n  mutate(farebox_rate = Total_Fares/Expenses) |&gt; \n  group_by(Agency, Mode) |&gt;\n  summarize(highest_ratio = max (farebox_rate)) |&gt;\n  arrange(desc(highest_ratio))\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(highest_farebox)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                    Mode  highest_ratio\n   &lt;chr&gt;                                                     &lt;chr&gt;         &lt;dbl&gt;\n 1 Anaheim Transportation Network                            Bus           0.865\n 2 City of Gainesville, FL                                   Bus           0.548\n 3 MTA New York City Transit                                 Heav…         0.435\n 4 Massachusetts Bay Transportation Authority                Heav…         0.375\n 5 Woods Hole, Martha's Vineyard and Nantucket Steamship Au… Ferr…         0.335\n 6 Metro-North Commuter Railroad Company, dba: MTA Metro-No… Comm…         0.331\n 7 Centre Area Transportation Authority                      Bus           0.324\n 8 MTA Long Island Rail Road                                 Comm…         0.286\n 9 Southeastern Pennsylvania Transportation Authority        Heav…         0.253\n10 Regional Transportation Commission of Southern Nevada     Bus           0.252\n# ℹ 136 more rows\n\n\nHere, we can clearly see that the transit system with the highest farebox recovery is Anaheim Transportation Network with their bus transportation. The fairbox recovery for each transit system is found by dividing their Total Fares by their Expenses. Interestingly enough, Anaheim Transportation Network and their buses have a ratio of .865, which is about a 57% greater than the second place spot, which is the City of Gainesville, FL with their buses!\n\nWhich transit system (agency and mode) has the lowest expenses per UPT?\n\n\nlow_expense_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter( UPT &gt;= 400000) |&gt;\n  mutate(exp_p_upt = Expenses/UPT) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(lowest_exp_p_upt = min(exp_p_upt)) |&gt;\n  arrange(lowest_exp_p_upt)\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(low_expense_UPT)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                 Mode  lowest_exp_p_upt\n   &lt;chr&gt;                                                  &lt;chr&gt;            &lt;dbl&gt;\n 1 Anaheim Transportation Network                         Bus               12.8\n 2 University of Georgia                                  Bus               14.9\n 3 University of Michigan Parking and Transportation Ser… Bus               16.2\n 4 Town of Blacksburg                                     Bus               17.5\n 5 Ames Transit Agency                                    Bus               21.5\n 6 Centre Area Transportation Authority                   Bus               23.5\n 7 MTA New York City Transit                              Heav…             31.1\n 8 Greater Lafayette Public Transportation Corporation    Bus               31.5\n 9 San Diego Metropolitan Transit System                  Ligh…             31.6\n10 Champaign-Urbana Mass Transit District                 Bus               39.8\n# ℹ 136 more rows\n\n\nSimilar to the last inquiry , the transit system that has the lowest expenses per UPT is Anaheim Transportation Network with their bus transportation. Their ratio of expenses per UPT is 12.8, which is drastically less than the highest. The highest expense per UPT is Northeast Illinois Regional Commuter Railroad Corporation and their Cable Cars, with a ratio of 600, which is an extreme difference between the two.\n\nWhich transit system (agency and mode) has the highest total fares per UPT?\n\n\nhigh_fare_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000) |&gt;\n  mutate(totalfares_p_upt = (Total_Fares/UPT)) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(high_fare_p_upt = max(totalfares_p_upt)) |&gt;\n  arrange(desc(high_fare_p_upt))\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(high_fare_UPT)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                  Mode  high_fare_p_upt\n   &lt;chr&gt;                                                   &lt;chr&gt;           &lt;dbl&gt;\n 1 New Jersey Transit Corporation                          Bus             536. \n 2 Northeast Illinois Regional Commuter Railroad Corporat… Comm…           265. \n 3 Metro-North Commuter Railroad Company, dba: MTA Metro-… Comm…           161. \n 4 New Jersey Transit Corporation                          Comm…           119. \n 5 MTA New York City Transit                               Comm…           112. \n 6 Massachusetts Bay Transportation Authority              Bus             107. \n 7 Massachusetts Bay Transportation Authority              Comm…            84.4\n 8 Woods Hole, Martha's Vineyard and Nantucket Steamship … Ferr…            78.0\n 9 MTA Long Island Rail Road                               Comm…            73.9\n10 Peninsula Corridor Joint Powers Board                   Comm…            67.5\n# ℹ 136 more rows\n\n\nThe transit system with the highest total fares per UPT is New Jersey Transit Corporation and their buses, with a total fare of 536 to one Unlinked Passenger Trip. This is more than double the second place spot, which goes to Northeast Illinois Regional Commuter Railroad Corporation and their Commuter Bus, with a total fare of 265 to one Unlinked Passenger Trip.\n\nWhich transit system (agency and mode) has the lowest expenses per VRM?\n\nInterestingly enough, I had to change my “Expenses” column name to “expense” for my code to recognize the column instead of the table we established prior, hence the reasoning for that code below.\n\ncolnames(USAGE_AND_FINANCIALS)[colnames(USAGE_AND_FINANCIALS) == \"Expenses\"] &lt;- \"expense\"\n\nlow_exp_vrm&lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000) |&gt;\n  mutate(exp_p_vrm = (expense/VRM)) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(low_exp_p_vrm = min(exp_p_vrm)) |&gt; \n  arrange(low_exp_p_vrm) \n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(low_exp_vrm)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                              Mode       low_exp_p_vrm\n   &lt;chr&gt;                                               &lt;chr&gt;              &lt;dbl&gt;\n 1 Interurban Transit Partnership                      Bus                 77.2\n 2 City of El Paso                                     Bus                 85.8\n 3 Des Moines Area Regional Transit Authority          Bus                 86.3\n 4 Central Florida Regional Transportation Authority   Bus                 87.5\n 5 San Francisco Bay Area Rapid Transit District       Heavy Rail          88.8\n 6 Transportation District Commission of Hampton Roads Bus                 88.9\n 7 City of Gainesville, FL                             Bus                 90.1\n 8 Greater Lafayette Public Transportation Corporation Bus                 90.5\n 9 Ames Transit Agency                                 Bus                 92.0\n10 Delaware Transit Corporation                        Bus                 92.4\n# ℹ 136 more rows\n\n\nThe transit system with the lowest expenses per Vehicle Revenue Miles was Interurban Transit Partnership and their Bus transportation, with an expense of 77.2 per Vehicle Revenue Mile. This is far cheaper than the leader of this category, which is the New York City Department of Transportation and their Ferryboat, with an expense of 771 per vehicle revenue mile. Interestingly enough, out of the ten highest expenses on the list, nine of the most expensive Modes of transportation is the Ferryboat, with only the Cable Car in San Francisco coming in at fourth. This shows that a Ferryboat is most-likely the most expensive transportation mode for transit agencies to run in comparison to its vehicle revenue miles.\n\nWhich transit system (agency and mode) has the highest total fares per VRM?\n\n\nhigh_fare_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000) |&gt;\n  mutate(totalfares_p_vrm = (Total_Fares/VRM)) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(high_fare_p_vrm = max(totalfares_p_vrm)) |&gt;\n  arrange(desc(high_fare_p_vrm))\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(high_fare_VRM)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                  Mode  high_fare_p_vrm\n   &lt;chr&gt;                                                   &lt;chr&gt;           &lt;dbl&gt;\n 1 Washington State Ferries                                Ferr…          1120. \n 2 Woods Hole, Martha's Vineyard and Nantucket Steamship … Ferr…           829. \n 3 New York City Economic Development Corporation          Ferr…           188. \n 4 Anaheim Transportation Network                          Bus             170. \n 5 Massachusetts Bay Transportation Authority              Ligh…           131. \n 6 Port Authority Trans-Hudson Corporation                 Heav…           119. \n 7 Massachusetts Bay Transportation Authority              Heav…           101. \n 8 MTA Long Island Rail Road                               Comm…            98.6\n 9 Metro-North Commuter Railroad Company, dba: MTA Metro-… Comm…            94.4\n10 MTA New York City Transit                               Heav…            91.5\n# ℹ 136 more rows\n\n\nThe transit system with the highest total fares per vehicle revenue miles is Washington State Ferries, with a ratio of 1120 total fares to one vehicle revenue mile. An interesting correlation to the last inquiry is that the top three highest total fares per VRM belong to ferryboats. Furthermore, the highest value on the list is almost 500% greater than the third value, which is 188, and that percentage only grows as that list goes further down."
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "Mini Project 01: Fiscal Characterisitics of Major US Public Transit Systems",
    "section": "",
    "text": "In regard to what transit system in the country is the most effective, it depends on which data points you use to weigh your opinion. If you are considering which system has the highest values in URM and UPT, meaning it is the most frequently used, the answer would be the major transit system of NYC’s MTA Transit. Not only is its heavy rail system extremely prevalent in these numbers, which makes sense since numerous rails span the lengths of New York City and are used daily by many, but the NYC MTA Bus also holds rank compared to others.\nHowever, if you are considering which transit system is the most cost effective, it would have to be the Anaheim Transportation Network. Since a high farebox recovery ratio indicates that a transit system is profitable, Anaheim Transportation Network bus’ numbers alone show that this system is highly more profitable than the rest. This agency also leads margin of lowest expenses per unlinked passenger trip, which shows that not only is this system great at making money, it also great at retaining it and not spending the majority of it on expenses. However, while this is great from a business standpoint, as a passenger this transportation agency ranks high on the total fares per vehicle revenue mile scale, as it is in 4th place. This means that this transportation is fairly expensive to ride compared to the others.\nIn my opinion, I believe that the MTA New York City Transit is the most effective in the country, due to the enormous scale it can run on at a moderate price compared to other agencies."
  },
  {
    "objectID": "mp01.html#introduction-farebox-recovery",
    "href": "mp01.html#introduction-farebox-recovery",
    "title": "Mini Project 01: Fiscal Characterisitics of Major US Public Transit Systems",
    "section": "",
    "text": "This project was taken as inspiration from a YouTuber City Nerd, who created a video about the “10 Transit Services That Do Huge Numbers at the Farebox”. Farebox Recovery is the fraction of revenues raised from fares instead of taxes.\nFor this project, we will use data from the [National Transit Database] as our primary source. The following tables/reports will be used:\n\nThe 2022 Fare Revenue table, in which we will mostly explore revenue.\nThe lastest Monthly Ridership tables, in which we will mostly explore the Unlinked Passenger Trips (UPT) and Vehicle Revenue Miles (VRM).\nThe 2022 Operating Expenses reports, in which we will mostly explore expenses.\n\nWe will use the 2022 verson of all reports, as up-to-date data and newer reports are often uploaded on a lag."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini Project 02: The Business of Show Business",
    "section": "",
    "text": "In this project, I will be utilizing data from the Internet Movie Database (IMDb) to discover what the next best movie is to produce. Before settling on my decision, I will dive into Hollywood’s history to identify key characteristics of successful movies, identify successful directors and actors, and examine some of Hollywood’s most famous flops. With all this information compiled together, I will pick a crew and a movie to remake, and then pitch the idea to the higher ups at my company.\n\n\n\nFirstly, I must download the data of Hollywood’s history into my rstudio. Since there were a sizable amount of people recorded in the industry, I decided to restrict my attention to people with at least two “known for” credits. This all can be seen in the folded code below:\n\n\nClick here to see how the data was downloaded\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\n\n\nget_imdb_file &lt;- function(fname){\n  BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n  fname_ext &lt;- paste0(fname, \".tsv.gz\")\n  if(!file.exists(fname_ext)){\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL, \n                  destfile = fname_ext)\n  }\n  as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\n\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\nRows: 771476 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (6): nconst, primaryName, birthYear, deathYear, primaryProfession, known...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\") \n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 4729628 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (8): tconst, titleType, primaryTitle, originalTitle, startYear, endYear,...\ndbl (1): isAdult\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\") \n\nRows: 8585773 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (4): tconst, parentTconst, seasonNumber, episodeNumber\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\") \n\nRows: 1490875 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): tconst\ndbl (2): averageRating, numVotes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\") \n\nRows: 10521826 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): tconst, directors, writers\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\n\nNow that I restricted the worker’s data set, I am curious to see how many obscure movies are in my data, as I want to avoid creating a low selling movie. Below is a chart that depicts the number of titles in comparison to their number of IMDb ratings.\n\nTITLE_RATINGS |&gt;\n    ggplot(aes(x=numVotes)) + \n    geom_histogram(bins=30) +\n    xlab(\"Number of IMDB Ratings\") + \n    ylab(\"Number of Titles\") + \n    ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\nHere, we can see the a majority of the movies in this data set have less than 100 ratings. To limit my data set further, I threw out any title that had less than 100 ratings. Looking at the quarterly ranges below, this shows that this drops about 75% of our data.\n\nTITLE_RATINGS |&gt;\n    pull(numVotes) |&gt;\n    quantile()\n\n     0%     25%     50%     75%    100% \n      5      11      26     100 2954275 \n\n\nSo, I performed this drop by performing the following code:\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\nWe now want to do the same for our “Title” tables, which can be seen below.\n\n\nClick here to see how this was done\n\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\n#| cache: false\n\n\nNow that our data has been significantly reduced, we can move on!\n\n\nAfter performing the glimpse function to examine each table, it was clear that some columns appeared as “character (string)” vectors, when they should be numeric. This is due to the null or N/A values that appears in the data sets. Since R cannot read these files as is, we must mutate the tables in a way that R can read them numerically. This is done with the following code:\n\n\nClick here to see how this was done\n\n\nlibrary(dplyr)\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  mutate(birthYear = as.numeric(birthYear),\n         deathYear = as.numeric(deathYear))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `birthYear = as.numeric(birthYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(isAdult = as.logical(isAdult),\n         endYear = as.numeric(endYear),\n         startYear = as.numeric(startYear),\n         runtimeMinutes = as.numeric(runtimeMinutes))\n\nWarning: There were 3 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `endYear = as.numeric(endYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n  mutate(seasonNumber = as.numeric(seasonNumber),\n         episodeNumber = as.numeric(episodeNumber))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `seasonNumber = as.numeric(seasonNumber)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\nNow that the data tables are cleaned up and ready to be used, I can begin to explore the data.\n\n\n\nFirstly, I want to find out how many movies are in our data set? How many TV series? And how many TV episodes?\n\n\nClick here to see the code\n\n\nNumber_of_Movies &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\") |&gt;\n  summarize(Number = n())\n\nprint(Number_of_Movies$Number)\n\n[1] 89305\n\n\n\nNumber_of_TVshows &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  summarize(Number = n())\n\nprint(Number_of_TVshows$Number)\n\n[1] 16742\n\n\n\nNumber_of_Episodes &lt;- TITLE_EPISODES |&gt;\n  summarize(Number = n())\n\nprint(Number_of_Episodes$Number)\n\n[1] 3026526\n\n\n\nAfter running the code above, I found that there were 89,305 movies, 16,742 TV shows, and 3,026,526 TV Episodes.\nSecondly, I want to know who the oldest person is in our data set?\nTo do this, I performed the following code:\n\noldest_person &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear)) |&gt;\n  arrange(birthYear) |&gt;\n  slice(1)\n\nprint(oldest_person)\n\n     nconst       primaryName birthYear deathYear primaryProfession\n1 nm0393833 William S. Hooser      1836        NA             actor\n                 knownForTitles\n1 tt0157755,tt0015357,tt0408145\n\n\nI quickly saw that the birth year of this person was 1836. So unless our oldest person is 188 years old, we need to put a restraint on how far back their birth year is, in case their death was not recorded. Since the oldest living person in the world is 116, the earliest possible birth year can be 1908. Many individuals came up for this birth year, however once I Google searched the ten oldest people in the world, none of them were on that list. I even saw that a few of these names did have deaths recorded on Google, but not on the database. Since there are only about 500,000 living individuals over the age of 100, which is only about .007% of the population, I decided to set my birth range to 100 years ago. Though someone may be slightly older than this and still alive, I thought this would be a safe bet to assume.\n\noldest_person &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear),\n         birthYear &gt;= 1924) |&gt;\n  arrange(birthYear) |&gt;\n  slice(1)\n\nprint(oldest_person)\n\n     nconst     primaryName birthYear deathYear           primaryProfession\n1 nm0001693 Eva Marie Saint      1924        NA actress,producer,soundtrack\n                           knownForTitles\n1 tt0047296,tt0053125,tt0348150,tt1837709\n\n\nThus, I found that Eva Marie Saint is the oldest and still alive person in our database. Her career spanned over eighty years and she won numerous awards for her works. She was even born close to New York, in Newark, NJ!\nThirdly, there is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. I want to discover what episode this is and what show it belongs to.\nTo do this, I restricted my data to only show me these restraints. This can be seen in the following code:\n\n\nClick here to see how this was done\n\n\nBest_Episode &lt;- TITLE_RATINGS |&gt; \n  filter(averageRating == 10,\n         numVotes &gt;= 200000)\n\n\nBest_E_series &lt;- TITLE_EPISODES |&gt;\n  filter(tconst == \"tt2301451\")\n\nprint(Best_E_series)\n\n     tconst parentTconst seasonNumber episodeNumber\n1 tt2301451    tt0903747            5            14\n\nBest_Episode_name &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt0903747\")\n\nprint(Best_Episode_name)\n\n     tconst titleType primaryTitle originalTitle isAdult startYear endYear\n1 tt0903747  tvSeries Breaking Bad  Breaking Bad   FALSE      2008    2013\n  runtimeMinutes               genres\n1             45 Crime,Drama,Thriller\n\n\n\nFrom doing the commands above, I found that the only episode that reached these ratings was episode 14 of season 5 in Breaking Bad. Looking this up on Google, I found that the title of the episode was “Ozymandias.” This episode is rated so highly due to the brilliance of how show runners depicted the main protagonist getting everything taken away from him. Pivotal events happen, including a main character’s demise which became a significant turning point for the narrative. Over a decade later, this episode is still raved and talked about by fans.\nFourthly, what four projects is the actor Mark Hamill most known for?\n\n\nClick here to see how this was done\n\n\nMark_Hamill &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\")\n\nprint(Mark_Hamill)\n\n     nconst primaryName birthYear deathYear       primaryProfession\n1 nm0000434 Mark Hamill        NA        NA actor,producer,director\n                           knownForTitles\n1 tt0076759,tt2527336,tt0080684,tt0086190\n\nMark_Hamill_Projects &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt0076759\") \n\nprint(Mark_Hamill_Projects)\n\n     tconst titleType                       primaryTitle originalTitle isAdult\n1 tt0076759     movie Star Wars: Episode IV - A New Hope     Star Wars   FALSE\n  startYear endYear runtimeMinutes                   genres\n1      1977      NA            121 Action,Adventure,Fantasy\n\nMark_Hamill_Projects &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt2527336\") \n\nprint(Mark_Hamill_Projects)\n\n[1] tconst         titleType      primaryTitle   originalTitle  isAdult       \n[6] startYear      endYear        runtimeMinutes genres        \n&lt;0 rows&gt; (or 0-length row.names)\n\nMark_Hamill_Projects &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt0086190\") \n\nprint(Mark_Hamill_Projects)\n\n     tconst titleType                               primaryTitle\n1 tt0086190     movie Star Wars: Episode VI - Return of the Jedi\n                               originalTitle isAdult startYear endYear\n1 Star Wars: Episode VI - Return of the Jedi   FALSE      1983      NA\n  runtimeMinutes                   genres\n1            131 Action,Adventure,Fantasy\n\nMark_Hamill_Projects &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt0080684\") \n\nprint(Mark_Hamill_Projects)\n\n     tconst titleType                                   primaryTitle\n1 tt0080684     movie Star Wars: Episode V - The Empire Strikes Back\n                                   originalTitle isAdult startYear endYear\n1 Star Wars: Episode V - The Empire Strikes Back   FALSE      1980      NA\n  runtimeMinutes                   genres\n1            124 Action,Adventure,Fantasy\n\n\n\nUsing the command above, I found that Mark Hamill was known for four Star Wars titles, including: Star Wars: Episode IV - A New Hope, Star Wars: Episode VIII - The Last Jedi, Star Wars: Episode V - The Empire Strikes Back, and Star Wards: Episode VI - Return of the Jedi.\nFifthly, I want to know what TV series, with more than 12 episodes, has the highest average rating?\nTo compute this, I did the following:\n\n\nClick here to see how this was done\n\n\nepisode_amount &lt;- TITLE_EPISODES |&gt;\n    group_by(parentTconst) |&gt;\n    summarise(total_episodes = n(), .groups = 'drop')\n\ntv_ratings &lt;- TITLE_BASICS |&gt;\n    filter(titleType == \"tvSeries\") |&gt;\n    inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n    inner_join(episode_amount, by = c(\"tconst\" = \"parentTconst\"))\n\n\nhighest_rating_series &lt;- tv_ratings |&gt;\n    filter(total_episodes &gt; 12) |&gt;\n    arrange(desc(averageRating)) |&gt;\n    slice(1)\n\nhighest_rating_series |&gt;\n    select(primaryTitle, total_episodes, averageRating)\n\n  primaryTitle total_episodes averageRating\n1  Craft Games            318           9.7\n\nprint(highest_rating_series)\n\n      tconst titleType primaryTitle originalTitle isAdult startYear endYear\n1 tt15613780  tvSeries  Craft Games   Craft Games   FALSE      2014      NA\n  runtimeMinutes                  genres averageRating numVotes total_episodes\n1             NA Adventure,Comedy,Family           9.7      151            318\n\n\n\nThus, I found out that Craft Games was the highest rated series with over 12 episodes, with an average rating of 9.7.\nLastly, I wanted to know more about the TV series Happy Days. The TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality.\nI want to know if it is true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\nTo do this, I must restrict my data set to only show me information about Happy Days. At first, I only restricted this by the name, but I quickly realized that there were numerous projects with the same name. Because of this, I also added the restrictions of the start year and end year of the series. By doing the following code, I find the necessary tconst that identifies the series. Then, I ran the following code to get the average rating of each season and compare them.\n\n\nClick here to see how this was done\n\n\nTITLE_BASICS |&gt;\n  filter(primaryTitle == \"Happy Days\",\n         startYear == \"1974\",\n         endYear == \"1984\") \n\n     tconst titleType primaryTitle originalTitle isAdult startYear endYear\n1 tt0070992  tvSeries   Happy Days    Happy Days   FALSE      1974    1984\n  runtimeMinutes              genres\n1             30 Comedy,Family,Music\n\n\n\nHappy_Days &lt;- TITLE_EPISODES |&gt;\n  filter(parentTconst == \"tt0070992\")\n\nHappy_Days_Scores &lt;- inner_join(Happy_Days, TITLE_RATINGS, join_by(tconst))\n\n\nHappy_Days_Seasons &lt;- Happy_Days_Scores |&gt;\n  group_by(seasonNumber) |&gt;\n  summarize(averagescore = mean(averageRating, na.rm = TRUE)) |&gt;\n  arrange(seasonNumber)\n\n\n\nlibrary(DT)\ndatatable(Happy_Days_Seasons) \n\n\n\n\n\nBy looking at the table above, we can see that the scores did in fact drop towards the later seasons, with Season 8 being the lowest scoring season. For the first half of the show, they average at a high 7 rating, but a downward decline began after season 3. Season 11, which was their final season, inevitably scored back in the 7 range, but the score still did not beat any from the first four seasons.\n\n\n\nFor my success metric, I decided to create a scale that assesses the total popularity of a work. To do this, I decided to multiply the average rating that it got by the number of votes. My logic is that a work cannot be successful without each of these metrics: a low scoring average with a lot of votes means that the movie flopped and a high scoring average with no votes means that the movie is too obscure. However, if a movie is averaged highly and has a lot of votes, this means that it is widely known and highly regarded. Thus, I believe that this is an accurate way to measure success.\nTo test my theory, I will run some tests to see if it holds up against actually highly regarded movies and some of the more obscure ones in Hollywood.\nFirstly, I will choose the top ten movies on my metric and confirm that they were box office successes.\n\n\nClick here to see how I filtered my data to show this\n\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  mutate(popularity = averageRating*numVotes) \n\nmovie_ratings &lt;- inner_join(TITLE_BASICS, TITLE_RATINGS, join_by(tconst))\n\nmovie_ratings &lt;- movie_ratings |&gt; select(tconst, titleType, primaryTitle, genres, startYear, averageRating, numVotes, popularity)\n\nmovie_ratings &lt;- movie_ratings |&gt; \n  filter(titleType == \"movie\") |&gt;\n  arrange(desc(popularity))\n\n\nTop_ten &lt;- movie_ratings |&gt;\n  slice(1:10)\n\nprint(Top_ten)\n\n      tconst titleType                                      primaryTitle\n1  tt0111161     movie                          The Shawshank Redemption\n2  tt0468569     movie                                   The Dark Knight\n3  tt1375666     movie                                         Inception\n4  tt0137523     movie                                        Fight Club\n5  tt0109830     movie                                      Forrest Gump\n6  tt0110912     movie                                      Pulp Fiction\n7  tt0816692     movie                                      Interstellar\n8  tt0068646     movie                                     The Godfather\n9  tt0120737     movie The Lord of the Rings: The Fellowship of the Ring\n10 tt0133093     movie                                        The Matrix\n                    genres startYear averageRating numVotes popularity\n1                    Drama      1994           9.3  2954275   27474758\n2       Action,Crime,Drama      2008           9.0  2935734   26421606\n3  Action,Adventure,Sci-Fi      2010           8.8  2605397   22927494\n4                    Drama      1999           8.8  2386403   21000346\n5            Drama,Romance      1994           8.8  2311754   20343435\n6              Crime,Drama      1994           8.9  2268830   20192587\n7   Adventure,Drama,Sci-Fi      2014           8.7  2179321   18960093\n8              Crime,Drama      1972           9.2  2059556   18947915\n9   Action,Adventure,Drama      2001           8.9  2052301   18265479\n10           Action,Sci-Fi      1999           8.7  2097695   18249947\n\n\n\nThe following are the top ten successes on my list:\n\nThe Shawshank Redemption\nThe Dark Knight\nFight Club\nForrest Gump\nPulp Fiction\nThe Godfather\nInterstellar\nThe Lord of the Rings: The Fellowship of the Ring\nThe Matrix\nThe Lord of the Rings: The Return of the King\n\nAll of which were MAJOR successes in the box office. So far, this success metric is looking good!\nSecondly, I will do the same as before, but I will choose the five lowest movies on my list and ensure they were of low quality.\n\n\nClick here to see how I filtered my data to show this\n\n\nmovie_ratings &lt;- movie_ratings |&gt; \n  filter(titleType == \"movie\") |&gt;\n  arrange(popularity)\n\n\nbottom_five &lt;- movie_ratings |&gt;\n  slice(1:5)\n\nprint(bottom_five)\n\n     tconst titleType                             primaryTitle\n1 tt0498734     movie                 Zwischen Glück und Krone\n2 tt0386134     movie                        Yubiley prokurora\n3 tt0309422     movie               The Case He Couldn't Crack\n4 tt1773029     movie                                 Rockland\n5 tt1666348     movie Vixen Highway 2006: It Came from Uranus!\n                  genres startYear averageRating numVotes popularity\n1            Documentary      1959           1.2      103      123.6\n2                 Comedy      2003           1.1      128      140.8\n3         Crime,Thriller      1981           1.0      153      153.0\n4          Comedy,Family      2010           1.5      102      153.0\n5 Action,Horror,Thriller      2010           1.1      145      159.5\n\n\n\nThe following are the 5 lowest rated on my list:\n\nZwischen Glück und Krone\nYubiley prokurora\nThe Case He Couldn’t Crack\n18’ler Takimi\nParentesi tonde\n\nThese all proved to be more obscure and poorly-rated movies. Therefore, the metric does work both ways.\nThirdly, now I will choose a prestige actor and confirm that they have many projects that score highly on my metric. For this, I decided to look into Tom Hanks and Leonardo DiCaprio, as I believe they are two major actors in the industry currently.\n\n\nClick here to see how I filtered my data to show this\n\n\nlibrary(tidyverse)\n\nLeo &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Leonardo DiCaprio\") |&gt;\n  separate_longer_delim(knownForTitles, \",\")\n\n\ncolnames(Leo)[colnames(Leo) == \"knownForTitles\"] &lt;- \"tconst\"\n\nLeo &lt;- inner_join(Leo, movie_ratings, join_by(tconst))\n\nHanks &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Tom Hanks\") |&gt;\n  separate_longer_delim(knownForTitles, \",\")\n\n\ncolnames(Hanks)[colnames(Hanks) == \"knownForTitles\"] &lt;- \"tconst\"\n\nHanks &lt;- inner_join(Hanks, movie_ratings, join_by(tconst)) \n\nLeo_Hanks &lt;- bind_rows(Leo, Hanks)\n\nLeo_Hanks2 &lt;- Leo_Hanks |&gt; select(primaryName, primaryTitle, averageRating, numVotes, popularity)\n\n\n\ndatatable(Leo_Hanks2)\n\n\n\n\n\nHere, we can see that the movies given back by my success metric are in fact box office successes. Movies such as Titanic, The Wolf of Wall Street, Cast Away, and Forrest Gump are all well-known movies that were incredibly well-rated. Thus, the metric still holds.\nFourthly, for my final test, I wanted to compare my metric with the highest-grossing movies of all time. After searching this data on Google, I found that some of the highest grossing movies of all time is Titanic with $2,264,750,694, Avatar: The Way of Water with $2,320,250,281, and Avatar with $2,923,706,026.\n\n\nClick here to see how I filtered my data to show this\n\n\nHighest_Grossing &lt;- data.frame(\n  tconst = c(\"tt0499549\", \"tt4154796\", \"tt1630029\", \"tt0120338\", \"tt2488496\"),\n  LifetimeGross = c(\"$2,923,706,026\", \"$2,799,439,100\", \"$2,320,250,281\", \"$2,264,750,694\", \"$2,071,310,218\")\n)\n\n\nHigh_Grossing &lt;- inner_join(movie_ratings, Highest_Grossing, join_by(tconst))\n\n\nThe chart below depicts three of the above movies. It is clear to see that the popularity score correlates with their gross, as both are high. This solidifies my success metric even further.\n\ndatatable(High_Grossing)\n\n\n\n\n\nLastly, now that my success metric has been solidified, I need to restrict my data set to only show me “successes.” To do this, I must distinguish a number that once a movie surpasses it in its “popularity” scale, then it is deemed a success. The number I decided to pick is 5,000,000, since I feel as though every great movie should be able to surpass that. Then, I reduced my data set to fit that benchmark using the following code.\n\nsolid_or_better &lt;- movie_ratings |&gt;\n  filter(popularity &gt;= 5000000)\n\n\n\n\nFirstly, I want to discover which genre had the most “successes” in each decade.\n\n\nClick here to see the code\n\n\nsolid_or_better &lt;- solid_or_better|&gt;\n  arrange(startYear)\n\n\nsolid_or_better &lt;- solid_or_better |&gt; separate_longer_delim(genres, \",\") \n\n\ndecades &lt;- solid_or_better |&gt;\n  count(genres)\n\n\nforties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1940 & startYear &lt;= 1949) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\nfifties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1950 & startYear &lt;= 1959) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\n\nsixties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1960 & startYear &lt;= 1969) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\n\nseventies &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1970 & startYear &lt;= 1979) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\neighties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1980 & startYear &lt;= 1989) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\nnineties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1990 & startYear &lt;= 1999) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\nthousands &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2000 & startYear &lt;= 2009) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\ntens &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2010 & startYear &lt;= 2019) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\ntwenties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2020 & startYear &lt;= 2029) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\nprint(forties)\n\n  genres n\n1  Drama 1\n\nprint(fifties)\n\n  genres n\n1  Crime 1\n\nprint(sixties)\n\n     genres n\n1 Adventure 2\n\nprint(seventies)\n\n  genres n\n1  Drama 7\n\nprint(eighties)\n\n  genres n\n1 Action 8\n\nprint(nineties)\n\n  genres  n\n1  Drama 25\n\nprint(thousands)\n\n     genres  n\n1 Adventure 30\n\nprint(tens)\n\n  genres  n\n1 Action 21\n\nprint(twenties)\n\n  genres n\n1 Action 3\n\n\n\nThus, the most successes per decade is as follows:\n\n40s: Drama (1)\n50s: Crime (1)\n60s: Adventure (2)\n70s: Drama (7)\n80s: Action (8)\n90s: Drama (25)\n00s: Drama/Adventure (30)\n10s: Action (21)\n20s: Action (2)\n\nSecondly, to see what genre had the most consistent “successes” and which one used to reliably produce “successes,” but has fallen off, I decided to create a data table. This way, all of the successful movies per genre throughout the decades can be easily visualized.\n\n\nClick here to see how the table was made\n\n\nforties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1940 & startYear &lt;= 1949) |&gt;\n  count(genres) \n\nfifties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1950 & startYear &lt;= 1959) |&gt;\n  count(genres) \n\n\nsixties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1960 & startYear &lt;= 1969) |&gt;\n  count(genres) \n\n\nseventies &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1970 & startYear &lt;= 1979) |&gt;\n  count(genres) \n\n\n\neighties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1980 & startYear &lt;= 1989) |&gt;\n  count(genres) \n\nnineties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1990 & startYear &lt;= 1999) |&gt;\n  count(genres) \n\n\nthousands &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2000 & startYear &lt;= 2009) |&gt;\n  count(genres) \n\n\ntens &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2010 & startYear &lt;= 2019) |&gt;\n  count(genres) \n\ntwenties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2020 & startYear &lt;= 2029) |&gt;\n  count(genres) \n\ndecades_total &lt;- solid_or_better |&gt;\n  count(genres) \n\ncolnames(forties)[colnames(forties) == \"n\"] &lt;- \"40s\"\ncolnames(fifties)[colnames(fifties) == \"n\"] &lt;- \"50s\"\ncolnames(sixties)[colnames(sixties) == \"n\"] &lt;- \"60s\"\ncolnames(seventies)[colnames(seventies) == \"n\"] &lt;- \"70s\"\ncolnames(eighties)[colnames(eighties) == \"n\"] &lt;- \"80s\"\ncolnames(nineties)[colnames(nineties) == \"n\"] &lt;- \"90s\"\ncolnames(thousands)[colnames(thousands) == \"n\"] &lt;- \"00s\"\ncolnames(tens)[colnames(tens) == \"n\"] &lt;- \"10s\"\ncolnames(twenties)[colnames(twenties) == \"n\"] &lt;- \"20s\"\ncolnames(decades_total)[colnames(decades_total) == \"n\"] &lt;- \"total\"\n\ndecades &lt;- full_join(forties, fifties, join_by(genres))\ndecades &lt;- full_join(decades, sixties, join_by(genres))\ndecades &lt;- full_join(decades, seventies, join_by(genres))\ndecades &lt;- full_join(decades, eighties, join_by(genres))\ndecades &lt;- full_join(decades, nineties, join_by(genres))\ndecades &lt;- full_join(decades, thousands, join_by(genres))\ndecades &lt;- full_join(decades, tens, join_by(genres))\ndecades &lt;- full_join(decades, twenties, join_by(genres))\ndecades &lt;- full_join(decades, decades_total, join_by(genres))\n\n\nBelow is a table that shows all of the genres throughout the years.\n\nlibrary(DT)\ndatatable(decades) \n\n\n\n\n\nLooking at the chart, it is clear to see that the genre with the most consistent successes each decade is Drama, as it is the only genre that had a least one success every decade from the forties until now. It also has gotten more successes recently, with its height being in the 2000s.\nThe genre that used to reliably produce successes, but has fallen out of favor is Crime, as it had many success from the 1970s to 2000s, entirely fell off in the 2010s with only three successes. However, since the 2000s, the Drama genre definitely fell off the most, going from 30 success to 14 to 3 per decade.\nThirdly, I want to know what genre has produced the most successes since 2010. By looking at the chart above, it is clear to see the genre with the most successes since 2010 is Action with 24 successes. The Adventure genre is closely behind that, with 21 successes.\nHowever, I want to know if this genre has only produced the most successes because it genuinely has the highest rating, or if there were just an abundance of Action movies that have been created since then to contribute to the score. To do that, I used the following code:\n\n\nClick here to see the code\n\n\ndecade2010_pop &lt;- movie_ratings |&gt;\n  filter(startYear &gt;= 2010 & startYear &lt;= 2019) |&gt;\n  separate_longer_delim(genres, \",\") |&gt; \n  group_by(genres) |&gt;\n  summarize(totalPopularity = sum(popularity)) |&gt;\n  arrange(desc(totalPopularity)) |&gt;\n  slice(1:3)\n\n\ndecade2010_amount &lt;- movie_ratings |&gt;\n  filter(startYear &gt;= 2010 & startYear &lt;= 2019) |&gt;\n  separate_longer_delim(genres, \",\") |&gt; \n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1:9) \n\n\n\nprint(decade2010_pop)\n\n# A tibble: 3 × 2\n  genres    totalPopularity\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Action         538827123.\n2 Drama          534760306.\n3 Adventure      415932283.\n\nprint(decade2010_amount)\n\n       genres    n\n1       Drama 3839\n2      Comedy 2259\n3      Action 1157\n4    Thriller 1074\n5      Horror  959\n6 Documentary  958\n7     Romance  932\n8       Crime  823\n9   Adventure  608\n\n\nHere, we can see that it ranked first on the total popularity list, but third on the amount of movies list. This shows that while this genre produces highly-rated movies, there is also a good amount of them being made. However, the drama genre is the most produced and is still below the action genre on total popularity, so this shows that each movie is more successful than others.\nLastly, the genre that has become more popular in recent years is definitely the Action genre. It had its first successful movie in the 1970s, while ten other genres had earlier successes, and still skyrocketed in later decades to get 66 total successes. Only two genres are ahead of the action genre, which is drama with 86 successes and adventure with 67 successes. However, since the 2000s, it has had 50 successes, which is only one behind the top spot for successes in that time frame, which is the adventure genre.\nBased on my findings, I decided to choose an Action genre, as while it is the third most produced in the last decade, it had the highest popularity score. It surpasses the Drama genre on the list, which is big considering that the drama genre has 20 more successes than the action genre. This shows me that the drama genre only have this high of a popularity due to successes that they had, which means each success had to average at the lower end of the success scale. Though the adventure genre also did very well on both of these lists, I decided not to choose it as I fear it may begin to get overplayed, much like the Drama genre. However, the Action genre seems to be an up-and-coming one, as it already has more successes than the Adventure genre in the 2020s, and the same amount as the Drama genre. Thus, I believe this is our best bet for a successful movie.\n\n\n\nNow that I have my target genre, I want to identify a few actors and one director to really anchor my project. To create a strong team, I want one prestigious actor who has many successes in many genres, and preferably at least one major success in the action genre. With this major name, I hope to bring in their major following and their broad skill set to perfect the movie. For my second actor, I want to pick someone who has been apart of a highly rated movie, but is still not widely recognized yet. I want someone who is up-and-coming, that can build a strong profile around being in this movie. Then, the connection with these two main leads should bring good publicity.\nFor my prestige actor, I want someone who has been in the industry for a good amount of time and that is older with some experience. Thus, I decided that I wanted my actor to be at least 40, but no older than 75.\n\n\nClick here to see how I filtered my data to show this\n\n\nalive_actors &lt;- NAME_BASICS |&gt;\n  filter(birthYear &gt;= 1949 & birthYear &lt;= 1984) |&gt;\n  filter(is.na(deathYear)) |&gt;\n  separate_longer_delim(primaryProfession, \",\") |&gt;\n  filter(primaryProfession == \"actor\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") \n\nsolid_or_better2 &lt;- movie_ratings |&gt;\n  filter(popularity &gt;= 5000000)\n\nsuccessful_movies &lt;- solid_or_better2 |&gt;\n  select(tconst, genres, popularity) \n\ncolnames(successful_movies)[colnames(successful_movies) == \"tconst\"] &lt;- \"knownForTitles\"\n\n\nsuccessful_actors &lt;- full_join(alive_actors, successful_movies, join_by(knownForTitles))\n\nsuccessful_actors &lt;- successful_actors |&gt;\n  arrange(desc(popularity))\n\nsuccessful_actors_average &lt;-successful_actors |&gt;\n  select(primaryName, popularity) |&gt;\n  group_by(primaryName) |&gt;\n  summarize(totalpopularity = sum(popularity, na.rm = TRUE)) |&gt;\n  arrange(desc(totalpopularity))\n\n\n\ndatatable(successful_actors_average)\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html\n\n\n\n\n\n\nOne by one, I decided to go down the list of people to see what their top works were. This can be seen in the code below.\n\n\nClick here to see to see the code\n\n\nOrlando_Bloom &lt;- successful_actors |&gt;\n  filter(primaryName == \"Orlando Bloom\")\n\nHans_Zimmer &lt;- successful_actors |&gt;\n  filter(primaryName == \"Orlando Bloom\")\n\nHugo_Weaving &lt;- successful_actors |&gt;\n  filter(primaryName == \"Hugo Weaving\")\n\nGary_Rizzo &lt;- successful_actors |&gt;\n  filter(primaryName == \"Gary A. Rizzo\")\n\nLeonardo_DiCaprio &lt;- successful_actors |&gt;\n  filter(primaryName == \"Leonardo DiCaprio\")\n\n\nI decided against Orlando Bloom, since he had 4 successful movies, three of which were apart of the Lord of the Rings franchise, and one Pirates of the Caribbean. I want a more diverse actor, rather than someone who mainly dominates the same universe.\nHugo Weaving was similar, with three Lord of the Rings movies and one movie titled “V for Vendetta.” Rizzo would have been perfect… until I researched and found out he was the music producer on these projects, and not an actor. Same with Zimmer.\nFinally, I got to my seventh person on the list, Leonardo DiCaprio. Not only did his top movies dominate different genres, including Crime, Drama, Thriller, Romance, Comedy, etc., his top movie was an Action/Adventure mixed genre. Also scoring highest on his list on the popularity scale, Inception became the fourth highest-grossing movie of 2010, with DiCaprio as the star of the movie. Thus, it was clear that he was my first pick actor.\nNext, I wanted to find someone younger and more up-and-coming. Thus, I chose to restrict the age gap to someone less than 30 years old. I did this with the following code:\n\n\nClick here to see to see the code\n\n\nalive_actors_young &lt;- NAME_BASICS |&gt;\n  filter(birthYear &gt;= 1995 & birthYear &lt;= 2024) |&gt;\n  filter(is.na(deathYear)) |&gt;\n  separate_longer_delim(primaryProfession, \",\") |&gt;\n  filter(primaryProfession == \"actor\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") \n\nsolid_or_better2 &lt;- movie_ratings |&gt;\n  filter(popularity &gt;= 5000000)\n\nsuccessful_movies &lt;- solid_or_better2 |&gt;\n  select(tconst, genres, popularity) \n\ncolnames(successful_movies)[colnames(successful_movies) == \"tconst\"] &lt;- \"knownForTitles\"\n\n\nsuccessful_actors_young &lt;- full_join(alive_actors_young, successful_movies, join_by(knownForTitles))\n\nsuccessful_actors_young &lt;- successful_actors_young |&gt;\n  arrange(desc(popularity))\n\nsuccessful_actors_average_y &lt;-successful_actors_young |&gt;\n  select(primaryName, popularity) |&gt;\n  group_by(primaryName) |&gt;\n  summarize(totalpopularity = sum(popularity, na.rm = TRUE)) |&gt;\n  arrange(desc(totalpopularity))\n\n\nTony_Revolori &lt;- successful_actors_young |&gt;\n  filter(primaryName == \"Tony Revolori\")\n\n\nI quickly decided upon Tony Revolori, as he was high on the list and was in a Spiderman movie recently that did fairly well. When I looked him up, he had some breakout awards and is expected to become a lot bigger in the upcoming years, especially as the projects he is currently working on begin to roll out.\nAt this point of the project, I realized that I had a slight oversight and did not include any female actresses into the mix. So, I decided to pick another younger actor to add to my team.\n\n\nClick here to see to see the code\n\n\nalive_actress_young &lt;- NAME_BASICS |&gt;\n  filter(birthYear &gt;= 1995 & birthYear &lt;= 2024) |&gt;\n  filter(is.na(deathYear)) |&gt;\n  separate_longer_delim(primaryProfession, \",\") |&gt;\n  filter(primaryProfession == \"actress\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") \n\n\nsuccessful_actress_young &lt;- full_join(alive_actress_young, successful_movies, join_by(knownForTitles))\n\nsuccessful_actress_average_y &lt;-successful_actress_young |&gt;\n  select(primaryName, popularity) |&gt;\n  group_by(primaryName) |&gt;\n  summarize(totalpopularity = sum(popularity, na.rm = TRUE)) |&gt;\n  arrange(desc(totalpopularity))\n\n\n\nTaylor_Geare &lt;- successful_actress_young |&gt;\n  filter(primaryName == \"Taylor Geare\")\n\n\nBy running this previous code, I found that Taylor Geare would be a perfect pick for the movie. Not only has she won one award and been in a few works recently, I discovered that she was also in Inception with Leonardo DiCaprio! At the time, she was five years old and she played Philipa, who was the daughter of DiCaprio’s character. I believe that this pairing can bring major publicity to our work, as fans of Inception will get to see this duo back together, especially in a way that Geare will be able to display her skills as an adult.\nNow, I must find a director for my movie. Using the same technique as above, I began my search.\n\n\nClick here to see to see the code\n\n\nalive_directors &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear)) |&gt;\n  separate_longer_delim(primaryProfession, \",\") |&gt;\n  filter(primaryProfession == \"director\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") \n\nsuccessful_directors &lt;- full_join(alive_directors, successful_movies, join_by(knownForTitles))\n\n\nsuccessful_directors2 &lt;- successful_directors |&gt;\n  group_by(primaryName) |&gt;\n  summarize(totalpopularity = sum(popularity, na.rm = TRUE)) |&gt;\n  arrange(desc(totalpopularity))\n\nWally_Pfister &lt;- successful_directors |&gt;\n  filter(primaryName == \"Wally Pfister\")\n\n\n\ndatatable(successful_directors2)\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html\n\n\n\n\n\n\nComing in second on the list, Wally Pfister easily proved to be the strongest candidate for this movie. Not only did he score incredibly highly on this list, he also worked on major movies such as The Dark Knight, The Prestige, Transcendence, and much more. However, most notably, he also worked on Inception with Leonardo DiCaprio and Taylor Geare. A picture of DiCaprio and Pfister can be seen below.\n\nThus, not only have they all worked together in the past so they know each other/have good chemistry, they were also able to create a masterpiece of a movie before in the same genre we are aspiring to. This could stir a lot of good media, as it not seen very often that three people from a very successful movie team up again to work on a remake of another successful movie. Thus, I feel very confident about the potential team I have put together.\n\n\n\nNow, I must find a classic movie in this genre to remake. I want to find a movie that has not been remade in the past 25 years, has a large number of IMDb ratings, has a high average rating, and has a fan base that wants a remake to happen. To do this, I must first filter my data sets to give me a movie that fits these first three restraints.\n\n\nClick here to see to see the code\n\n\nremakemovie &lt;- movie_ratings |&gt;\n  filter(startYear &lt; 1999) |&gt;\n  filter(averageRating &gt;  8) |&gt;\n  filter(numVotes &gt; 20000) |&gt;\n  arrange(desc(popularity))\n\n\n\ndatatable(remakemovie)\n\n\n\n\n\nI want to look at what the top movies are in the action genre, however, I also want to stay away from remaking any Star Wars movies. Thus, coming in at number 14, I debating on remaking Léon: The Professional, however, when I looked up if fans wanted a remake, they seemed entirely opposed. Many fans believe the film is perfect as is and it would not do any justice for it to be remade today. Thus, I decided to keep looking.\nAt number 31, I found Die Hard, a movie made in 1988 about an NYPD cop that has to take matters into his own hands when a group of robbers take control of the building he is in, holding everyone hostage except for him.\nWhen I looked up if fans would want a remake of Die Hard, numerous articles came up showing great approval for this. Some of these are linked below:\n\nBleeding Cool News\nMedium\n\nThus, a Die Hard remake is the best bet for a successful action movie.\nHowever, before we move further, our legal department must reach out to certain members of the original movie production to secure the rights for the project. So, let’s find out who is still alive from the original.\n\n\nClick here to see to see the code\n\n\nDie_Hard &lt;- NAME_BASICS |&gt;\n  separate_longer_delim(knownForTitles, \",\") |&gt;\n  filter(knownForTitles == \"tt0083658\") |&gt;\n  filter(is.na(deathYear))\n\nprint(Die_Hard)\n\n       nconst           primaryName birthYear deathYear\n1   nm0000435          Daryl Hannah      1960        NA\n2   nm0000631          Ridley Scott      1937        NA\n3   nm0000707            Sean Young      1959        NA\n4   nm0001026        Joanna Cassidy        NA        NA\n5   nm0001579    Edward James Olmos      1947        NA\n6   nm0003256          Joshua Pines        NA        NA\n7   nm0003666         Debbie Denise        NA        NA\n8   nm0007072       Katherine Haber      1944        NA\n9   nm0022370          Vickie Alper        NA        NA\n10  nm0029959            Tim Angulo        NA        NA\n11  nm0045630    Michael Backauskas        NA        NA\n12  nm0047460      Robert D. Bailey        NA        NA\n13  nm0048193            Beau Baker      1953        NA\n14  nm0048396             Don Baker        NA        NA\n15  nm0049792         Peter Baldock      1949        NA\n16  nm0081795   William Biggerstaff        NA        NA\n17  nm0099156     Christian Boudman        NA        NA\n18  nm0114926       Winnie D. Brown        NA        NA\n19  nm0123746            Tom Burton        NA        NA\n20  nm0137698      Elizabeth Carlon        NA        NA\n21  nm0164425          Robert Clark        NA        NA\n22  nm0172782           Alan Collis        NA        NA\n23  nm0173645            Gary Combs      1935        NA\n24  nm0180108        Peter Cornberg        NA        NA\n25  nm0190038           Eugene Crum        NA        NA\n26  nm0193345           Greg Curtis        NA        NA\n27  nm0199344          Stephen Dane        NA        NA\n28  nm0203343 Howard Brady Davidson        NA        NA\n29  nm0213057           Lisa Deaner        NA        NA\n30  nm0214303        Michael Deeley      1932        NA\n31  nm0218540     Carolyn DeMirjian        NA        NA\n32  nm0220984        Linda DeScenna      1949        NA\n33  nm0238734           David Dryer      1943        NA\n34  nm0244956            Syd Dutton      1944        NA\n35  nm0252914              Bud Elam        NA        NA\n36  nm0266684       Hampton Fancher      1938        NA\n37  nm0270670         Jane Feinberg        NA        NA\n38  nm0274928      Michael Ferriter        NA        NA\n39  nm0280029      William E. Fitch        NA        NA\n40  nm0281551        Linda Fleisher        NA        NA\n41  nm0283845           Stephen Fog        NA        NA\n42  nm0285893          Terence Ford      1945        NA\n43  nm0290519          Chris Franco        NA        NA\n44  nm0292476       Terry D. Frazee        NA        NA\n45  nm0302091          Steve Galich        NA        NA\n46  nm0302406         Joe Gallagher      1953        NA\n47  nm0303421        Kurt P. Galvao        NA        NA\n48  nm0312920         Michael Genne        NA        NA\n49  nm0325242        Joyce Goldberg        NA        NA\n50  nm0333849         David Grafton        NA        NA\n51  nm0341454         Cary Griffith        NA        NA\n52  nm0356040           Robert Hall        NA        NA\n53  nm0362046   David R. Hardberger      1948        NA\n54  nm0362257          Alan Harding        NA        NA\n55  nm0367242   Graham V. Hartstone      1944        NA\n56  nm0369220          Donald Hauer        NA        NA\n57  nm0372264            Les Healey      1950        NA\n58  nm0381500            Linda Hess        NA        NA\n59  nm0384603       Richard L. Hill        NA        NA\n60  nm0390897  Richard E. Hollander        NA        NA\n61  nm0394803           Bob E. Horn        NA        NA\n62  nm0404527   Gillian L. Hutshing        NA        NA\n63  nm0438325        Michael Kaplan        NA        NA\n64  nm0452191       Madeleine Klein      1946        NA\n65  nm0453139           Crit Killen        NA        NA\n66  nm0461802       Michael Knutsen        NA        NA\n67  nm0487514         James Lapidus        NA        NA\n68  nm0507809        Terry E. Lewis        NA        NA\n69  nm0513924          Marci Liroff        NA        NA\n70  nm0518430        Basil Lombardo        NA        NA\n71  nm0519436          Ronald Longo        NA        NA\n72  nm0523370       Stephanie Lowry        NA        NA\n73  nm0560094        Linda Matthews        NA        NA\n74  nm0570488            Tim McHugh        NA        NA\n75  nm0573488    Gregory L. McMurry      1952        NA\n76  nm0590114         Michael Mills        NA        NA\n77  nm0616694          Donald Myers        NA        NA\n78  nm0624443          Tony Negrete        NA        NA\n79  nm0640384         John O'Connor        NA        NA\n80  nm0649687    James F. Orendorff        NA        NA\n81  nm0655669       Shirley Padgett        NA        NA\n82  nm0672065         Peter Pennell      1939        NA\n83  nm0672459    David Webb Peoples      1940        NA\n84  nm0689279     Thomas R. Polizzi        NA        NA\n85  nm0689332   George Polkinghorne        NA        NA\n86  nm0689503        Tama Takahashi        NA        NA\n87  nm0694138           Ivor Powell        NA        NA\n88  nm0703436        David Q. Quick        NA        NA\n89  nm0709588          Gary Randall        NA        NA\n90  nm0711047           Karen Rasch        NA        NA\n91  nm0722541    Victoria E. Rhodes        NA        NA\n92  nm0728110        Richard Rippel        NA        NA\n93  nm0745076     Jonathan Rothbart        NA        NA\n94  nm0747438     Thomas L. Roysden      1944        NA\n95  nm0761836     William Sanderson      1944        NA\n96  nm0768102          Fumi Mashimo        NA        NA\n97  nm0769731       Steve Schaeffer        NA        NA\n98  nm0770435          John Scheele        NA        NA\n99  nm0774039       Suzie Schneider        NA        NA\n100 nm0775562 Richard Peter Schroer        NA        NA\n101 nm0778771     John A. Scott III        NA        NA\n102 nm0779860             Tom Scott      1948        NA\n103 nm0791107    Victor A. Shelehov        NA        NA\n104 nm0794135        Arthur Shippee      1948        NA\n105 nm0811435       David L. Snyder        NA        NA\n106 nm0816170         Tom Southwell        NA        NA\n                                          primaryProfession knownForTitles\n1                                 actress,producer,director      tt0083658\n2                     producer,director,production_designer      tt0083658\n3                            actress,miscellaneous,director      tt0083658\n4                            actress,producer,miscellaneous      tt0083658\n5                                   actor,producer,director      tt0083658\n6              visual_effects,editorial_department,director      tt0083658\n7                     visual_effects,producer,miscellaneous      tt0083658\n8                            miscellaneous,producer,actress      tt0083658\n9                                             miscellaneous      tt0083658\n10         visual_effects,cinematographer,camera_department      tt0083658\n11               visual_effects,editorial_department,editor      tt0083658\n12                           visual_effects,director,writer      tt0083658\n13                     sound_department,actor,miscellaneous      tt0083658\n14                visual_effects,camera_department,director      tt0083658\n15              sound_department,editorial_department,actor      tt0083658\n16                             art_department,miscellaneous      tt0083658\n17        visual_effects,miscellaneous,editorial_department      tt0083658\n18                      costume_department,costume_designer      tt0083658\n19                   director,producer,animation_department      tt0083658\n20                                           visual_effects      tt0083658\n21        art_department,special_effects,make_up_department      tt0083658\n22                producer,production_manager,miscellaneous      tt0083658\n23                          stunts,assistant_director,actor      tt0083658\n24           production_manager,assistant_director,producer      tt0083658\n25                                          special_effects      tt0083658\n26                           special_effects,art_department      tt0083658\n27          art_department,art_director,production_designer      tt0083658\n28                                transportation_department      tt0083658\n29                                           visual_effects      tt0083658\n30                  sound_department,producer,miscellaneous      tt0083658\n31                                                  actress      tt0083658\n32                set_decorator,production_designer,actress      tt0083658\n33                         visual_effects,camera_department      tt0083658\n34                    visual_effects,special_effects,writer      tt0083658\n35                         visual_effects,camera_department      tt0083658\n36                                    actor,writer,director      tt0083658\n37             casting_director,casting_department,producer      tt0083658\n38      visual_effects,special_effects,editorial_department      tt0083658\n39                                        camera_department      tt0083658\n40                                           visual_effects      tt0083658\n41                                           visual_effects      tt0083658\n42               actor,assistant_director,camera_department      tt0083658\n43                                        camera_department      tt0083658\n44                                          special_effects      tt0083658\n45            special_effects,visual_effects,art_department      tt0083658\n46   sound_department,editorial_department,music_department      tt0083658\n47              producer,editorial_department,miscellaneous      tt0083658\n48                  camera_department,cinematographer,actor      tt0083658\n49                             visual_effects,miscellaneous      tt0083658\n50                                           visual_effects      tt0083658\n51                                        camera_department      tt0083658\n52                                           visual_effects      tt0083658\n53         visual_effects,camera_department,cinematographer      tt0083658\n54                         visual_effects,camera_department      tt0083658\n55                    sound_department,editorial_department      tt0083658\n56           assistant_director,production_manager,producer      tt0083658\n57             editor,editorial_department,sound_department      tt0083658\n58                miscellaneous,producer,production_manager      tt0083658\n59                           special_effects,art_department      tt0083658\n60        visual_effects,miscellaneous,animation_department      tt0083658\n61                      costume_department,costume_designer      tt0083658\n62                   editor,music_department,visual_effects      tt0083658\n63                costume_designer,costume_department,actor      tt0083658\n64                             miscellaneous,actress,stunts      tt0083658\n65            special_effects,art_department,visual_effects      tt0083658\n66                                            miscellaneous      tt0083658\n67        costume_designer,costume_department,miscellaneous      tt0083658\n68                                           art_department      tt0083658\n69                  casting_director,producer,miscellaneous      tt0083658\n70                                           art_department      tt0083658\n71                                           visual_effects      tt0083658\n72   music_department,sound_department,editorial_department      tt0083658\n73                      costume_department,costume_designer      tt0083658\n74                         visual_effects,producer,director      tt0083658\n75        visual_effects,miscellaneous,animation_department      tt0083658\n76                 make_up_department,actor,special_effects      tt0083658\n77                                          special_effects      tt0083658\n78                                         sound_department      tt0083658\n79     camera_department,cinematographer,assistant_director      tt0083658\n80                                           art_department      tt0083658\n81                                       make_up_department      tt0083658\n82                                         sound_department      tt0083658\n83                                   writer,editor,director      tt0083658\n84           producer,assistant_director,production_manager      tt0083658\n85                                           visual_effects      tt0083658\n86                         camera_department,visual_effects      tt0083658\n87                       producer,writer,assistant_director      tt0083658\n88                                           art_department      tt0083658\n89          production_designer,art_director,art_department      tt0083658\n90  editorial_department,script_department,sound_department      tt0083658\n91                              assistant_director,producer      tt0083658\n92                                           visual_effects      tt0083658\n93                  visual_effects,producer,special_effects      tt0083658\n94         set_decorator,art_department,production_designer      tt0083658\n95                                    actor,archive_footage      tt0083658\n96        visual_effects,animation_department,miscellaneous      tt0083658\n97                                music_department,composer      tt0083658\n98             visual_effects,producer,animation_department      tt0083658\n99                           visual_effects,special_effects      tt0083658\n100               assistant_director,producer,miscellaneous      tt0083658\n101                                          art_department      tt0083658\n102                         music_department,composer,actor      tt0083658\n103                                       camera_department      tt0083658\n104                                          art_department      tt0083658\n105         production_designer,art_department,art_director      tt0083658\n106         art_department,production_designer,art_director      tt0083658\n\n\n\nThus, 135 people from the original are still alive. Some actor and actresses are still alive as well, so we may want to consider adding them in the plot as a “fan service,” such as the main actor Bruce Willis.\n\n\n\nFor our next production, a remake of the 1988 Die Hard would be a strong competitor in the industry, starring Leonardo DiCaprio, Taylor Geare, and Tony Revolori. Directed by Academy Award winning Wally Pfister, the movie is bound to have an artistic expression like no other, similar to his eight other success. A guest appearance from Bruce Willis will have the fan base, who is practically begging for a remake, running to theaters.\nFrom the 1990s to the 2000s, the Action genre experienced over a 350% growth in successes, becoming one of the most wanted and rewarding genre in the industry. With beloved actor Leonardo Dicaprio combining forces with innovative director Pfister and up-and-coming Geare, we will give Inception fans a wanted reunion in this remake, growing our anticipated fan base even further. Lastly, including young actor Revolori with resonate with the younger generation, especially those tied to Marvel, as he is a fan favorite in that sphere. Thus, this combination would be an absolute recipe for success, especially since the action genre has the most successes in the 2020s thus far! If we move forward with this plan, we can easily be the next success!"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini Project 02: The Business of Show Business",
    "section": "",
    "text": "In this project, I will be utilizing data from the Internet Movie Database (IMDb) to discover what the next best movie is to produce. Before settling on my decision, I will dive into Hollywood’s history to identify key characteristics of successful movies, identify successful directors and actors, and examine some of Hollywood’s most famous flops. With all this information compiled together, I will pick a crew and a movie to remake, and then pitch the idea to the higher ups at my company."
  },
  {
    "objectID": "mp02.html#diving-into-the-data",
    "href": "mp02.html#diving-into-the-data",
    "title": "Mini Project 02: The Business of Show Business",
    "section": "",
    "text": "Firstly, I must download the data of Hollywood’s history into my rstudio. Since there were a sizable amount of people recorded in the industry, I decided to restrict my attention to people with at least two “known for” credits. This all can be seen in the folded code below:\n\n\nClick here to see how the data was downloaded\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\n\n\nget_imdb_file &lt;- function(fname){\n  BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n  fname_ext &lt;- paste0(fname, \".tsv.gz\")\n  if(!file.exists(fname_ext)){\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL, \n                  destfile = fname_ext)\n  }\n  as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\n\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\nRows: 771476 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (6): nconst, primaryName, birthYear, deathYear, primaryProfession, known...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\") \n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 4729628 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (8): tconst, titleType, primaryTitle, originalTitle, startYear, endYear,...\ndbl (1): isAdult\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\") \n\nRows: 8585773 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (4): tconst, parentTconst, seasonNumber, episodeNumber\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\") \n\nRows: 1490875 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): tconst\ndbl (2): averageRating, numVotes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\") \n\nRows: 10521826 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): tconst, directors, writers\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\n\nNow that I restricted the worker’s data set, I am curious to see how many obscure movies are in my data, as I want to avoid creating a low selling movie. Below is a chart that depicts the number of titles in comparison to their number of IMDb ratings.\n\nTITLE_RATINGS |&gt;\n    ggplot(aes(x=numVotes)) + \n    geom_histogram(bins=30) +\n    xlab(\"Number of IMDB Ratings\") + \n    ylab(\"Number of Titles\") + \n    ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\nHere, we can see the a majority of the movies in this data set have less than 100 ratings. To limit my data set further, I threw out any title that had less than 100 ratings. Looking at the quarterly ranges below, this shows that this drops about 75% of our data.\n\nTITLE_RATINGS |&gt;\n    pull(numVotes) |&gt;\n    quantile()\n\n     0%     25%     50%     75%    100% \n      5      11      26     100 2954275 \n\n\nSo, I performed this drop by performing the following code:\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\nWe now want to do the same for our “Title” tables, which can be seen below.\n\n\nClick here to see how this was done\n\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\n#| cache: false\n\n\nNow that our data has been significantly reduced, we can move on!\n\n\nAfter performing the glimpse function to examine each table, it was clear that some columns appeared as “character (string)” vectors, when they should be numeric. This is due to the null or N/A values that appears in the data sets. Since R cannot read these files as is, we must mutate the tables in a way that R can read them numerically. This is done with the following code:\n\n\nClick here to see how this was done\n\n\nlibrary(dplyr)\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  mutate(birthYear = as.numeric(birthYear),\n         deathYear = as.numeric(deathYear))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `birthYear = as.numeric(birthYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(isAdult = as.logical(isAdult),\n         endYear = as.numeric(endYear),\n         startYear = as.numeric(startYear),\n         runtimeMinutes = as.numeric(runtimeMinutes))\n\nWarning: There were 3 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `endYear = as.numeric(endYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n  mutate(seasonNumber = as.numeric(seasonNumber),\n         episodeNumber = as.numeric(episodeNumber))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `seasonNumber = as.numeric(seasonNumber)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\nNow that the data tables are cleaned up and ready to be used, I can begin to explore the data.\n\n\n\nFirstly, I want to find out how many movies are in our data set? How many TV series? And how many TV episodes?\n\n\nClick here to see the code\n\n\nNumber_of_Movies &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\") |&gt;\n  summarize(Number = n())\n\nprint(Number_of_Movies$Number)\n\n[1] 89305\n\n\n\nNumber_of_TVshows &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  summarize(Number = n())\n\nprint(Number_of_TVshows$Number)\n\n[1] 16742\n\n\n\nNumber_of_Episodes &lt;- TITLE_EPISODES |&gt;\n  summarize(Number = n())\n\nprint(Number_of_Episodes$Number)\n\n[1] 3026526\n\n\n\nAfter running the code above, I found that there were 89,305 movies, 16,742 TV shows, and 3,026,526 TV Episodes.\nSecondly, I want to know who the oldest person is in our data set?\nTo do this, I performed the following code:\n\noldest_person &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear)) |&gt;\n  arrange(birthYear) |&gt;\n  slice(1)\n\nprint(oldest_person)\n\n     nconst       primaryName birthYear deathYear primaryProfession\n1 nm0393833 William S. Hooser      1836        NA             actor\n                 knownForTitles\n1 tt0157755,tt0015357,tt0408145\n\n\nI quickly saw that the birth year of this person was 1836. So unless our oldest person is 188 years old, we need to put a restraint on how far back their birth year is, in case their death was not recorded. Since the oldest living person in the world is 116, the earliest possible birth year can be 1908. Many individuals came up for this birth year, however once I Google searched the ten oldest people in the world, none of them were on that list. I even saw that a few of these names did have deaths recorded on Google, but not on the database. Since there are only about 500,000 living individuals over the age of 100, which is only about .007% of the population, I decided to set my birth range to 100 years ago. Though someone may be slightly older than this and still alive, I thought this would be a safe bet to assume.\n\noldest_person &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear),\n         birthYear &gt;= 1924) |&gt;\n  arrange(birthYear) |&gt;\n  slice(1)\n\nprint(oldest_person)\n\n     nconst     primaryName birthYear deathYear           primaryProfession\n1 nm0001693 Eva Marie Saint      1924        NA actress,producer,soundtrack\n                           knownForTitles\n1 tt0047296,tt0053125,tt0348150,tt1837709\n\n\nThus, I found that Eva Marie Saint is the oldest and still alive person in our database. Her career spanned over eighty years and she won numerous awards for her works. She was even born close to New York, in Newark, NJ!\nThirdly, there is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. I want to discover what episode this is and what show it belongs to.\nTo do this, I restricted my data to only show me these restraints. This can be seen in the following code:\n\n\nClick here to see how this was done\n\n\nBest_Episode &lt;- TITLE_RATINGS |&gt; \n  filter(averageRating == 10,\n         numVotes &gt;= 200000)\n\n\nBest_E_series &lt;- TITLE_EPISODES |&gt;\n  filter(tconst == \"tt2301451\")\n\nprint(Best_E_series)\n\n     tconst parentTconst seasonNumber episodeNumber\n1 tt2301451    tt0903747            5            14\n\nBest_Episode_name &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt0903747\")\n\nprint(Best_Episode_name)\n\n     tconst titleType primaryTitle originalTitle isAdult startYear endYear\n1 tt0903747  tvSeries Breaking Bad  Breaking Bad   FALSE      2008    2013\n  runtimeMinutes               genres\n1             45 Crime,Drama,Thriller\n\n\n\nFrom doing the commands above, I found that the only episode that reached these ratings was episode 14 of season 5 in Breaking Bad. Looking this up on Google, I found that the title of the episode was “Ozymandias.” This episode is rated so highly due to the brilliance of how show runners depicted the main protagonist getting everything taken away from him. Pivotal events happen, including a main character’s demise which became a significant turning point for the narrative. Over a decade later, this episode is still raved and talked about by fans.\nFourthly, what four projects is the actor Mark Hamill most known for?\n\n\nClick here to see how this was done\n\n\nMark_Hamill &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\")\n\nprint(Mark_Hamill)\n\n     nconst primaryName birthYear deathYear       primaryProfession\n1 nm0000434 Mark Hamill        NA        NA actor,producer,director\n                           knownForTitles\n1 tt0076759,tt2527336,tt0080684,tt0086190\n\nMark_Hamill_Projects &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt0076759\") \n\nprint(Mark_Hamill_Projects)\n\n     tconst titleType                       primaryTitle originalTitle isAdult\n1 tt0076759     movie Star Wars: Episode IV - A New Hope     Star Wars   FALSE\n  startYear endYear runtimeMinutes                   genres\n1      1977      NA            121 Action,Adventure,Fantasy\n\nMark_Hamill_Projects &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt2527336\") \n\nprint(Mark_Hamill_Projects)\n\n[1] tconst         titleType      primaryTitle   originalTitle  isAdult       \n[6] startYear      endYear        runtimeMinutes genres        \n&lt;0 rows&gt; (or 0-length row.names)\n\nMark_Hamill_Projects &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt0086190\") \n\nprint(Mark_Hamill_Projects)\n\n     tconst titleType                               primaryTitle\n1 tt0086190     movie Star Wars: Episode VI - Return of the Jedi\n                               originalTitle isAdult startYear endYear\n1 Star Wars: Episode VI - Return of the Jedi   FALSE      1983      NA\n  runtimeMinutes                   genres\n1            131 Action,Adventure,Fantasy\n\nMark_Hamill_Projects &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt0080684\") \n\nprint(Mark_Hamill_Projects)\n\n     tconst titleType                                   primaryTitle\n1 tt0080684     movie Star Wars: Episode V - The Empire Strikes Back\n                                   originalTitle isAdult startYear endYear\n1 Star Wars: Episode V - The Empire Strikes Back   FALSE      1980      NA\n  runtimeMinutes                   genres\n1            124 Action,Adventure,Fantasy\n\n\n\nUsing the command above, I found that Mark Hamill was known for four Star Wars titles, including: Star Wars: Episode IV - A New Hope, Star Wars: Episode VIII - The Last Jedi, Star Wars: Episode V - The Empire Strikes Back, and Star Wards: Episode VI - Return of the Jedi.\nFifthly, I want to know what TV series, with more than 12 episodes, has the highest average rating?\nTo compute this, I did the following:\n\n\nClick here to see how this was done\n\n\nepisode_amount &lt;- TITLE_EPISODES |&gt;\n    group_by(parentTconst) |&gt;\n    summarise(total_episodes = n(), .groups = 'drop')\n\ntv_ratings &lt;- TITLE_BASICS |&gt;\n    filter(titleType == \"tvSeries\") |&gt;\n    inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n    inner_join(episode_amount, by = c(\"tconst\" = \"parentTconst\"))\n\n\nhighest_rating_series &lt;- tv_ratings |&gt;\n    filter(total_episodes &gt; 12) |&gt;\n    arrange(desc(averageRating)) |&gt;\n    slice(1)\n\nhighest_rating_series |&gt;\n    select(primaryTitle, total_episodes, averageRating)\n\n  primaryTitle total_episodes averageRating\n1  Craft Games            318           9.7\n\nprint(highest_rating_series)\n\n      tconst titleType primaryTitle originalTitle isAdult startYear endYear\n1 tt15613780  tvSeries  Craft Games   Craft Games   FALSE      2014      NA\n  runtimeMinutes                  genres averageRating numVotes total_episodes\n1             NA Adventure,Comedy,Family           9.7      151            318\n\n\n\nThus, I found out that Craft Games was the highest rated series with over 12 episodes, with an average rating of 9.7.\nLastly, I wanted to know more about the TV series Happy Days. The TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality.\nI want to know if it is true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\nTo do this, I must restrict my data set to only show me information about Happy Days. At first, I only restricted this by the name, but I quickly realized that there were numerous projects with the same name. Because of this, I also added the restrictions of the start year and end year of the series. By doing the following code, I find the necessary tconst that identifies the series. Then, I ran the following code to get the average rating of each season and compare them.\n\n\nClick here to see how this was done\n\n\nTITLE_BASICS |&gt;\n  filter(primaryTitle == \"Happy Days\",\n         startYear == \"1974\",\n         endYear == \"1984\") \n\n     tconst titleType primaryTitle originalTitle isAdult startYear endYear\n1 tt0070992  tvSeries   Happy Days    Happy Days   FALSE      1974    1984\n  runtimeMinutes              genres\n1             30 Comedy,Family,Music\n\n\n\nHappy_Days &lt;- TITLE_EPISODES |&gt;\n  filter(parentTconst == \"tt0070992\")\n\nHappy_Days_Scores &lt;- inner_join(Happy_Days, TITLE_RATINGS, join_by(tconst))\n\n\nHappy_Days_Seasons &lt;- Happy_Days_Scores |&gt;\n  group_by(seasonNumber) |&gt;\n  summarize(averagescore = mean(averageRating, na.rm = TRUE)) |&gt;\n  arrange(seasonNumber)\n\n\n\nlibrary(DT)\ndatatable(Happy_Days_Seasons) \n\n\n\n\n\nBy looking at the table above, we can see that the scores did in fact drop towards the later seasons, with Season 8 being the lowest scoring season. For the first half of the show, they average at a high 7 rating, but a downward decline began after season 3. Season 11, which was their final season, inevitably scored back in the 7 range, but the score still did not beat any from the first four seasons.\n\n\n\nFor my success metric, I decided to create a scale that assesses the total popularity of a work. To do this, I decided to multiply the average rating that it got by the number of votes. My logic is that a work cannot be successful without each of these metrics: a low scoring average with a lot of votes means that the movie flopped and a high scoring average with no votes means that the movie is too obscure. However, if a movie is averaged highly and has a lot of votes, this means that it is widely known and highly regarded. Thus, I believe that this is an accurate way to measure success.\nTo test my theory, I will run some tests to see if it holds up against actually highly regarded movies and some of the more obscure ones in Hollywood.\nFirstly, I will choose the top ten movies on my metric and confirm that they were box office successes.\n\n\nClick here to see how I filtered my data to show this\n\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  mutate(popularity = averageRating*numVotes) \n\nmovie_ratings &lt;- inner_join(TITLE_BASICS, TITLE_RATINGS, join_by(tconst))\n\nmovie_ratings &lt;- movie_ratings |&gt; select(tconst, titleType, primaryTitle, genres, startYear, averageRating, numVotes, popularity)\n\nmovie_ratings &lt;- movie_ratings |&gt; \n  filter(titleType == \"movie\") |&gt;\n  arrange(desc(popularity))\n\n\nTop_ten &lt;- movie_ratings |&gt;\n  slice(1:10)\n\nprint(Top_ten)\n\n      tconst titleType                                      primaryTitle\n1  tt0111161     movie                          The Shawshank Redemption\n2  tt0468569     movie                                   The Dark Knight\n3  tt1375666     movie                                         Inception\n4  tt0137523     movie                                        Fight Club\n5  tt0109830     movie                                      Forrest Gump\n6  tt0110912     movie                                      Pulp Fiction\n7  tt0816692     movie                                      Interstellar\n8  tt0068646     movie                                     The Godfather\n9  tt0120737     movie The Lord of the Rings: The Fellowship of the Ring\n10 tt0133093     movie                                        The Matrix\n                    genres startYear averageRating numVotes popularity\n1                    Drama      1994           9.3  2954275   27474758\n2       Action,Crime,Drama      2008           9.0  2935734   26421606\n3  Action,Adventure,Sci-Fi      2010           8.8  2605397   22927494\n4                    Drama      1999           8.8  2386403   21000346\n5            Drama,Romance      1994           8.8  2311754   20343435\n6              Crime,Drama      1994           8.9  2268830   20192587\n7   Adventure,Drama,Sci-Fi      2014           8.7  2179321   18960093\n8              Crime,Drama      1972           9.2  2059556   18947915\n9   Action,Adventure,Drama      2001           8.9  2052301   18265479\n10           Action,Sci-Fi      1999           8.7  2097695   18249947\n\n\n\nThe following are the top ten successes on my list:\n\nThe Shawshank Redemption\nThe Dark Knight\nFight Club\nForrest Gump\nPulp Fiction\nThe Godfather\nInterstellar\nThe Lord of the Rings: The Fellowship of the Ring\nThe Matrix\nThe Lord of the Rings: The Return of the King\n\nAll of which were MAJOR successes in the box office. So far, this success metric is looking good!\nSecondly, I will do the same as before, but I will choose the five lowest movies on my list and ensure they were of low quality.\n\n\nClick here to see how I filtered my data to show this\n\n\nmovie_ratings &lt;- movie_ratings |&gt; \n  filter(titleType == \"movie\") |&gt;\n  arrange(popularity)\n\n\nbottom_five &lt;- movie_ratings |&gt;\n  slice(1:5)\n\nprint(bottom_five)\n\n     tconst titleType                             primaryTitle\n1 tt0498734     movie                 Zwischen Glück und Krone\n2 tt0386134     movie                        Yubiley prokurora\n3 tt0309422     movie               The Case He Couldn't Crack\n4 tt1773029     movie                                 Rockland\n5 tt1666348     movie Vixen Highway 2006: It Came from Uranus!\n                  genres startYear averageRating numVotes popularity\n1            Documentary      1959           1.2      103      123.6\n2                 Comedy      2003           1.1      128      140.8\n3         Crime,Thriller      1981           1.0      153      153.0\n4          Comedy,Family      2010           1.5      102      153.0\n5 Action,Horror,Thriller      2010           1.1      145      159.5\n\n\n\nThe following are the 5 lowest rated on my list:\n\nZwischen Glück und Krone\nYubiley prokurora\nThe Case He Couldn’t Crack\n18’ler Takimi\nParentesi tonde\n\nThese all proved to be more obscure and poorly-rated movies. Therefore, the metric does work both ways.\nThirdly, now I will choose a prestige actor and confirm that they have many projects that score highly on my metric. For this, I decided to look into Tom Hanks and Leonardo DiCaprio, as I believe they are two major actors in the industry currently.\n\n\nClick here to see how I filtered my data to show this\n\n\nlibrary(tidyverse)\n\nLeo &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Leonardo DiCaprio\") |&gt;\n  separate_longer_delim(knownForTitles, \",\")\n\n\ncolnames(Leo)[colnames(Leo) == \"knownForTitles\"] &lt;- \"tconst\"\n\nLeo &lt;- inner_join(Leo, movie_ratings, join_by(tconst))\n\nHanks &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Tom Hanks\") |&gt;\n  separate_longer_delim(knownForTitles, \",\")\n\n\ncolnames(Hanks)[colnames(Hanks) == \"knownForTitles\"] &lt;- \"tconst\"\n\nHanks &lt;- inner_join(Hanks, movie_ratings, join_by(tconst)) \n\nLeo_Hanks &lt;- bind_rows(Leo, Hanks)\n\nLeo_Hanks2 &lt;- Leo_Hanks |&gt; select(primaryName, primaryTitle, averageRating, numVotes, popularity)\n\n\n\ndatatable(Leo_Hanks2)\n\n\n\n\n\nHere, we can see that the movies given back by my success metric are in fact box office successes. Movies such as Titanic, The Wolf of Wall Street, Cast Away, and Forrest Gump are all well-known movies that were incredibly well-rated. Thus, the metric still holds.\nFourthly, for my final test, I wanted to compare my metric with the highest-grossing movies of all time. After searching this data on Google, I found that some of the highest grossing movies of all time is Titanic with $2,264,750,694, Avatar: The Way of Water with $2,320,250,281, and Avatar with $2,923,706,026.\n\n\nClick here to see how I filtered my data to show this\n\n\nHighest_Grossing &lt;- data.frame(\n  tconst = c(\"tt0499549\", \"tt4154796\", \"tt1630029\", \"tt0120338\", \"tt2488496\"),\n  LifetimeGross = c(\"$2,923,706,026\", \"$2,799,439,100\", \"$2,320,250,281\", \"$2,264,750,694\", \"$2,071,310,218\")\n)\n\n\nHigh_Grossing &lt;- inner_join(movie_ratings, Highest_Grossing, join_by(tconst))\n\n\nThe chart below depicts three of the above movies. It is clear to see that the popularity score correlates with their gross, as both are high. This solidifies my success metric even further.\n\ndatatable(High_Grossing)\n\n\n\n\n\nLastly, now that my success metric has been solidified, I need to restrict my data set to only show me “successes.” To do this, I must distinguish a number that once a movie surpasses it in its “popularity” scale, then it is deemed a success. The number I decided to pick is 5,000,000, since I feel as though every great movie should be able to surpass that. Then, I reduced my data set to fit that benchmark using the following code.\n\nsolid_or_better &lt;- movie_ratings |&gt;\n  filter(popularity &gt;= 5000000)\n\n\n\n\nFirstly, I want to discover which genre had the most “successes” in each decade.\n\n\nClick here to see the code\n\n\nsolid_or_better &lt;- solid_or_better|&gt;\n  arrange(startYear)\n\n\nsolid_or_better &lt;- solid_or_better |&gt; separate_longer_delim(genres, \",\") \n\n\ndecades &lt;- solid_or_better |&gt;\n  count(genres)\n\n\nforties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1940 & startYear &lt;= 1949) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\nfifties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1950 & startYear &lt;= 1959) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\n\nsixties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1960 & startYear &lt;= 1969) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\n\nseventies &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1970 & startYear &lt;= 1979) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\neighties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1980 & startYear &lt;= 1989) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\nnineties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1990 & startYear &lt;= 1999) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\nthousands &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2000 & startYear &lt;= 2009) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\ntens &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2010 & startYear &lt;= 2019) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\ntwenties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2020 & startYear &lt;= 2029) |&gt;\n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1)\n\n\nprint(forties)\n\n  genres n\n1  Drama 1\n\nprint(fifties)\n\n  genres n\n1  Crime 1\n\nprint(sixties)\n\n     genres n\n1 Adventure 2\n\nprint(seventies)\n\n  genres n\n1  Drama 7\n\nprint(eighties)\n\n  genres n\n1 Action 8\n\nprint(nineties)\n\n  genres  n\n1  Drama 25\n\nprint(thousands)\n\n     genres  n\n1 Adventure 30\n\nprint(tens)\n\n  genres  n\n1 Action 21\n\nprint(twenties)\n\n  genres n\n1 Action 3\n\n\n\nThus, the most successes per decade is as follows:\n\n40s: Drama (1)\n50s: Crime (1)\n60s: Adventure (2)\n70s: Drama (7)\n80s: Action (8)\n90s: Drama (25)\n00s: Drama/Adventure (30)\n10s: Action (21)\n20s: Action (2)\n\nSecondly, to see what genre had the most consistent “successes” and which one used to reliably produce “successes,” but has fallen off, I decided to create a data table. This way, all of the successful movies per genre throughout the decades can be easily visualized.\n\n\nClick here to see how the table was made\n\n\nforties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1940 & startYear &lt;= 1949) |&gt;\n  count(genres) \n\nfifties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1950 & startYear &lt;= 1959) |&gt;\n  count(genres) \n\n\nsixties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1960 & startYear &lt;= 1969) |&gt;\n  count(genres) \n\n\nseventies &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1970 & startYear &lt;= 1979) |&gt;\n  count(genres) \n\n\n\neighties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1980 & startYear &lt;= 1989) |&gt;\n  count(genres) \n\nnineties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1990 & startYear &lt;= 1999) |&gt;\n  count(genres) \n\n\nthousands &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2000 & startYear &lt;= 2009) |&gt;\n  count(genres) \n\n\ntens &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2010 & startYear &lt;= 2019) |&gt;\n  count(genres) \n\ntwenties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2020 & startYear &lt;= 2029) |&gt;\n  count(genres) \n\ndecades_total &lt;- solid_or_better |&gt;\n  count(genres) \n\ncolnames(forties)[colnames(forties) == \"n\"] &lt;- \"40s\"\ncolnames(fifties)[colnames(fifties) == \"n\"] &lt;- \"50s\"\ncolnames(sixties)[colnames(sixties) == \"n\"] &lt;- \"60s\"\ncolnames(seventies)[colnames(seventies) == \"n\"] &lt;- \"70s\"\ncolnames(eighties)[colnames(eighties) == \"n\"] &lt;- \"80s\"\ncolnames(nineties)[colnames(nineties) == \"n\"] &lt;- \"90s\"\ncolnames(thousands)[colnames(thousands) == \"n\"] &lt;- \"00s\"\ncolnames(tens)[colnames(tens) == \"n\"] &lt;- \"10s\"\ncolnames(twenties)[colnames(twenties) == \"n\"] &lt;- \"20s\"\ncolnames(decades_total)[colnames(decades_total) == \"n\"] &lt;- \"total\"\n\ndecades &lt;- full_join(forties, fifties, join_by(genres))\ndecades &lt;- full_join(decades, sixties, join_by(genres))\ndecades &lt;- full_join(decades, seventies, join_by(genres))\ndecades &lt;- full_join(decades, eighties, join_by(genres))\ndecades &lt;- full_join(decades, nineties, join_by(genres))\ndecades &lt;- full_join(decades, thousands, join_by(genres))\ndecades &lt;- full_join(decades, tens, join_by(genres))\ndecades &lt;- full_join(decades, twenties, join_by(genres))\ndecades &lt;- full_join(decades, decades_total, join_by(genres))\n\n\nBelow is a table that shows all of the genres throughout the years.\n\nlibrary(DT)\ndatatable(decades) \n\n\n\n\n\nLooking at the chart, it is clear to see that the genre with the most consistent successes each decade is Drama, as it is the only genre that had a least one success every decade from the forties until now. It also has gotten more successes recently, with its height being in the 2000s.\nThe genre that used to reliably produce successes, but has fallen out of favor is Crime, as it had many success from the 1970s to 2000s, entirely fell off in the 2010s with only three successes. However, since the 2000s, the Drama genre definitely fell off the most, going from 30 success to 14 to 3 per decade.\nThirdly, I want to know what genre has produced the most successes since 2010. By looking at the chart above, it is clear to see the genre with the most successes since 2010 is Action with 24 successes. The Adventure genre is closely behind that, with 21 successes.\nHowever, I want to know if this genre has only produced the most successes because it genuinely has the highest rating, or if there were just an abundance of Action movies that have been created since then to contribute to the score. To do that, I used the following code:\n\n\nClick here to see the code\n\n\ndecade2010_pop &lt;- movie_ratings |&gt;\n  filter(startYear &gt;= 2010 & startYear &lt;= 2019) |&gt;\n  separate_longer_delim(genres, \",\") |&gt; \n  group_by(genres) |&gt;\n  summarize(totalPopularity = sum(popularity)) |&gt;\n  arrange(desc(totalPopularity)) |&gt;\n  slice(1:3)\n\n\ndecade2010_amount &lt;- movie_ratings |&gt;\n  filter(startYear &gt;= 2010 & startYear &lt;= 2019) |&gt;\n  separate_longer_delim(genres, \",\") |&gt; \n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1:9) \n\n\n\nprint(decade2010_pop)\n\n# A tibble: 3 × 2\n  genres    totalPopularity\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Action         538827123.\n2 Drama          534760306.\n3 Adventure      415932283.\n\nprint(decade2010_amount)\n\n       genres    n\n1       Drama 3839\n2      Comedy 2259\n3      Action 1157\n4    Thriller 1074\n5      Horror  959\n6 Documentary  958\n7     Romance  932\n8       Crime  823\n9   Adventure  608\n\n\nHere, we can see that it ranked first on the total popularity list, but third on the amount of movies list. This shows that while this genre produces highly-rated movies, there is also a good amount of them being made. However, the drama genre is the most produced and is still below the action genre on total popularity, so this shows that each movie is more successful than others.\nLastly, the genre that has become more popular in recent years is definitely the Action genre. It had its first successful movie in the 1970s, while ten other genres had earlier successes, and still skyrocketed in later decades to get 66 total successes. Only two genres are ahead of the action genre, which is drama with 86 successes and adventure with 67 successes. However, since the 2000s, it has had 50 successes, which is only one behind the top spot for successes in that time frame, which is the adventure genre.\nBased on my findings, I decided to choose an Action genre, as while it is the third most produced in the last decade, it had the highest popularity score. It surpasses the Drama genre on the list, which is big considering that the drama genre has 20 more successes than the action genre. This shows me that the drama genre only have this high of a popularity due to successes that they had, which means each success had to average at the lower end of the success scale. Though the adventure genre also did very well on both of these lists, I decided not to choose it as I fear it may begin to get overplayed, much like the Drama genre. However, the Action genre seems to be an up-and-coming one, as it already has more successes than the Adventure genre in the 2020s, and the same amount as the Drama genre. Thus, I believe this is our best bet for a successful movie.\n\n\n\nNow that I have my target genre, I want to identify a few actors and one director to really anchor my project. To create a strong team, I want one prestigious actor who has many successes in many genres, and preferably at least one major success in the action genre. With this major name, I hope to bring in their major following and their broad skill set to perfect the movie. For my second actor, I want to pick someone who has been apart of a highly rated movie, but is still not widely recognized yet. I want someone who is up-and-coming, that can build a strong profile around being in this movie. Then, the connection with these two main leads should bring good publicity.\nFor my prestige actor, I want someone who has been in the industry for a good amount of time and that is older with some experience. Thus, I decided that I wanted my actor to be at least 40, but no older than 75.\n\n\nClick here to see how I filtered my data to show this\n\n\nalive_actors &lt;- NAME_BASICS |&gt;\n  filter(birthYear &gt;= 1949 & birthYear &lt;= 1984) |&gt;\n  filter(is.na(deathYear)) |&gt;\n  separate_longer_delim(primaryProfession, \",\") |&gt;\n  filter(primaryProfession == \"actor\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") \n\nsolid_or_better2 &lt;- movie_ratings |&gt;\n  filter(popularity &gt;= 5000000)\n\nsuccessful_movies &lt;- solid_or_better2 |&gt;\n  select(tconst, genres, popularity) \n\ncolnames(successful_movies)[colnames(successful_movies) == \"tconst\"] &lt;- \"knownForTitles\"\n\n\nsuccessful_actors &lt;- full_join(alive_actors, successful_movies, join_by(knownForTitles))\n\nsuccessful_actors &lt;- successful_actors |&gt;\n  arrange(desc(popularity))\n\nsuccessful_actors_average &lt;-successful_actors |&gt;\n  select(primaryName, popularity) |&gt;\n  group_by(primaryName) |&gt;\n  summarize(totalpopularity = sum(popularity, na.rm = TRUE)) |&gt;\n  arrange(desc(totalpopularity))\n\n\n\ndatatable(successful_actors_average)\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html\n\n\n\n\n\n\nOne by one, I decided to go down the list of people to see what their top works were. This can be seen in the code below.\n\n\nClick here to see to see the code\n\n\nOrlando_Bloom &lt;- successful_actors |&gt;\n  filter(primaryName == \"Orlando Bloom\")\n\nHans_Zimmer &lt;- successful_actors |&gt;\n  filter(primaryName == \"Orlando Bloom\")\n\nHugo_Weaving &lt;- successful_actors |&gt;\n  filter(primaryName == \"Hugo Weaving\")\n\nGary_Rizzo &lt;- successful_actors |&gt;\n  filter(primaryName == \"Gary A. Rizzo\")\n\nLeonardo_DiCaprio &lt;- successful_actors |&gt;\n  filter(primaryName == \"Leonardo DiCaprio\")\n\n\nI decided against Orlando Bloom, since he had 4 successful movies, three of which were apart of the Lord of the Rings franchise, and one Pirates of the Caribbean. I want a more diverse actor, rather than someone who mainly dominates the same universe.\nHugo Weaving was similar, with three Lord of the Rings movies and one movie titled “V for Vendetta.” Rizzo would have been perfect… until I researched and found out he was the music producer on these projects, and not an actor. Same with Zimmer.\nFinally, I got to my seventh person on the list, Leonardo DiCaprio. Not only did his top movies dominate different genres, including Crime, Drama, Thriller, Romance, Comedy, etc., his top movie was an Action/Adventure mixed genre. Also scoring highest on his list on the popularity scale, Inception became the fourth highest-grossing movie of 2010, with DiCaprio as the star of the movie. Thus, it was clear that he was my first pick actor.\nNext, I wanted to find someone younger and more up-and-coming. Thus, I chose to restrict the age gap to someone less than 30 years old. I did this with the following code:\n\n\nClick here to see to see the code\n\n\nalive_actors_young &lt;- NAME_BASICS |&gt;\n  filter(birthYear &gt;= 1995 & birthYear &lt;= 2024) |&gt;\n  filter(is.na(deathYear)) |&gt;\n  separate_longer_delim(primaryProfession, \",\") |&gt;\n  filter(primaryProfession == \"actor\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") \n\nsolid_or_better2 &lt;- movie_ratings |&gt;\n  filter(popularity &gt;= 5000000)\n\nsuccessful_movies &lt;- solid_or_better2 |&gt;\n  select(tconst, genres, popularity) \n\ncolnames(successful_movies)[colnames(successful_movies) == \"tconst\"] &lt;- \"knownForTitles\"\n\n\nsuccessful_actors_young &lt;- full_join(alive_actors_young, successful_movies, join_by(knownForTitles))\n\nsuccessful_actors_young &lt;- successful_actors_young |&gt;\n  arrange(desc(popularity))\n\nsuccessful_actors_average_y &lt;-successful_actors_young |&gt;\n  select(primaryName, popularity) |&gt;\n  group_by(primaryName) |&gt;\n  summarize(totalpopularity = sum(popularity, na.rm = TRUE)) |&gt;\n  arrange(desc(totalpopularity))\n\n\nTony_Revolori &lt;- successful_actors_young |&gt;\n  filter(primaryName == \"Tony Revolori\")\n\n\nI quickly decided upon Tony Revolori, as he was high on the list and was in a Spiderman movie recently that did fairly well. When I looked him up, he had some breakout awards and is expected to become a lot bigger in the upcoming years, especially as the projects he is currently working on begin to roll out.\nAt this point of the project, I realized that I had a slight oversight and did not include any female actresses into the mix. So, I decided to pick another younger actor to add to my team.\n\n\nClick here to see to see the code\n\n\nalive_actress_young &lt;- NAME_BASICS |&gt;\n  filter(birthYear &gt;= 1995 & birthYear &lt;= 2024) |&gt;\n  filter(is.na(deathYear)) |&gt;\n  separate_longer_delim(primaryProfession, \",\") |&gt;\n  filter(primaryProfession == \"actress\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") \n\n\nsuccessful_actress_young &lt;- full_join(alive_actress_young, successful_movies, join_by(knownForTitles))\n\nsuccessful_actress_average_y &lt;-successful_actress_young |&gt;\n  select(primaryName, popularity) |&gt;\n  group_by(primaryName) |&gt;\n  summarize(totalpopularity = sum(popularity, na.rm = TRUE)) |&gt;\n  arrange(desc(totalpopularity))\n\n\n\nTaylor_Geare &lt;- successful_actress_young |&gt;\n  filter(primaryName == \"Taylor Geare\")\n\n\nBy running this previous code, I found that Taylor Geare would be a perfect pick for the movie. Not only has she won one award and been in a few works recently, I discovered that she was also in Inception with Leonardo DiCaprio! At the time, she was five years old and she played Philipa, who was the daughter of DiCaprio’s character. I believe that this pairing can bring major publicity to our work, as fans of Inception will get to see this duo back together, especially in a way that Geare will be able to display her skills as an adult.\nNow, I must find a director for my movie. Using the same technique as above, I began my search.\n\n\nClick here to see to see the code\n\n\nalive_directors &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear)) |&gt;\n  separate_longer_delim(primaryProfession, \",\") |&gt;\n  filter(primaryProfession == \"director\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") \n\nsuccessful_directors &lt;- full_join(alive_directors, successful_movies, join_by(knownForTitles))\n\n\nsuccessful_directors2 &lt;- successful_directors |&gt;\n  group_by(primaryName) |&gt;\n  summarize(totalpopularity = sum(popularity, na.rm = TRUE)) |&gt;\n  arrange(desc(totalpopularity))\n\nWally_Pfister &lt;- successful_directors |&gt;\n  filter(primaryName == \"Wally Pfister\")\n\n\n\ndatatable(successful_directors2)\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html\n\n\n\n\n\n\nComing in second on the list, Wally Pfister easily proved to be the strongest candidate for this movie. Not only did he score incredibly highly on this list, he also worked on major movies such as The Dark Knight, The Prestige, Transcendence, and much more. However, most notably, he also worked on Inception with Leonardo DiCaprio and Taylor Geare. A picture of DiCaprio and Pfister can be seen below.\n\nThus, not only have they all worked together in the past so they know each other/have good chemistry, they were also able to create a masterpiece of a movie before in the same genre we are aspiring to. This could stir a lot of good media, as it not seen very often that three people from a very successful movie team up again to work on a remake of another successful movie. Thus, I feel very confident about the potential team I have put together.\n\n\n\nNow, I must find a classic movie in this genre to remake. I want to find a movie that has not been remade in the past 25 years, has a large number of IMDb ratings, has a high average rating, and has a fan base that wants a remake to happen. To do this, I must first filter my data sets to give me a movie that fits these first three restraints.\n\n\nClick here to see to see the code\n\n\nremakemovie &lt;- movie_ratings |&gt;\n  filter(startYear &lt; 1999) |&gt;\n  filter(averageRating &gt;  8) |&gt;\n  filter(numVotes &gt; 20000) |&gt;\n  arrange(desc(popularity))\n\n\n\ndatatable(remakemovie)\n\n\n\n\n\nI want to look at what the top movies are in the action genre, however, I also want to stay away from remaking any Star Wars movies. Thus, coming in at number 14, I debating on remaking Léon: The Professional, however, when I looked up if fans wanted a remake, they seemed entirely opposed. Many fans believe the film is perfect as is and it would not do any justice for it to be remade today. Thus, I decided to keep looking.\nAt number 31, I found Die Hard, a movie made in 1988 about an NYPD cop that has to take matters into his own hands when a group of robbers take control of the building he is in, holding everyone hostage except for him.\nWhen I looked up if fans would want a remake of Die Hard, numerous articles came up showing great approval for this. Some of these are linked below:\n\nBleeding Cool News\nMedium\n\nThus, a Die Hard remake is the best bet for a successful action movie.\nHowever, before we move further, our legal department must reach out to certain members of the original movie production to secure the rights for the project. So, let’s find out who is still alive from the original.\n\n\nClick here to see to see the code\n\n\nDie_Hard &lt;- NAME_BASICS |&gt;\n  separate_longer_delim(knownForTitles, \",\") |&gt;\n  filter(knownForTitles == \"tt0083658\") |&gt;\n  filter(is.na(deathYear))\n\nprint(Die_Hard)\n\n       nconst           primaryName birthYear deathYear\n1   nm0000435          Daryl Hannah      1960        NA\n2   nm0000631          Ridley Scott      1937        NA\n3   nm0000707            Sean Young      1959        NA\n4   nm0001026        Joanna Cassidy        NA        NA\n5   nm0001579    Edward James Olmos      1947        NA\n6   nm0003256          Joshua Pines        NA        NA\n7   nm0003666         Debbie Denise        NA        NA\n8   nm0007072       Katherine Haber      1944        NA\n9   nm0022370          Vickie Alper        NA        NA\n10  nm0029959            Tim Angulo        NA        NA\n11  nm0045630    Michael Backauskas        NA        NA\n12  nm0047460      Robert D. Bailey        NA        NA\n13  nm0048193            Beau Baker      1953        NA\n14  nm0048396             Don Baker        NA        NA\n15  nm0049792         Peter Baldock      1949        NA\n16  nm0081795   William Biggerstaff        NA        NA\n17  nm0099156     Christian Boudman        NA        NA\n18  nm0114926       Winnie D. Brown        NA        NA\n19  nm0123746            Tom Burton        NA        NA\n20  nm0137698      Elizabeth Carlon        NA        NA\n21  nm0164425          Robert Clark        NA        NA\n22  nm0172782           Alan Collis        NA        NA\n23  nm0173645            Gary Combs      1935        NA\n24  nm0180108        Peter Cornberg        NA        NA\n25  nm0190038           Eugene Crum        NA        NA\n26  nm0193345           Greg Curtis        NA        NA\n27  nm0199344          Stephen Dane        NA        NA\n28  nm0203343 Howard Brady Davidson        NA        NA\n29  nm0213057           Lisa Deaner        NA        NA\n30  nm0214303        Michael Deeley      1932        NA\n31  nm0218540     Carolyn DeMirjian        NA        NA\n32  nm0220984        Linda DeScenna      1949        NA\n33  nm0238734           David Dryer      1943        NA\n34  nm0244956            Syd Dutton      1944        NA\n35  nm0252914              Bud Elam        NA        NA\n36  nm0266684       Hampton Fancher      1938        NA\n37  nm0270670         Jane Feinberg        NA        NA\n38  nm0274928      Michael Ferriter        NA        NA\n39  nm0280029      William E. Fitch        NA        NA\n40  nm0281551        Linda Fleisher        NA        NA\n41  nm0283845           Stephen Fog        NA        NA\n42  nm0285893          Terence Ford      1945        NA\n43  nm0290519          Chris Franco        NA        NA\n44  nm0292476       Terry D. Frazee        NA        NA\n45  nm0302091          Steve Galich        NA        NA\n46  nm0302406         Joe Gallagher      1953        NA\n47  nm0303421        Kurt P. Galvao        NA        NA\n48  nm0312920         Michael Genne        NA        NA\n49  nm0325242        Joyce Goldberg        NA        NA\n50  nm0333849         David Grafton        NA        NA\n51  nm0341454         Cary Griffith        NA        NA\n52  nm0356040           Robert Hall        NA        NA\n53  nm0362046   David R. Hardberger      1948        NA\n54  nm0362257          Alan Harding        NA        NA\n55  nm0367242   Graham V. Hartstone      1944        NA\n56  nm0369220          Donald Hauer        NA        NA\n57  nm0372264            Les Healey      1950        NA\n58  nm0381500            Linda Hess        NA        NA\n59  nm0384603       Richard L. Hill        NA        NA\n60  nm0390897  Richard E. Hollander        NA        NA\n61  nm0394803           Bob E. Horn        NA        NA\n62  nm0404527   Gillian L. Hutshing        NA        NA\n63  nm0438325        Michael Kaplan        NA        NA\n64  nm0452191       Madeleine Klein      1946        NA\n65  nm0453139           Crit Killen        NA        NA\n66  nm0461802       Michael Knutsen        NA        NA\n67  nm0487514         James Lapidus        NA        NA\n68  nm0507809        Terry E. Lewis        NA        NA\n69  nm0513924          Marci Liroff        NA        NA\n70  nm0518430        Basil Lombardo        NA        NA\n71  nm0519436          Ronald Longo        NA        NA\n72  nm0523370       Stephanie Lowry        NA        NA\n73  nm0560094        Linda Matthews        NA        NA\n74  nm0570488            Tim McHugh        NA        NA\n75  nm0573488    Gregory L. McMurry      1952        NA\n76  nm0590114         Michael Mills        NA        NA\n77  nm0616694          Donald Myers        NA        NA\n78  nm0624443          Tony Negrete        NA        NA\n79  nm0640384         John O'Connor        NA        NA\n80  nm0649687    James F. Orendorff        NA        NA\n81  nm0655669       Shirley Padgett        NA        NA\n82  nm0672065         Peter Pennell      1939        NA\n83  nm0672459    David Webb Peoples      1940        NA\n84  nm0689279     Thomas R. Polizzi        NA        NA\n85  nm0689332   George Polkinghorne        NA        NA\n86  nm0689503        Tama Takahashi        NA        NA\n87  nm0694138           Ivor Powell        NA        NA\n88  nm0703436        David Q. Quick        NA        NA\n89  nm0709588          Gary Randall        NA        NA\n90  nm0711047           Karen Rasch        NA        NA\n91  nm0722541    Victoria E. Rhodes        NA        NA\n92  nm0728110        Richard Rippel        NA        NA\n93  nm0745076     Jonathan Rothbart        NA        NA\n94  nm0747438     Thomas L. Roysden      1944        NA\n95  nm0761836     William Sanderson      1944        NA\n96  nm0768102          Fumi Mashimo        NA        NA\n97  nm0769731       Steve Schaeffer        NA        NA\n98  nm0770435          John Scheele        NA        NA\n99  nm0774039       Suzie Schneider        NA        NA\n100 nm0775562 Richard Peter Schroer        NA        NA\n101 nm0778771     John A. Scott III        NA        NA\n102 nm0779860             Tom Scott      1948        NA\n103 nm0791107    Victor A. Shelehov        NA        NA\n104 nm0794135        Arthur Shippee      1948        NA\n105 nm0811435       David L. Snyder        NA        NA\n106 nm0816170         Tom Southwell        NA        NA\n                                          primaryProfession knownForTitles\n1                                 actress,producer,director      tt0083658\n2                     producer,director,production_designer      tt0083658\n3                            actress,miscellaneous,director      tt0083658\n4                            actress,producer,miscellaneous      tt0083658\n5                                   actor,producer,director      tt0083658\n6              visual_effects,editorial_department,director      tt0083658\n7                     visual_effects,producer,miscellaneous      tt0083658\n8                            miscellaneous,producer,actress      tt0083658\n9                                             miscellaneous      tt0083658\n10         visual_effects,cinematographer,camera_department      tt0083658\n11               visual_effects,editorial_department,editor      tt0083658\n12                           visual_effects,director,writer      tt0083658\n13                     sound_department,actor,miscellaneous      tt0083658\n14                visual_effects,camera_department,director      tt0083658\n15              sound_department,editorial_department,actor      tt0083658\n16                             art_department,miscellaneous      tt0083658\n17        visual_effects,miscellaneous,editorial_department      tt0083658\n18                      costume_department,costume_designer      tt0083658\n19                   director,producer,animation_department      tt0083658\n20                                           visual_effects      tt0083658\n21        art_department,special_effects,make_up_department      tt0083658\n22                producer,production_manager,miscellaneous      tt0083658\n23                          stunts,assistant_director,actor      tt0083658\n24           production_manager,assistant_director,producer      tt0083658\n25                                          special_effects      tt0083658\n26                           special_effects,art_department      tt0083658\n27          art_department,art_director,production_designer      tt0083658\n28                                transportation_department      tt0083658\n29                                           visual_effects      tt0083658\n30                  sound_department,producer,miscellaneous      tt0083658\n31                                                  actress      tt0083658\n32                set_decorator,production_designer,actress      tt0083658\n33                         visual_effects,camera_department      tt0083658\n34                    visual_effects,special_effects,writer      tt0083658\n35                         visual_effects,camera_department      tt0083658\n36                                    actor,writer,director      tt0083658\n37             casting_director,casting_department,producer      tt0083658\n38      visual_effects,special_effects,editorial_department      tt0083658\n39                                        camera_department      tt0083658\n40                                           visual_effects      tt0083658\n41                                           visual_effects      tt0083658\n42               actor,assistant_director,camera_department      tt0083658\n43                                        camera_department      tt0083658\n44                                          special_effects      tt0083658\n45            special_effects,visual_effects,art_department      tt0083658\n46   sound_department,editorial_department,music_department      tt0083658\n47              producer,editorial_department,miscellaneous      tt0083658\n48                  camera_department,cinematographer,actor      tt0083658\n49                             visual_effects,miscellaneous      tt0083658\n50                                           visual_effects      tt0083658\n51                                        camera_department      tt0083658\n52                                           visual_effects      tt0083658\n53         visual_effects,camera_department,cinematographer      tt0083658\n54                         visual_effects,camera_department      tt0083658\n55                    sound_department,editorial_department      tt0083658\n56           assistant_director,production_manager,producer      tt0083658\n57             editor,editorial_department,sound_department      tt0083658\n58                miscellaneous,producer,production_manager      tt0083658\n59                           special_effects,art_department      tt0083658\n60        visual_effects,miscellaneous,animation_department      tt0083658\n61                      costume_department,costume_designer      tt0083658\n62                   editor,music_department,visual_effects      tt0083658\n63                costume_designer,costume_department,actor      tt0083658\n64                             miscellaneous,actress,stunts      tt0083658\n65            special_effects,art_department,visual_effects      tt0083658\n66                                            miscellaneous      tt0083658\n67        costume_designer,costume_department,miscellaneous      tt0083658\n68                                           art_department      tt0083658\n69                  casting_director,producer,miscellaneous      tt0083658\n70                                           art_department      tt0083658\n71                                           visual_effects      tt0083658\n72   music_department,sound_department,editorial_department      tt0083658\n73                      costume_department,costume_designer      tt0083658\n74                         visual_effects,producer,director      tt0083658\n75        visual_effects,miscellaneous,animation_department      tt0083658\n76                 make_up_department,actor,special_effects      tt0083658\n77                                          special_effects      tt0083658\n78                                         sound_department      tt0083658\n79     camera_department,cinematographer,assistant_director      tt0083658\n80                                           art_department      tt0083658\n81                                       make_up_department      tt0083658\n82                                         sound_department      tt0083658\n83                                   writer,editor,director      tt0083658\n84           producer,assistant_director,production_manager      tt0083658\n85                                           visual_effects      tt0083658\n86                         camera_department,visual_effects      tt0083658\n87                       producer,writer,assistant_director      tt0083658\n88                                           art_department      tt0083658\n89          production_designer,art_director,art_department      tt0083658\n90  editorial_department,script_department,sound_department      tt0083658\n91                              assistant_director,producer      tt0083658\n92                                           visual_effects      tt0083658\n93                  visual_effects,producer,special_effects      tt0083658\n94         set_decorator,art_department,production_designer      tt0083658\n95                                    actor,archive_footage      tt0083658\n96        visual_effects,animation_department,miscellaneous      tt0083658\n97                                music_department,composer      tt0083658\n98             visual_effects,producer,animation_department      tt0083658\n99                           visual_effects,special_effects      tt0083658\n100               assistant_director,producer,miscellaneous      tt0083658\n101                                          art_department      tt0083658\n102                         music_department,composer,actor      tt0083658\n103                                       camera_department      tt0083658\n104                                          art_department      tt0083658\n105         production_designer,art_department,art_director      tt0083658\n106         art_department,production_designer,art_director      tt0083658\n\n\n\nThus, 135 people from the original are still alive. Some actor and actresses are still alive as well, so we may want to consider adding them in the plot as a “fan service,” such as the main actor Bruce Willis.\n\n\n\nFor our next production, a remake of the 1988 Die Hard would be a strong competitor in the industry, starring Leonardo DiCaprio, Taylor Geare, and Tony Revolori. Directed by Academy Award winning Wally Pfister, the movie is bound to have an artistic expression like no other, similar to his eight other success. A guest appearance from Bruce Willis will have the fan base, who is practically begging for a remake, running to theaters.\nFrom the 1990s to the 2000s, the Action genre experienced over a 350% growth in successes, becoming one of the most wanted and rewarding genre in the industry. With beloved actor Leonardo Dicaprio combining forces with innovative director Pfister and up-and-coming Geare, we will give Inception fans a wanted reunion in this remake, growing our anticipated fan base even further. Lastly, including young actor Revolori with resonate with the younger generation, especially those tied to Marvel, as he is a fan favorite in that sphere. Thus, this combination would be an absolute recipe for success, especially since the action genre has the most successes in the 2020s thus far! If we move forward with this plan, we can easily be the next success!"
  },
  {
    "objectID": "mp02.html#how-many-movies-are-in-our-data-set-how-many-tv-series-how-many-tv-episodes",
    "href": "mp02.html#how-many-movies-are-in-our-data-set-how-many-tv-series-how-many-tv-episodes",
    "title": "Mini Project 02: The Business of Show Business",
    "section": "",
    "text": "Number_of_Movies &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\") |&gt;\n  summarize(Number = n())\n\nprint(Number_of_Movies$Number)\n\n[1] 74051\n\n\n\nNumber_of_TVshows &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  summarize(Number = n())\n\nprint(Number_of_TVshows$Number)\n\n[1] 11545\n\n\n\nNumber_of_Episodes &lt;- TITLE_EPISODES |&gt;\n  summarize(Number = n())\n\nprint(Number_of_Episodes$Number)\n\n[1] 1832877"
  },
  {
    "objectID": "mp02.html#who-is-the-oldest-living-person-in-our-data-set",
    "href": "mp02.html#who-is-the-oldest-living-person-in-our-data-set",
    "title": "Mini Project 02: The Business of Show Business",
    "section": "",
    "text": "oldest_person &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear)) |&gt;\n  arrange(birthYear) |&gt;\n  slice(1)\n\nprint(oldest_person)\n\n     nconst    primaryName birthYear deathYear primaryProfession\n1 nm1441282 Richard Dybeck      1811        NA        soundtrack\n                           knownForTitles\n1 tt0021783,tt0022126,tt0036372,tt0037562\n\n\nUnless our oldest actor is 4000 years old, we need to restrict the data set. Oldest living person is 116.\n\noldest_person &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear),\n         birthYear &gt;= 1924) |&gt;\n  arrange(birthYear) |&gt;\n  slice(1)\n\nprint(oldest_person)\n\n     nconst     primaryName birthYear deathYear           primaryProfession\n1 nm0001693 Eva Marie Saint      1924        NA actress,producer,soundtrack\n                           knownForTitles\n1 tt0047296,tt0053125,tt0348150,tt1837709\n\n\nOldest is Eva Marie Saint.\n\nThere is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\n\n\nBest_Episode &lt;- TITLE_RATINGS |&gt; \n  filter(averageRating == 10,\n         numVotes &gt;= 200000)\n\nprint(Best_Episode)\n\n[1] tconst        averageRating numVotes     \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\nBest_Episode_Name &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt2301451\")\n\nprint(Best_Episode_Name)\n\n[1] tconst         titleType      primaryTitle   originalTitle  isAdult       \n[6] startYear      endYear        runtimeMinutes genres        \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nOzymandias Breaking Bad.\nWhat four projects is the actor Mark Hamill most known for?\n\nMark_Hamill &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\")\n\nprint(Mark_Hamill)\n\n     nconst primaryName birthYear deathYear       primaryProfession\n1 nm0000434 Mark Hamill        NA        NA actor,producer,director\n                           knownForTitles\n1 tt0076759,tt2527336,tt0080684,tt0086190\n\nMark_Hamill_Projects &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt0076759\") \n\nprint(Mark_Hamill_Projects)\n\n     tconst titleType                       primaryTitle originalTitle isAdult\n1 tt0076759     movie Star Wars: Episode IV - A New Hope     Star Wars   FALSE\n  startYear endYear runtimeMinutes                   genres\n1      1977      NA            121 Action,Adventure,Fantasy\n\nMark_Hamill_Projects &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt2527336\") \n\nprint(Mark_Hamill_Projects)\n\n[1] tconst         titleType      primaryTitle   originalTitle  isAdult       \n[6] startYear      endYear        runtimeMinutes genres        \n&lt;0 rows&gt; (or 0-length row.names)\n\nMark_Hamill_Projects &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt0086190\") \n\nprint(Mark_Hamill_Projects)\n\n     tconst titleType                               primaryTitle\n1 tt0086190     movie Star Wars: Episode VI - Return of the Jedi\n                               originalTitle isAdult startYear endYear\n1 Star Wars: Episode VI - Return of the Jedi   FALSE      1983      NA\n  runtimeMinutes                   genres\n1            131 Action,Adventure,Fantasy\n\nMark_Hamill_Projects &lt;- TITLE_BASICS |&gt;\n  filter(tconst == \"tt0080684\") \n\nprint(Mark_Hamill_Projects)\n\n     tconst titleType                                   primaryTitle\n1 tt0080684     movie Star Wars: Episode V - The Empire Strikes Back\n                                   originalTitle isAdult startYear endYear\n1 Star Wars: Episode V - The Empire Strikes Back   FALSE      1980      NA\n  runtimeMinutes                   genres\n1            124 Action,Adventure,Fantasy\n\n\nWhat TV series, with more than 12 episodes, has the highest average rating?\n\ntwelve_episodes &lt;- TITLE_EPISODES |&gt; \n  filter(episodeNumber &gt; 12)  \n\nTVseries12 &lt;- inner_join(twelve_episodes, TITLE_RATINGS, join_by(tconst))\n\nseriesaverage &lt;- TVseries12 |&gt;\n  group_by(parentTconst) |&gt;\n  summarize(average_score = mean(averageRating, na.rm = TRUE)) |&gt;\n  arrange(desc(average_score))\n\nprint(seriesaverage)\n\n# A tibble: 1,024 × 2\n   parentTconst average_score\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 tt0170980              9.9\n 2 tt0491739              9.9\n 3 tt1134887              9.8\n 4 tt3032476              9.8\n 5 tt4158110              9.8\n 6 tt0043192              9.7\n 7 tt0061296              9.7\n 8 tt0155428              9.7\n 9 tt0840891              9.7\n10 tt0870039              9.7\n# ℹ 1,014 more rows\n\n\n\nTITLE_BASICS |&gt;\n  filter(tconst == \"tt0409579\") \n\n     tconst titleType primaryTitle originalTitle isAdult startYear endYear\n1 tt0409579  tvSeries         Made          Made   FALSE      2003      NA\n  runtimeMinutes     genres\n1             60 Reality-TV\n\nTITLE_BASICS |&gt;\n  filter(tconst == \"tt0491739\")\n\n     tconst titleType        primaryTitle originalTitle isAdult startYear\n1 tt0491739  tvSeries In the Nick of Time    Sto para 5   FALSE      2005\n  endYear runtimeMinutes               genres\n1    2007             45 Comedy,Drama,Mystery\n\nTITLE_BASICS |&gt;\n  filter(tconst == \"tt11289784\") \n\n      tconst titleType primaryTitle originalTitle isAdult startYear endYear\n1 tt11289784  tvSeries   Unus Annus    Unus Annus   FALSE      2019    2020\n  runtimeMinutes            genres\n1             NA Comedy,Reality-TV\n\nTITLE_BASICS |&gt;\n  filter(tconst == \"tt11363282\") \n\n      tconst titleType                          primaryTitle\n1 tt11363282  tvSeries The Real Housewives of Salt Lake City\n                          originalTitle isAdult startYear endYear\n1 The Real Housewives of Salt Lake City   FALSE      2020      NA\n  runtimeMinutes     genres\n1             43 Reality-TV\n\nTITLE_BASICS |&gt;\n  filter(tconst == \"tt21278628\") \n\n[1] tconst         titleType      primaryTitle   originalTitle  isAdult       \n[6] startYear      endYear        runtimeMinutes genres        \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\nThe TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality.\n\nIs it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\n\nTITLE_BASICS |&gt;\n  filter(primaryTitle == \"Happy Days\",\n         startYear == \"1974\",\n         endYear == \"1984\") \n\n     tconst titleType primaryTitle originalTitle isAdult startYear endYear\n1 tt0070992  tvSeries   Happy Days    Happy Days   FALSE      1974    1984\n  runtimeMinutes              genres\n1             30 Comedy,Family,Music\n\n\n\nHappy_Days &lt;- TITLE_EPISODES |&gt;\n  filter(parentTconst == \"tt0070992\")\n\nHappy_Days_Scores &lt;- inner_join(Happy_Days, TITLE_RATINGS, join_by(tconst))\n\n\nHappy_Days_Seasons &lt;- Happy_Days_Scores |&gt;\n  group_by(seasonNumber) |&gt;\n  summarize(averagescore = mean(averageRating, na.rm = TRUE)) |&gt;\n  arrange(seasonNumber)\n\nprint(Happy_Days_Seasons)\n\n# A tibble: 11 × 2\n   seasonNumber averagescore\n          &lt;dbl&gt;        &lt;dbl&gt;\n 1            1         7.58\n 2            2         7.69\n 3            3         7.7 \n 4            4         7.43\n 5            5         7   \n 6            6         7.02\n 7            7         6.33\n 8            8         5.4 \n 9            9         6.4 \n10           10         6.7 \n11           11         7.33\n\n\n#Task 3\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  mutate(popularity = averageRating*numVotes) \n\nmovie_ratings &lt;- inner_join(TITLE_BASICS, TITLE_RATINGS, join_by(tconst))\n\nmovie_ratings &lt;- movie_ratings |&gt; select(tconst, titleType, primaryTitle, genres, startYear, averageRating, numVotes, popularity)\n\nmovie_ratings &lt;- movie_ratings |&gt; \n  filter(titleType == \"movie\") |&gt;\n  arrange(desc(popularity))\n\n\nTop_ten &lt;- movie_ratings |&gt;\n  slice(1:10)\n\nprint(Top_ten)\n\n      tconst titleType                                      primaryTitle\n1  tt0111161     movie                          The Shawshank Redemption\n2  tt0468569     movie                                   The Dark Knight\n3  tt0137523     movie                                        Fight Club\n4  tt0109830     movie                                      Forrest Gump\n5  tt0110912     movie                                      Pulp Fiction\n6  tt0068646     movie                                     The Godfather\n7  tt0816692     movie                                      Interstellar\n8  tt0120737     movie The Lord of the Rings: The Fellowship of the Ring\n9  tt0133093     movie                                        The Matrix\n10 tt0167260     movie     The Lord of the Rings: The Return of the King\n                   genres startYear averageRating numVotes popularity\n1                   Drama      1994           9.3  2945417   27392378\n2      Action,Crime,Drama      2008           9.0  2925552   26329968\n3                   Drama      1999           8.8  2377517   20922150\n4           Drama,Romance      1994           8.8  2303729   20272815\n5             Crime,Drama      1994           8.9  2261906   20130963\n6             Crime,Drama      1972           9.2  2053013   18887720\n7  Adventure,Drama,Sci-Fi      2014           8.7  2165108   18836440\n8  Action,Adventure,Drama      2001           8.9  2045311   18203268\n9           Action,Sci-Fi      1999           8.7  2091447   18195589\n10 Action,Adventure,Drama      2003           9.0  2015928   18143352\n\n\n\nmovie_ratings &lt;- movie_ratings |&gt; \n  filter(titleType == \"movie\") |&gt;\n  arrange(popularity)\n\n\nbottom_five &lt;- movie_ratings |&gt;\n  slice(1:5)\n\nprint(bottom_five)\n\n     tconst titleType               primaryTitle         genres startYear\n1 tt0498734     movie   Zwischen Glück und Krone    Documentary      1959\n2 tt0386134     movie          Yubiley prokurora         Comedy      2003\n3 tt0309422     movie The Case He Couldn't Crack Crime,Thriller      1981\n4 tt0960709     movie              18'ler Takimi   Comedy,Crime      2007\n5 tt1047512     movie            Parentesi tonde         Comedy      2006\n  averageRating numVotes popularity\n1           1.2      103      123.6\n2           1.1      128      140.8\n3           1.0      153      153.0\n4           1.3      126      163.8\n5           1.1      161      177.1\n\n\n\nLeo &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Leonardo DiCaprio\") |&gt;\n  separate_longer_delim(knownForTitles, \",\")\n\n\ncolnames(Leo)[colnames(Leo) == \"knownForTitles\"] &lt;- \"tconst\"\n\nLeo &lt;- inner_join(Leo, movie_ratings, join_by(tconst))\n\n\nHanks &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Tom Hanks\") |&gt;\n  separate_longer_delim(knownForTitles, \",\")\n\n\ncolnames(Hanks)[colnames(Hanks) == \"knownForTitles\"] &lt;- \"tconst\"\n\nHanks &lt;- inner_join(Hanks, movie_ratings, join_by(tconst))\n\n\nHighest_Grossing &lt;- data.frame(\n  tconst = c(\"tt0499549\", \"tt4154796\", \"tt1630029\", \"tt0120338\", \"tt2488496\"),\n  LifetimeGross = c(\"$2,923,706,026\", \"$2,799,439,100\", \"$2,320,250,281\", \"$2,264,750,694\", \"$2,071,310,218\")\n)\n\n\nHigh_Grossing &lt;- inner_join(movie_ratings, Highest_Grossing, join_by(tconst))\n\n\nsolid_or_better &lt;- movie_ratings |&gt;\n  filter(popularity &gt;= 5000000)\n\n#Task 4\n\nsolid_or_better &lt;- solid_or_better|&gt;\n  arrange(startYear)\n\n\nsolid_or_better &lt;- solid_or_better |&gt; separate_longer_delim(genres, \",\") \n\n\ndecades_total &lt;- solid_or_better |&gt;\n  count(genres) \n\n\nforties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1940 & startYear &lt;= 1949) |&gt;\n  count(genres) \n\nfifties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1950 & startYear &lt;= 1959) |&gt;\n  count(genres) \n\n\nsixties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1960 & startYear &lt;= 1969) |&gt;\n  count(genres) \n\n\nseventies &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1970 & startYear &lt;= 1979) |&gt;\n  count(genres) \n\n\n\neighties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1980 & startYear &lt;= 1989) |&gt;\n  count(genres) \n\nnineties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 1990 & startYear &lt;= 1999) |&gt;\n  count(genres) \n\n\nthousands &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2000 & startYear &lt;= 2009) |&gt;\n  count(genres) \n\n\ntens &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2010 & startYear &lt;= 2019) |&gt;\n  count(genres) \n\ntwenties &lt;- solid_or_better |&gt;\n  filter(startYear &gt;= 2020 & startYear &lt;= 2029) |&gt;\n  count(genres) \n\n40s - drama (1) 50s - Crime (1) 60s - adventure (2) 70s - Drama (7) 80s - Action (8) 90s - Drama (25) 00s - Adventure (30) 10s - Action (42) 20s - Action (4)\n\ncolnames(forties)[colnames(forties) == \"n\"] &lt;- \"40s\"\ncolnames(fifties)[colnames(fifties) == \"n\"] &lt;- \"50s\"\ncolnames(sixties)[colnames(sixties) == \"n\"] &lt;- \"60s\"\ncolnames(seventies)[colnames(seventies) == \"n\"] &lt;- \"70s\"\ncolnames(eighties)[colnames(eighties) == \"n\"] &lt;- \"80s\"\ncolnames(nineties)[colnames(nineties) == \"n\"] &lt;- \"90s\"\ncolnames(thousands)[colnames(thousands) == \"n\"] &lt;- \"00s\"\ncolnames(tens)[colnames(tens) == \"n\"] &lt;- \"10s\"\ncolnames(twenties)[colnames(twenties) == \"n\"] &lt;- \"20s\"\ncolnames(decades_total)[colnames(decades_total) == \"n\"] &lt;- \"total\"\n\ndecades &lt;- full_join(forties, fifties, join_by(genres))\ndecades &lt;- full_join(decades, sixties, join_by(genres))\ndecades &lt;- full_join(decades, seventies, join_by(genres))\ndecades &lt;- full_join(decades, eighties, join_by(genres))\ndecades &lt;- full_join(decades, nineties, join_by(genres))\ndecades &lt;- full_join(decades, thousands, join_by(genres))\ndecades &lt;- full_join(decades, tens, join_by(genres))\ndecades &lt;- full_join(decades, twenties, join_by(genres))\ndecades &lt;- full_join(decades, decades_total, join_by(genres))\n\nBelow is a table that shows all of the genres throughout the years.\n\nlibrary(DT)\ndatatable(decades) \n\n\n\n\n\nLooking at the chart, it is clear to see that the genre with the most consistent successes each decade is Drama, as it is the only genre that had a least one success every decade from the forties until now. The genre that used to reliably produce successes, but has fallen out of favor is ???? COME BACK TO THIS\nThe genre that has produced the most successes since 2010 is the Adventure drama with 15 successes.\n\ndecade2010_pop &lt;- movie_ratings |&gt;\n  filter(startYear &gt;= 2010 & startYear &lt;= 2019) |&gt;\n  separate_longer_delim(genres, \",\") |&gt; \n  group_by(genres) |&gt;\n  summarize(totalPopularity = sum(popularity)) |&gt;\n  arrange(desc(totalPopularity)) |&gt;\n  slice(1:3)\n\nprint(decade2010_pop)\n\n# A tibble: 3 × 2\n  genres    totalPopularity\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Adventure      198759518.\n2 Drama          195157332.\n3 Action         194724207.\n\n\n\ndecade2010_amount &lt;- movie_ratings |&gt;\n  filter(startYear &gt;= 2010 & startYear &lt;= 2019) |&gt;\n  separate_longer_delim(genres, \",\") |&gt; \n  count(genres) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1:9) \n  \n\nprint(decade2010_amount)\n\n       genres    n\n1       Drama 1089\n2      Comedy  619\n3 Documentary  364\n4      Action  351\n5    Thriller  288\n6     Romance  263\n7      Horror  247\n8       Crime  240\n9   Adventure  217\n\n\nIt ranked high on the total popularity, but in the middle on the amount of productions.\nLooking again at the chart from earlier, we can see that the genre that became popular most frequently is the Action genre. It had its first successful movie in the 70s, eight the following year, to its max of 25 in the 2000s.\nBased on my findings, I decided to choose an Action genre, as while it is the fifth most produced in the last decade, it had the second highest popularity score, being one of two of the only genres to reach a success score of over a billion. The first was the drama genre, but it also leads the category with about 7,500 more movies than second place, and almost 13500 more than the action category. This shows me that they only have this high of a popularity due to the numerous successes that they had, which means each success had to average at the lower end of the success scale. On the other hand, with the action genre having less movies, but the second highest score, this shows me that each movie had to average at a high score on the success spectrum in order to secure this spot."
  },
  {
    "objectID": "mp02.html#task-1-column-type-correction-1",
    "href": "mp02.html#task-1-column-type-correction-1",
    "title": "Mini Project 02: The Business of Show Business",
    "section": "",
    "text": "TITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(isAdult = as.logical(isAdult),\n         startYear = as.numeric(startYear),\n         endYear = as.numeric(endYear),\n         runtimeMinutes = as.numeric(runtimeMinutes))\n\nWarning: There were 3 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `startYear = as.numeric(startYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n  mutate(seasonNumber = as.numeric(seasonNumber),\n         episodeNumber = as.numeric(episodeNumber))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `seasonNumber = as.numeric(seasonNumber)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\nNAME_BASICS |&gt; separate_longer_delim(knownForTitles, \",\") |&gt; slice_head(n=10) \n\n      nconst     primaryName birthYear deathYear\n1  nm0000001    Fred Astaire      1899      1987\n2  nm0000001    Fred Astaire      1899      1987\n3  nm0000001    Fred Astaire      1899      1987\n4  nm0000001    Fred Astaire      1899      1987\n5  nm0000002   Lauren Bacall      1924      2014\n6  nm0000002   Lauren Bacall      1924      2014\n7  nm0000002   Lauren Bacall      1924      2014\n8  nm0000002   Lauren Bacall      1924      2014\n9  nm0000003 Brigitte Bardot      1934        NA\n10 nm0000003 Brigitte Bardot      1934        NA\n                    primaryProfession knownForTitles\n1        actor,miscellaneous,producer      tt0072308\n2        actor,miscellaneous,producer      tt0050419\n3        actor,miscellaneous,producer      tt0053137\n4        actor,miscellaneous,producer      tt0027125\n5  actress,soundtrack,archive_footage      tt0037382\n6  actress,soundtrack,archive_footage      tt0075213\n7  actress,soundtrack,archive_footage      tt0117057\n8  actress,soundtrack,archive_footage      tt0038355\n9   actress,music_department,producer      tt0057345\n10  actress,music_department,producer      tt0049189\n\n\n#Task 2"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "In this project, I will be writing a political fact-check and investigate the claim that the US Electoral College systematically biases election results away from the popular vote.\nBefore we begin, we must download the code here. This code shows us how many votes each candidate got througout the years, for both the Presidential Elections and the House of Representatives Elections.\nNow, I must download all the shape files into R.\nNow, I will make a Chloropleth Visualization of the 2000 Presidential Election Electoral College Results.\nprint(USA2000Map)\nBelow is a faceted map of the presidential party winners per state from 1976 to 2020.\nelection_outcomes |&gt;\n  st_shift_longitude() |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = party)) +\n  geom_sf() +\n  coord_sf(xlim = c(170, 300)) +\n  scale_fill_manual(name = \"Party\", values = c(\"DEMOCRAT\" = \"royalblue2\", \"REPUBLICAN\" = \"firebrick2\")) +\n  theme_bw() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  labs(title = \"Presidential Election Outcomes, 1976-2020\") +\n  facet_wrap(~year)\nThough things have changed over time due to amendment, statue, and technology, the basic outline of allocation has remained the same:\nNotably, the Constitution sets no rules on how the electoral college votes for a particular state is allocated. In the past, states have:\nTo complete this fact check, we will compare the effects of all of the Electoral College Votes allocation rules. For this, we will assume that the District of Columbia has 3 Electoral College Votes.\nFirst, let’s see who actually won each election throughout the years and how many electoral college votes they received to get this win.\ndatatable(actual_results)\nA state-wide approach would be that all electoral college votes of each state goes to whoever has the popular vote within that state.\ndatatable(country_winner)\nUsing this method, we can see that some elections would have turned out differently than what happened. For example, Gerald Ford would have won over Jimmy Carter in 1976, George H. W. Bush would have won over Bill Clinton in 1992, Robert Dole would have won over Bill Clinton in 1996, John McCain would have won over Barack Obama in 2008, and Mitt Romney would have won over Barack Obama in 2012. That is five elections over the past 46 years that would have been swayed due to this type of voting system.\nI do not believe that this way of voting is fair, as it does not properly represent the voters of the minority party in each state. That is, if 51% of voters vote for Party A, while 49% vote for Party B, all of Party B’s voices will not be heard. That is a large majority of people who are being misheard. This also affects the election more, as the states who have less residents, but a good amount of electoral college votes can sway the election in their favor.\nThus, let’s go to the next voting option.\nFor the second option, this means that all Electoral College Votes are given to whoever wins the popular vote of the whole country.\ndatatable(country_winner2)\nUsing this method, there were only two instances where the winners here differ from the actually winners of each elections. The two scenarios are as follows: Al Gore would have won over George W. Bush in 2000 and Hillary Clinton would have won over Donald Trump in 2020.\nThough this voting process produces similar outcomes to the actual election, I believe it is unfair due to the same reasoning as the first voting process. That is, again, if 51% of voters vote for Party A, while 49% vote for Party B, all of Party B’s voices will not be heard. That is a large majority of people who are being misheard.\nLet’s go to the final method.\ndatatable(final_method_winners)\nUsing this method, we can see six elections would have had different outcomes than what would have actually occurred. These elections are as follows: Jimmy Carter would have won over Ronald Reagan in 1980, Walter Mondale would have won over Ronald Reagan in 1984, Michael Dukakis would have won over George H. W. Bush in 1988, Robert Dole would have won over Bill Clinton in 1996, Mitt Romney would have won over Barrack Obama in 2012, and Donald Trump would have won over Joseph Biden in 2020.\nI do not believe this is fair, as it still gives the majority voters a competitive advantage in the voting process. Not only are they getting the majority of the votes from being the popular vote, but they are also getting two electoral college votes on top of that. Thus, I do not believe this is fair.\nI believe the most fair way to divide the Electoral College Votes up in each state is on a proportional scale. Thus, each party is being accurately represented and no one party’s voice is being heard more than the other."
  },
  {
    "objectID": "mp03.html#which-states-have-gained-and-lost-the-most-seats-in-the-us-house-of-representatives-between-1976-and-2022",
    "href": "mp03.html#which-states-have-gained-and-lost-the-most-seats-in-the-us-house-of-representatives-between-1976-and-2022",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?",
    "text": "Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\n\nhouse_seats &lt;- X1976_2022_house |&gt;\n  filter(year %in% c(1976, 2022)) |&gt;\n  group_by(year, state) |&gt; \n  summarize(seats=n_distinct(district))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nhouse_seats_spread &lt;- house_seats |&gt;\n  pivot_wider(names_from = year, values_from = seats) |&gt;\n  mutate(difference = (`2022` - `1976`)) |&gt;\n  filter(difference != 0)\n\n\ndiff_seats &lt;- house_seats_spread |&gt;\n  select(state, difference)\n\nggplot(diff_seats, aes(x = state, y = difference, fill = difference &gt; 0)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"tomato\", \"springgreen2\")) +\n  coord_flip() +\n  labs(title = \"Each State's Gain/Loss of Seats in the US House of Representatives from 1976 - 2022\", x = \"State\", y = \"Difference of Seats\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10),\n        axis.text.y = element_text(size = 8),\n        plot.margin = margin(3, 3, 3))\n\n\n\n\n\n\n\n\n\nfusion_voting &lt;- X1976_2022_house |&gt;\n  group_by(year, state, district, candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = 'drop') |&gt;\n  group_by(year, state, district) |&gt;\n  filter(total_votes == max(total_votes)) |&gt;\n  ungroup()\n\ncolnames(fusion_voting)[colnames(fusion_voting) == \"total_votes\"] &lt;- \"total_fusion_votes\"\n\n\nno_fusion_voting &lt;- X1976_2022_house |&gt;\n  filter(party %in% c(\"REPUBLICAN\", \"DEMOCRAT\")) |&gt;\n  group_by(year, state, district, candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = 'drop') |&gt;\n  group_by(year, state, district) |&gt;\n  filter(total_votes == max(total_votes)) |&gt;\n  ungroup()\n\nhouse_voting &lt;- left_join(fusion_voting, no_fusion_voting, by = (c(\"state\",\"district\")))\n\nWarning in left_join(fusion_voting, no_fusion_voting, by = (c(\"state\", \"district\"))): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nhouse_voting &lt;- fusion_voting |&gt;\n  left_join(no_fusion_voting, by = c(\"year\", \"state\", \"district\")) |&gt;\n  mutate(difference = (`total_fusion_votes` - `total_votes`)) |&gt;\n  filter(difference != 0) |&gt;\n  arrange(desc(difference))\n\n\nlibrary(DT)\ndatatable(house_voting) \n\n\n\n\ninstances &lt;- nrow(house_voting)\nprint(instances)\n\n[1] 689\n\n\n\nhouse_candidates &lt;- X1976_2022_house |&gt;\n  filter(party %in% c(\"REPUBLICAN\", \"DEMOCRAT\")) |&gt;\n  group_by(year, party) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = 'drop') |&gt;\n  group_by(year, party)\n\n\npresident_candidates &lt;- X1976_2020_president |&gt;\n  filter(party_detailed %in% c(\"REPUBLICAN\", \"DEMOCRAT\")) |&gt;\n  group_by(year, party_detailed) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = 'drop') |&gt;\n  group_by(year, party_detailed)\n\n\ncolnames(president_candidates)[colnames(president_candidates) == \"party_detailed\"] &lt;- \"party\"\n\ncolnames(president_candidates)[colnames(president_candidates) == \"total_votes\"] &lt;- \"president_votes\"\n\ncolnames(house_candidates)[colnames(house_candidates) == \"total_votes\"] &lt;- \"house_votes\"\n\nVote_difference &lt;- president_candidates |&gt;\n  inner_join(house_candidates, by = c(\"year\", \"party\")) |&gt;\n  mutate(difference = (president_votes - house_votes))\n\nggplot(Vote_difference, aes (x = year, y = difference, color = party)) + \n  geom_point() +\n  geom_line(linewidth = .5) + \n  scale_color_manual(values = c(\"DEMOCRAT\" = \"royalblue2\", \"REPUBLICAN\" = \"firebrick2\")) +\n  labs(title = \"Difference Between Presidential and House Votes Per Year\", x = \"Year\", y = \"Difference of Votes (President - House)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ninstall.packages('plyr', repos = \"http://cran.us.r-project.org\")\n\nInstalling package into 'C:/Users/laure/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'plyr' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\laure\\AppData\\Local\\Temp\\RtmpGQeWjy\\downloaded_packages\n\noptions(repos = list(CRAN=\"http://cran.rstudio.com/\"))\n\nlibrary(utils)\ninstall.packages(\"sf\")\n\nWarning: package 'sf' is in use and will not be installed\n\nlibrary(sf)\n\nlibrary(sf)\ns.sf &lt;- st_read(\"districtShapes/districts106.shp\")\n\nReading layer `districts106' from data source \n  `C:\\Users\\laure\\OneDrive\\Documents\\STA9750-2024-FALL\\districtShapes\\districts106.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 436 features and 15 fields (with 1 geometry empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1473 ymin: 18.9177 xmax: 179.7785 ymax: 71.35256\nGeodetic CRS:  GRS 1980(IUGG, 1980)\n\nhead(s.sf, n=4)\n\nSimple feature collection with 4 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -118.5992 ymin: 26.94532 xmax: -82.05435 ymax: 34.33793\nGeodetic CRS:  GRS 1980(IUGG, 1980)\n   STATENAME           ID DISTRICT STARTCONG ENDCONG DISTRICTSI COUNTY PAGE\n1 California 006103107026       26       103     107       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n2 California 006103107029       29       103     107       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n3 California 006103107030       30       103     107       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n4    Florida 012105107013       13       105     107       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n   LAW NOTE BESTDEC RNOTE FROMCOUNTY                 LASTCHANGE\n1 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;          F 2016-05-29 16:44:10.857626\n2 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;          F 2016-05-29 16:44:10.857626\n3 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;          F 2016-05-29 16:44:10.857626\n4 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;          F 2016-05-29 16:44:10.857626\n                   FINALNOTE                       geometry\n1 {\"From US Census website\"} MULTIPOLYGON (((-118.5075 3...\n2 {\"From US Census website\"} MULTIPOLYGON (((-118.354 34...\n3 {\"From US Census website\"} MULTIPOLYGON (((-118.184 34...\n4 {\"From US Census website\"} MULTIPOLYGON (((-82.42332 2...\n\npresident_2000 &lt;- X1976_2020_president |&gt;\n  filter(year == '2000',\n         candidate %in% c(\"BUSH, GEORGE W.\", \"GORE, AL\")) |&gt;\n  group_by(state) |&gt;\n  summarize(\n    highest_votes = max(candidatevotes),\n    party = party_detailed[which.max(candidatevotes)]\n  )\n\n\nUSA2000results &lt;- s.sf |&gt;\n  mutate(STATENAME = toupper(trimws(STATENAME))) |&gt;\n  left_join(president_2000, join_by(STATENAME == state)) \n\nUSA2000Map &lt;- ggplot(USA2000results, aes(geometry = geometry, fill = party),\n                     color = \"black\") +\n  geom_sf() + \n  scale_fill_manual(values = c(\"REPUBLICAN\" = \"firebrick2\", \"DEMOCRAT\" = \"royalblue2\")) +\n  theme_minimal() +\n  coord_sf(xlim = c(-180, -50), ylim = c(10,80), expand = FALSE) +\n  labs(title = \"2000 Presidential Election Electoral College Results\", fill = \"Winning Party\") \n\nprint(USA2000Map)"
  },
  {
    "objectID": "mp03.html#exploration-of-vote-count-data",
    "href": "mp03.html#exploration-of-vote-count-data",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Exploration of Vote Count Data",
    "text": "Exploration of Vote Count Data"
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "mp03.html#chloropleth-visualization",
    "href": "mp03.html#chloropleth-visualization",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Chloropleth Visualization",
    "text": "Chloropleth Visualization"
  },
  {
    "objectID": "mp03.html#advanced-chloropleth-visualization-of-electoral-college-results",
    "href": "mp03.html#advanced-chloropleth-visualization-of-electoral-college-results",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Advanced Chloropleth Visualization of Electoral College Results",
    "text": "Advanced Chloropleth Visualization of Electoral College Results"
  },
  {
    "objectID": "mp03.html#comparing-the-effects-of-the-ecv-allocation-rules",
    "href": "mp03.html#comparing-the-effects-of-the-ecv-allocation-rules",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Comparing the Effects of the ECV Allocation Rules",
    "text": "Comparing the Effects of the ECV Allocation Rules"
  },
  {
    "objectID": "finalproject.html",
    "href": "finalproject.html",
    "title": "Final Project Team 9",
    "section": "",
    "text": "Final Project Team 9\n\n\nClick here to see how the libraries were downloaded\n\n\ninstall.packages('plyr', repos = \"http://cran.us.r-project.org\")\n\nInstalling package into 'C:/Users/laure/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'plyr' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\laure\\AppData\\Local\\Temp\\Rtmpw98s8d\\downloaded_packages\n\noptions(repos = list(CRAN=\"http://cran.rstudio.com/\"))\n\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(DT)\nlibrary(tigris)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(plotly)\nlibrary(gganimate)\nlibrary(viridis)\n\nWarning: package 'viridis' was built under R version 4.4.2\n\n\nLoading required package: viridisLite\n\ncensus_api_key('fd444ca335bf9020633084575dbe45c1529be65f', install = TRUE, overwrite = TRUE)\n\nYour original .Renviron will be backed up and stored in your R HOME directory if needed.\n\n\nYour API key has been stored in your .Renviron and can be accessed by Sys.getenv(\"CENSUS_API_KEY\"). \nTo use now, restart R or run `readRenviron(\"~/.Renviron\")`\n\n\n[1] \"fd444ca335bf9020633084575dbe45c1529be65f\"\n\n\n\nChange of Variables\n\n\nClick here to see how the variables were changed\n\n\nvars_2018 &lt;- load_variables(2018, \"acs1\", cache = TRUE)\n\nvar_map &lt;- c(\n  \"B02001_001\" = \"Estimated Total\",\n  \"B02001_002\" = \"White Alone\", \n  \"B02001_003\" = \"Black or African American Alone\", \n  \"B02001_004\" = \"American Indian and Alaska Native Alone\",\n  \"B02001_005\" = \"Asian Alone\", \n  \"B02001_006\" = \"Native Hawaiian and other Pacific Islander Alone\",\n  \"B02001_007\" = \"Some Other Race\", \n  \"B02001_008\" = \"2 or more races\"\n)\n\n\nPopulation Report for 2018 - 2023, based on Race\n\n\nClick here to see how the data was downloaded into R\n\n\n# Getting 2018: \n\npopulation_1901_2018 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"B02001_001\", \"B02001_002\", \"B02001_003\", \"B02001_004\", \"B02001_005\", \"B02001_006\", \"B02001_007\", \"B02001_008\"),\n  year = 2018,\n  survey = \"acs1\"\n)\n\nGetting data from the 2018 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\npopulation_1901_2018 &lt;- population_1901_2018 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(race = variable) |&gt;\n  rename(y2018 = estimate) |&gt;\n  select(-moe)\n\n\npopulation_1901_2018 &lt;- population_1901_2018 |&gt;\n  mutate(race = var_map[race])\n\n# Getting 2019\n\npopulation_1901_2019 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"B02001_001\", \"B02001_002\", \"B02001_003\", \"B02001_004\", \"B02001_005\", \"B02001_006\", \"B02001_007\", \"B02001_008\"),\n  year = 2019,\n  survey = \"acs1\"\n)\n\nGetting data from the 2019 1-year ACS\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\npopulation_1901_2019 &lt;- population_1901_2019 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(race = variable) |&gt;\n  rename(y2019 = estimate) |&gt;\n  select(-moe)\n\n\npopulation_1901_2019 &lt;- population_1901_2019 |&gt;\n  mutate(race = var_map[race])\n\n\n# Getting 2021\n\npopulation_1901_2021 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"B02001_001\", \"B02001_002\", \"B02001_003\", \"B02001_004\", \"B02001_005\", \"B02001_006\", \"B02001_007\", \"B02001_008\"),\n  year = 2021,\n  survey = \"acs1\"\n)\n\nGetting data from the 2021 1-year ACS\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\npopulation_1901_2021 &lt;- population_1901_2021 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(race = variable) |&gt;\n  rename(y2021 = estimate) |&gt;\n  select(-moe)\n\n\npopulation_1901_2021 &lt;- population_1901_2021 |&gt;\n  mutate(race = var_map[race])\n\n\n# Getting 2022\n\n\npopulation_1901_2022 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"B02001_001\", \"B02001_002\", \"B02001_003\", \"B02001_004\", \"B02001_005\", \"B02001_006\", \"B02001_007\", \"B02001_008\"),\n  year = 2022,\n  survey = \"acs1\"\n)\n\nGetting data from the 2022 1-year ACS\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\npopulation_1901_2022 &lt;- population_1901_2022 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(race = variable) |&gt;\n  rename(y2022 = estimate) |&gt;\n  select(-moe)\n\n\npopulation_1901_2022 &lt;- population_1901_2022 |&gt;\n  mutate(race = var_map[race])\n\n\n# Getting 2023\n\npopulation_1901_2023 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"B02001_001\", \"B02001_002\", \"B02001_003\", \"B02001_004\", \"B02001_005\", \"B02001_006\", \"B02001_007\", \"B02001_008\"),\n  year = 2023,\n  survey = \"acs1\"\n)\n\nGetting data from the 2023 1-year ACS\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\npopulation_1901_2023 &lt;- population_1901_2023 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(race = variable) |&gt;\n  rename(y2023 = estimate) |&gt;\n  select(-moe)\n\n\npopulation_1901_2023 &lt;- population_1901_2023 |&gt;\n  mutate(race = var_map[race]) \n\n\nData table that shows all of the races from 2018-2023, by race and county.\n\n\nClick here to see how this was done\n\n\ntotal_population_race &lt;- population_1901_2018 |&gt;\n  left_join(select(population_1901_2019, y2019, GEOID, race), by = c(\"GEOID\", \"race\")) |&gt;\n  left_join(select(population_1901_2021, y2021, GEOID, race), by = c(\"GEOID\", \"race\")) |&gt;\n  left_join(select(population_1901_2022, y2022, GEOID, race), by = c(\"GEOID\", \"race\")) |&gt;\n  left_join(select(population_1901_2023, y2023, GEOID, race), by = c(\"GEOID\", \"race\")) \n\n\n\ndatatable(total_population_race)\n\n\n\n\n\nGet NY State map by county\n\n\nClick here to see how this was done\n\n\nny_counties &lt;- counties(state = \"NY\", cb = TRUE)\n\nRetrieving data for the year 2022\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\nny_counties_sf &lt;- st_as_sf(ny_counties)\n\n\n\nggplot(data = ny_counties_sf) +\n  geom_sf() +\n  theme_minimal() +\n  labs(title = \"Counties of New York State\",\n       caption = \"Source: US Census Bureau\")\n\n\n\n\n\n\n\n\n\n\nClick here to see how this was done\n\n\n# Only total population \n\nestimated_total &lt;- total_population_race |&gt;\n  filter(grepl(\"Estimated Total\", race))\n\n            \nestimated_total_long &lt;- estimated_total |&gt;\n  pivot_longer(cols = starts_with(\"y\"),\n               names_to = \"year\",\n               values_to = \"population\") |&gt;\n  mutate(year = as.numeric(gsub(\"y\", \"\", year))) |&gt;\n  select(-race, -NAME)\n\n\nNY_pop_long &lt;- ny_counties_sf |&gt;\n  left_join(estimated_total_long, by = \"GEOID\")\n            \n\nlibrary(gganimate)\nlibrary(gifski)\n\nWarning: package 'gifski' was built under R version 4.4.2\n\nlibrary(magick)\n\nWarning: package 'magick' was built under R version 4.4.2\n\n\nLinking to ImageMagick 6.9.12.98\nEnabled features: cairo, freetype, fftw, ghostscript, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fontconfig, x11\n\np &lt;- ggplot(NY_pop_long) +\n  geom_sf(aes(fill = population), color = \"white\") +  \n  scale_fill_viridis(name = \"population\", option = \"C\", trans = \"log\") + \n  theme_minimal() +\n  labs(title = \"Population by County in New York ({frame_time})\",  # Add the title and year\n       subtitle = \"Year: {frame_time}\",\n       caption = \"Source: Your Dataset\") +\n  theme(legend.position = \"right\") +\n  transition_time(year) +  \n  ease_aes('linear') \n\n\nData by House Hold Income:\nPopulation Report for 2018 - 2023, based on Race\n\n\nClick here to see how the data was downloaded into R\n\n\nvar_map2 &lt;- c(\n  \"S1901_C01_001\" = \"Total Households\",\n  \"S1901_C01_002\" = \"Income &lt; $10,000\", \n  \"S1901_C01_003\" = \"Income $10,000 - $14,999\", \n  \"S1901_C01_004\" = \"Income $15,000 - $24,999\",\n  \"S1901_C01_005\" = \"Income $25,000 - $34,999\", \n  \"S1901_C01_006\" = \"Income $35,000 - $49,999\",\n  \"S1901_C01_007\" = \"Income $50,000 - $74,999\", \n  \"S1901_C01_008\" = \"Income $75,000 - $99,999\",\n  \"S1901_C01_009\" = \"Income $100,000 - $149,999\",\n  \"S1901_C01_010\" = \"Income $150,000 - $199,999\",\n  \"S1901_C01_011\" = \"Income &gt; $200,000\"\n)\n\n# Income 2018\nIncome_2018 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"S1901_C01_001E\", \"S1901_C01_002E\", \"S1901_C01_003E\", \"S1901_C01_004E\", \"S1901_C01_005E\", \"S1901_C01_006E\", \"S1901_C01_007E\", \"S1901_C01_008E\", \"S1901_C01_009E\", \"S1901_C01_010E\", \"S1901_C01_011E\"),\n  year = 2018,\n  survey = \"acs1\"\n)\n\nGetting data from the 2018 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nUsing the ACS Subject Tables\n\nIncome_2018 &lt;- Income_2018 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(income = variable) |&gt;\n  rename(y2018 = estimate) |&gt;\n  select(-moe)\n\n\nIncome_2018 &lt;- Income_2018 |&gt;\n  mutate(income = var_map2[income])\n\n# Income 2019\n\nIncome_2019 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"S1901_C01_001E\", \"S1901_C01_002E\", \"S1901_C01_003E\", \"S1901_C01_004E\", \"S1901_C01_005E\", \"S1901_C01_006E\", \"S1901_C01_007E\", \"S1901_C01_008E\", \"S1901_C01_009E\", \"S1901_C01_010E\", \"S1901_C01_011E\"),\n  year = 2019,\n  survey = \"acs1\"\n)\n\nGetting data from the 2019 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nUsing the ACS Subject Tables\n\nIncome_2019 &lt;- Income_2019 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(income = variable) |&gt;\n  rename(y2019 = estimate) |&gt;\n  select(-moe)\n\n\nIncome_2019 &lt;- Income_2019 |&gt;\n  mutate(income = var_map2[income])\n\n# Get 2021\n\nIncome_2021 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"S1901_C01_001E\", \"S1901_C01_002E\", \"S1901_C01_003E\", \"S1901_C01_004E\", \"S1901_C01_005E\", \"S1901_C01_006E\", \"S1901_C01_007E\", \"S1901_C01_008E\", \"S1901_C01_009E\", \"S1901_C01_010E\", \"S1901_C01_011E\"),\n  year = 2021,\n  survey = \"acs1\"\n)\n\nGetting data from the 2021 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nUsing the ACS Subject Tables\n\nIncome_2021 &lt;- Income_2021 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(income = variable) |&gt;\n  rename(y2021 = estimate) |&gt;\n  select(-moe)\n\n\nIncome_2021 &lt;- Income_2021 |&gt;\n  mutate(income = var_map2[income])\n\n\n# Get 2022\n\nIncome_2022 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"S1901_C01_001E\", \"S1901_C01_002E\", \"S1901_C01_003E\", \"S1901_C01_004E\", \"S1901_C01_005E\", \"S1901_C01_006E\", \"S1901_C01_007E\", \"S1901_C01_008E\", \"S1901_C01_009E\", \"S1901_C01_010E\", \"S1901_C01_011E\"),\n  year = 2022,\n  survey = \"acs1\"\n)\n\nGetting data from the 2022 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nUsing the ACS Subject Tables\n\nIncome_2022 &lt;- Income_2022 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(income = variable) |&gt;\n  rename(y2022 = estimate) |&gt;\n  select(-moe)\n\n\nIncome_2022 &lt;- Income_2022 |&gt;\n  mutate(income = var_map2[income])\n\n# Get 2023\n\nIncome_2023 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"S1901_C01_001E\", \"S1901_C01_002E\", \"S1901_C01_003E\", \"S1901_C01_004E\", \"S1901_C01_005E\", \"S1901_C01_006E\", \"S1901_C01_007E\", \"S1901_C01_008E\", \"S1901_C01_009E\", \"S1901_C01_010E\", \"S1901_C01_011E\"),\n  year = 2023,\n  survey = \"acs1\"\n)\n\nGetting data from the 2023 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nUsing the ACS Subject Tables\n\nIncome_2023 &lt;- Income_2023 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(income = variable) |&gt;\n  rename(y2023 = estimate) |&gt;\n  select(-moe)\n\n\nIncome_2023 &lt;- Income_2023 |&gt;\n  mutate(income = var_map2[income])\n\n\nData table that shows all of the households from 2018-2023, by income and county.\n\n\nClick here to see how this was done\n\n\n# Join tables\n\ntotal_income_household &lt;- Income_2018 |&gt;\n  left_join(select(Income_2019, y2019, GEOID, income), by = c(\"GEOID\", \"income\")) |&gt;\n  left_join(select(Income_2021, y2021, GEOID, income), by = c(\"GEOID\", \"income\")) |&gt;\n  left_join(select(Income_2022, y2022, GEOID, income), by = c(\"GEOID\", \"income\")) |&gt;\n  left_join(select(Income_2023, y2023, GEOID, income), by = c(\"GEOID\", \"income\")) \n\n\n\ndatatable(total_income_household)\n\n\n\n\n\nlong version:\n\ntotal_income_household_long &lt;- total_income_household |&gt;\n  pivot_longer(cols = starts_with(\"y\"),\n               names_to = \"year\",\n               values_to = \"households\") |&gt;\n  mutate(year = as.numeric(gsub(\"y\", \"\", year))) \n\n\ndatatable(total_income_household_long)\n\n\n\n\n\nMean Income per capita per race by county:\n\n\nClick here to see how the data was downloaded into R\n\n\n# Mean Incomes per capita \n\nvar_map3 &lt;- c(\n  \"S1902_C03_001\" = \"All Household Income Mean\",\n  \"S1902_C03_020\" = \"White Alone\", \n  \"S1902_C03_021\" = \"Black or African American Alone\", \n  \"S1902_C03_022\" = \"American Indian and Alaska Native Alone\",\n  \"S1902_C03_023\" = \"Asian Alone\", \n  \"S1902_C03_024\" = \"Native Hawaiian and other Pacific Islander Alone\",\n  \"S1902_C03_025\" = \"Some Other Race\", \n  \"S1902_C03_026\" = \"2 or more races\"\n)\n\n# Get 2018\n\nmean_income_2018 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"S1902_C03_001E\", \"S1902_C03_020E\", \"S1902_C03_021E\", \"S1902_C03_022E\", \"S1902_C03_023E\", \"S1902_C03_024E\", \"S1902_C03_025E\", \"S1902_C03_026E\"),\n  year = 2018,\n  survey = \"acs1\"\n)\n\nGetting data from the 2018 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nUsing the ACS Subject Tables\n\nmean_income_2018 &lt;- mean_income_2018 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(race = variable) |&gt;\n  rename(y2018 = estimate) |&gt;\n  select(-moe)\n\n\nmean_income_2018 &lt;- mean_income_2018 |&gt;\n  mutate(race = var_map3[race])\n\n\n  \n\n\n# Get 2019\n\nmean_income_2019 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"S1902_C03_001E\", \"S1902_C03_020E\", \"S1902_C03_021E\", \"S1902_C03_022E\", \"S1902_C03_023E\", \"S1902_C03_024E\", \"S1902_C03_025E\", \"S1902_C03_026E\"),\n  year = 2019,\n  survey = \"acs1\"\n)\n\nGetting data from the 2019 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nUsing the ACS Subject Tables\n\nmean_income_2019 &lt;- mean_income_2019 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(race = variable) |&gt;\n  rename(y2019 = estimate) |&gt;\n  select(-moe)\n\n\nmean_income_2019 &lt;- mean_income_2019 |&gt;\n  mutate(race = var_map3[race])\n\n\n\n# Get 2021\n\nmean_income_2021 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"S1902_C03_001E\", \"S1902_C03_020E\", \"S1902_C03_021E\", \"S1902_C03_022E\", \"S1902_C03_023E\", \"S1902_C03_024E\", \"S1902_C03_025E\", \"S1902_C03_026E\"),\n  year = 2021,\n  survey = \"acs1\"\n)\n\nGetting data from the 2021 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nUsing the ACS Subject Tables\n\nmean_income_2021 &lt;- mean_income_2021 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(race = variable) |&gt;\n  rename(y2021 = estimate) |&gt;\n  select(-moe)\n\n\nmean_income_2021 &lt;- mean_income_2021 |&gt;\n  mutate(race = var_map3[race])\n\n\n# Get 2022\n\nmean_income_2022 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"S1902_C03_001E\", \"S1902_C03_020E\", \"S1902_C03_021E\", \"S1902_C03_022E\", \"S1902_C03_023E\", \"S1902_C03_024E\", \"S1902_C03_025E\", \"S1902_C03_026E\"),\n  year = 2022,\n  survey = \"acs1\"\n)\n\nGetting data from the 2022 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nUsing the ACS Subject Tables\n\nmean_income_2022 &lt;- mean_income_2022 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(race = variable) |&gt;\n  rename(y2022 = estimate) |&gt;\n  select(-moe)\n\n\nmean_income_2022 &lt;- mean_income_2022 |&gt;\n  mutate(race = var_map3[race])\n\n\n\n# Get 2023\n\nmean_income_2023 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\"S1902_C03_001E\", \"S1902_C03_020E\", \"S1902_C03_021E\", \"S1902_C03_022E\", \"S1902_C03_023E\", \"S1902_C03_024E\", \"S1902_C03_025E\", \"S1902_C03_026E\"),\n  year = 2023,\n  survey = \"acs1\"\n)\n\nGetting data from the 2023 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nUsing the ACS Subject Tables\n\nmean_income_2023 &lt;- mean_income_2023 |&gt;\n  filter(str_detect(NAME, regex(\"New York\", ignore_case = TRUE))) |&gt;\n  rename(race = variable) |&gt;\n  rename(y2023 = estimate) |&gt;\n  select(-moe)\n\n\nmean_income_2023 &lt;- mean_income_2023 |&gt;\n  mutate(race = var_map3[race])\n\n\nData table that shows all of the households from 2018-2023, by income and county.\n\n\nClick here to see how this was done\n\n\n# Join tables\n\ntotal_mean_incomes &lt;- mean_income_2018 |&gt;\n  left_join(select(mean_income_2019, y2019, GEOID, race), by = c(\"GEOID\", \"race\")) |&gt;\n  left_join(select(mean_income_2021, y2021, GEOID, race), by = c(\"GEOID\", \"race\")) |&gt;\n  left_join(select(mean_income_2022, y2022, GEOID, race), by = c(\"GEOID\", \"race\")) |&gt;\n  left_join(select(mean_income_2023, y2023, GEOID, race), by = c(\"GEOID\", \"race\")) \n\n\n\ndatatable(total_mean_incomes)\n\n\n\n\n\nlong version:\n\ntotal_mean_incomes_long &lt;- total_mean_incomes |&gt;\n  pivot_longer(cols = starts_with(\"y\"),\n               names_to = \"year\",\n               values_to = \"mean_income\") |&gt;\n  mutate(year = as.numeric(gsub(\"y\", \"\", year))) \n\n\ndatatable(total_mean_incomes_long)"
  },
  {
    "objectID": "mp04.html#teachers-retirement-system-trs",
    "href": "mp04.html#teachers-retirement-system-trs",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Teachers Retirement System (TRS)",
    "text": "Teachers Retirement System (TRS)\nThe TRS plan is a traditional pension plan: after retirement, the employer continues to pay employees a fraction of their salary until death. This type of plan is called a “defined-benefit” because the retirement pay is fixed a priori and the employer takes the market risk. If the market under performs expectations, CUNY has to make up the gap; if the market over performs expectations, CUNY pockets the excess balance.\nEmployees pay a fixed percentage of their paycheck into the pension fund. The contribution rates are based on employee’s salary amount, and are as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThen, the retirement benefit is calculated based on the Final three years salary of the employee, which changed after 2024. Before 2024, it was based on the prior five years.\nIf N is the number of years served, the annual retirement benefit is:\n\n1.67% * FAS * N, if N is less than 20 years\n1.75% * FAS * N, if N is equal to 20 years\n(35% + 2% * N) * FAS, if N is greater than 20 years\n\nIn each case, the benefit is paid out equally over 12 months.\nThe benefit is increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent. For example, a CPI of 2.9% gives an inflation adjustment of 1.5%. The benefit is capped below at 1% and above at 3%, so a CPI of 10% leads to a 3% inflation adjustment while a CPI of 0% leads to a 1% inflation adjustment.\nThe inflation adjustment is effective each September and the CPI used is the aggregate monthly CPI of the previous 12 months; so the September 2024 adjustment depends on the CPI from September 2023 to August 2024."
  },
  {
    "objectID": "mp04.html#optional-retirement-plan-orp",
    "href": "mp04.html#optional-retirement-plan-orp",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Optional Retirement Plan (ORP)",
    "text": "Optional Retirement Plan (ORP)\nThe ORP plan is more similar to a 401(k) plan offered by a private employer. The employee and the employer both make contributions to a retirement account which is then invested in the employee’s choice of mutual funds. Those investments grow “tax-free” until the employee begins to withdraw them upon retirement. If the employee does not use up the funds, they are passed down to that employee’s spouse, children, or other heirs; if the employee uses the funds too quickly and zeros out the account balance, no additional retirement funds are available. This type of plan is called a defined-contribution plan as only the contributions to the retirement account are fixed by contract: the final balance depends on market factors outside of the employee’s control.\nWhen retired, an employee has access to the funds and can withdraw at any rate they wish. Usually, withdrawing 4% of the value per year is the best practice, but funds in the plan will continue to experience market returns.\nThe funds available in an ORP account depend strongly on the investments chosen. For this project, we will assume that participants invest in a Fidelity Freedom Fund with the following asset allocation:\n\nAge 25 to Age 49:\n\n54% US Equities\n36% International Equities\n10% Bonds\n\nAge 50 to Age 59:\n\n47% US Equities\n32% International Equities\n21% Bonds\n\nAge 60 to Age 74:\n\n34% US Equities\n23% International Equities\n43% Bonds\n\nAge 75 or older:\n\n19% US Equities\n13% International Equities\n62% Bonds\n6% Short-Term Debt\n\n\nBoth the employee and the employer make monthly contributions to the employee’s ORP account. These returns are calculated as a percentage of the employee’s annual salary, which is the same rate as the TRS:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer contribution is fixed at: - 8% for the first seven years of employment at CUNY - 10% for all years after"
  },
  {
    "objectID": "mp04.html#trs",
    "href": "mp04.html#trs",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "TRS:",
    "text": "TRS:\nSince 2007 to 2024 is only 17 years, we fall under the first bracket with the following equation: 1.75% * FAS * N, if N is equal to 20 years.\n\nmonth_one_TRS &lt;- (.0167 * 60000 * 17) / 12 \n\nprint(month_one_TRS)\n\n[1] 1419.5"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans\n\n\n\nIntroduction\n\nNewly hired at CUNY, I have thirty days to choose between one of two retirement plans. These two plans can be viewed here. This is an early choice for faculty and is important as it is essentially permanent and cannot be changed. In this project, I will use historical financial data and a bootstrap inference strategy to estimate the probability that one plan is better than the other.\nThe two plans are better explained below:\n\nTeachers Retirement System (TRS)\nThe TRS plan is a traditional pension plan: after retirement, the employer continues to pay employees a fraction of their salary until death. This type of plan is called a “defined-benefit” because the retirement pay is fixed a priori and the employer takes the market risk. If the market under performs expectations, CUNY has to make up the gap; if the market over performs expectations, CUNY pockets the excess balance.\nEmployees pay a fixed percentage of their paycheck into the pension fund. The contribution rates are based on employee’s salary amount, and are as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThen, the retirement benefit is calculated based on the Final three years salary of the employee, which changed after 2024. Before 2024, it was based on the prior five years.\nIf N is the number of years served, the annual retirement benefit is:\n\n1.67% * FAS * N, if N is less than 20 years\n1.75% * FAS * N, if N is equal to 20 years\n(35% + 2% * N) * FAS, if N is greater than 20 years\n\nIn each case, the benefit is paid out equally over 12 months.\nThe benefit is increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent. For example, a CPI of 2.9% gives an inflation adjustment of 1.5%. The benefit is capped below at 1% and above at 3%, so a CPI of 10% leads to a 3% inflation adjustment while a CPI of 0% leads to a 1% inflation adjustment.\nThe inflation adjustment is effective each September and the CPI used is the aggregate monthly CPI of the previous 12 months; so the September 2024 adjustment depends on the CPI from September 2023 to August 2024.\n\n\nOptional Retirement Plan (ORP)\nThe ORP plan is more similar to a 401(k) plan offered by a private employer. The employee and the employer both make contributions to a retirement account which is then invested in the employee’s choice of mutual funds. Those investments grow “tax-free” until the employee begins to withdraw them upon retirement. If the employee does not use up the funds, they are passed down to that employee’s spouse, children, or other heirs; if the employee uses the funds too quickly and zeros out the account balance, no additional retirement funds are available. This type of plan is called a defined-contribution plan as only the contributions to the retirement account are fixed by contract: the final balance depends on market factors outside of the employee’s control.\nWhen retired, an employee has access to the funds and can withdraw at any rate they wish. Usually, withdrawing 4% of the value per year is the best practice, but funds in the plan will continue to experience market returns.\nThe funds available in an ORP account depend strongly on the investments chosen. For this project, we will assume that participants invest in a Fidelity Freedom Fund with the following asset allocation:\n\nAge 25 to Age 49:\n\n54% US Equities\n36% International Equities\n10% Bonds\n\nAge 50 to Age 59:\n\n47% US Equities\n32% International Equities\n21% Bonds\n\nAge 60 to Age 74:\n\n34% US Equities\n23% International Equities\n43% Bonds\n\nAge 75 or older:\n\n19% US Equities\n13% International Equities\n62% Bonds\n6% Short-Term Debt\n\n\nBoth the employee and the employer make monthly contributions to the employee’s ORP account. These returns are calculated as a percentage of the employee’s annual salary, which is the same rate as the TRS:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer contribution is fixed at: - 8% for the first seven years of employment at CUNY - 10% for all years after\n\n\nData Acquisition\n\nFor this project, we will use data from two economic and financial data services. They are as follows:\n\nAlphaVantage: a commercial stock market data provider\nFRED: the Federal Reserve Economic Data repository maintained by the Federal Reserve Bank of St. Louis\n\nTo begin my Monte Carlo analysis, I will need historical data covering the following. For ease, linked the data I plan to use to each point.\n\nWage Growth\nInflation\nUS Equity Market Total Returns\nInternational Equity Market total returns\nBond Market Total Returns\nShort-term debt returns\n\nFirst, we must set up the pathways to these sites to get the data.\n\n\nClick here to see how to set up the pathway\n\n\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(dplyr)\nlibrary(gtExtras)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(DT)\n\nalpha_vantage_key &lt;- \"657RUGIZTAGDBDOF\"\nfred_key &lt;- \"2220adc19cf9dd1075bb3d55cadc8031\"\n\n\nget_alpha_data &lt;- function(symbol, interval = \"TIME_SERIES_DAILY\", api_key) {\n  url &lt;- paste0(\"https://www.alphavantage.co/query?function=\", interval,\n                \"&symbol=\", symbol, \"&apikey=\", api_key, \"&outputsize=full&datatype=json\")\n  \n\n  response &lt;- request(url) |&gt; req_perform()\n  if (response |&gt; resp_status() != 200) {\n    stop(\"Failed to retrieve Alpha Vantage data. HTTP Status: \", response |&gt; resp_status())\n  }\n  \n  data &lt;- fromJSON(response |&gt; resp_body_string())\n  timeseries &lt;- data[[\"Time Series (Daily)\"]]\n  if (is.null(timeseries)) stop(\"Failed to retrieve Alpha Vantage data for symbol: \", symbol)\n  \n  df &lt;- as.data.frame(do.call(rbind, timeseries))\n  df$date &lt;- rownames(df)\n  rownames(df) &lt;- NULL\n  \n  \n  df &lt;- df |&gt;\n    rename(close = `4. close`) |&gt;\n    mutate(\n      date = as.Date(date),\n      close = as.numeric(close)\n    ) |&gt;\n    arrange(date)\n  \n  df &lt;- df |&gt;\n    mutate(month = format(date, \"%Y-%m\")) |&gt;\n    group_by(month) |&gt;\n    summarize(\n      monthly_return = last(close) / first(close) - 1,\n      .groups = 'drop'\n    ) |&gt;\n    mutate(date = as.Date(paste0(month, \"-01\"))) |&gt;\n    select(date, monthly_return)\n  \n  return(df)\n}\n\nget_fred_data &lt;- function(series_id, api_key) {\n  url &lt;- paste0(\"https://api.stlouisfed.org/fred/series/observations?series_id=\", \n                series_id, \"&api_key=\", api_key, \"&file_type=json\")\n  \n\n  response &lt;- request(url) |&gt; req_perform()\n  if (response |&gt; resp_status() != 200) {\n    stop(\"Failed to retrieve FRED data. HTTP Status: \", response |&gt; resp_status())\n  }\n  \n \n  data &lt;- fromJSON(response |&gt; resp_body_string())\n  if (is.null(data$observations)) stop(\"No observations found for series: \", series_id)\n  \n\n  df &lt;- as.data.frame(data$observations) |&gt;\n    mutate(\n      date = as.Date(date),\n      value = suppressWarnings(as.numeric(value))\n    ) |&gt;\n    filter(!is.na(value)) |&gt;\n    select(date, value)\n  \n  return(df)\n}\n\n\nNow, we must retrieve the data from these sites.\n\n\nClick here to see how the data was downloaded into R\n\n\n# Wage Growth \n\nwage_growth &lt;- get_fred_data(\"SMU36935610500000003\", fred_key) |&gt;\n  mutate(month = format(date, \"%Y-%m\")) |&gt;\n  group_by(month) |&gt;\n  summarize(wage_growth = last(value), .groups = 'drop') |&gt;\n  mutate(date = as.Date(paste0(month, \"-01\"))) |&gt;\n  select(date, wage_growth) |&gt;\n  mutate(salary = wage_growth * 40 * 52)\n\n\n# Inflation  \ninflation &lt;- get_fred_data(\"CUURA101SA0\", fred_key) |&gt;\n  mutate(month = format(date, \"%Y-%m\"))|&gt;\n  group_by(month) |&gt;\n  summarize(inflation = last(value), .groups = 'drop') |&gt;\n  mutate(date = as.Date(paste0(month, \"-01\")))|&gt;\n  select(date, inflation)\n\n# U.S. Market Total Returns \nus_equity_data &lt;- get_alpha_data(\"SPY\", \"TIME_SERIES_DAILY\", alpha_vantage_key) |&gt;\n  rename(us_equity_return = monthly_return)\n\n\n# International Stock Market Total Returns\ninternational_equity_data &lt;- get_alpha_data(\"EFA\", \"TIME_SERIES_DAILY\", alpha_vantage_key) |&gt;\n  rename(international_equity_return = monthly_return)\n\n\n# 5. Bond Market Total Returns\nbond_market_return &lt;- get_fred_data(\"BAMLHYH0A3CMTRIV\", fred_key) |&gt;\n  mutate(month = format(date, \"%Y-%m\")) |&gt;\n  group_by(month) |&gt;\n  summarize(bond_return = last(value), .groups = 'drop') |&gt;\n  mutate(date = as.Date(paste0(month, \"-01\"))) |&gt;\n  select(date, bond_return)\n\n# Short-Term Debt Returns\nshort_debt_data &lt;- get_fred_data(\"DTB3\", fred_key) |&gt;\n  mutate(month = format(date, \"%Y-%m\")) |&gt;\n  group_by(month) |&gt;\n  summarize(short_term_debt_rate = last(value), .groups = 'drop') |&gt;\n  mutate(date = as.Date(paste0(month, \"-01\"))) |&gt;\n  select(date, short_term_debt_rate)\n\n# Merge All Data\ndata &lt;- list(\n  wage_growth,\n  inflation,\n  us_equity_data,\n  international_equity_data,\n  bond_market_return,\n  short_debt_data\n) |&gt;\n  reduce(full_join, by = \"date\") |&gt;\n  arrange(date) |&gt;\n  drop_na()  \n\n\n\ndatatable(data)\n\n\n\n\n\n\nInitial Analysis\n\nFirst, I want to see the difference between certain aspects of our data. Firstly, I want to see the difference between the growth in hourly wage rate and inflation. This can help me anticipate how much the value of money will continue to be in the future compared to the price of goods.\n\n\nClick here to see how to make the line graph\n\n\nwage_vs_inflation &lt;- list(\n  wage_growth,\n  inflation\n) |&gt;\n  reduce(full_join, by = \"date\") |&gt;\n  arrange(date) |&gt;\n  drop_na() |&gt;\n  select(-salary)\n\nOvertime &lt;- ggplot(wage_vs_inflation, aes (x = date)) +\n  geom_line(aes(y = wage_growth, color = \"Wage Growth Rate\")) + \n  geom_line(aes(y = inflation, color = \"Inflation Growth Rate\")) + \n  scale_color_manual(values = c(\"Wage Growth Rate\" = \"chartreuse4\", \"Inflation Growth Rate\" = \"firebrick2\")) +\n  labs(title = \"The Growth Rate Between Wage Rate and Inflation Rate Overtime, 2007-2024\", x = \"Date\", y = \"Rate\") +\n  theme_minimal()\n\n\n\nprint(Overtime)\n\n\n\n\n\n\n\n\nBelow shows the difference between the United States Stock Market Total Returns and the International Stock Market Total Returns. This can show me how well the United States’ stock market is doing compared to the rest of the world.\n\n\nClick here to see how to make the line graph\n\n\nUS_vs_international &lt;- list(\n  us_equity_data,\n  international_equity_data\n) |&gt;\n  reduce(full_join, by = \"date\") |&gt;\n  arrange(date) |&gt;\n  drop_na() \n\nOvertime2 &lt;- ggplot(US_vs_international, aes (x = date)) +\n  geom_line(aes(y = us_equity_return, color = \"US Stock Market Total Returns\"), linewidth = .5) + \n  geom_line(aes(y = international_equity_return, color = \"International Stock Market Total Returns\"), linewidth = .5) + \n  scale_color_manual(values = c(\"US Stock Market Total Returns\" = \"darkseagreen\", \"International Stock Market Total Returns\" = \"darkorchid\")) +\n  labs(title = \"US Stock Market Total Returns vs International Stock Market Total Returns, 2007-2024\", x = \"Date\", y = \"Monthly Return\") +\n  theme_minimal()\n\n\n\nprint(Overtime2)\n\n\n\n\n\n\n\n\nThe following graph shows the relationship between the Bond Market Total Returns and Short-Term Debt Returns.\n\n\nClick here to see how to make the line graph\n\n\nBondreturn_vs_shortdebt &lt;- list(\n  bond_market_return,\n  short_debt_data\n) |&gt;\n  reduce(full_join, by = \"date\") |&gt;\n  arrange(date) |&gt;\n  drop_na() \n\n\nOvertime3 &lt;- ggplot(Bondreturn_vs_shortdebt, aes (x = date)) +\n  geom_line(aes(y = bond_return, color = \"Bond Market Total Return\"), linewidth = 1) + \n  geom_line(aes(y = short_term_debt_rate, color = \"Short-Term Debt Returns\"), linewidth = 1) + \n  scale_color_manual(values = c(\"Bond Market Total Return\" = \"darkseagreen\", \"Short-Term Debt Returns\" = \"darkorchid\")) +\n  labs(title = \"Bond Market Total Return vs Short-Term Debt Returns, 2007-2024\", x = \"Date\", y = \"Monthly Returns\") +\n  theme_minimal()\n\n\n\nprint(Overtime3)\n\n\n\n\n\n\n\n\nNow, I want to know the average monthly average of each of these data points. The following can be seen below.\n\n\nClick here to see how the averages were calculated\n\n\naverages &lt;- data.frame(\n  Average = c(\"Wage_Average\", \"Inflation_Average\", \"US_Stock_Market_Return_Average\", \"International_Stock Market_Return_Average\", \"Bond_Market_Return_Average\", \"Short_Term_Debt_Return_Average\"),\n  Value = c(wage_average &lt;- mean(wage_growth$wage_growth), \n            inflation_average &lt;- mean(inflation$inflation), \n            US_Market_average &lt;- mean(us_equity_data$us_equity_return), \n            international_average &lt;- mean(international_equity_data$international_equity_return), \n            bond_average &lt;- mean(bond_market_return$bond_return), \n            short_term_debt_average &lt;- mean(short_debt_data$short_term_debt_rate))) \n\n\n\ndatatable(averages)\n\n\n\n\n\n\nHistorical Comparison\n\nTo conduct this comparison, we must make some assumptions. Firstly, we will assume that we joined CUNY in the first month of the historical data, January 1, 2007, and retired in the final month of the data, October 1, 2024. Since the average salary in New York in 2007 was $58,094.40, we will assume a starting salary of $60,000.\n\nJanuary_salary &lt;- wage_growth[format(wage_growth$date, \"%m\") == \"01\", ] |&gt;\n  mutate(difference = salary - lag(salary))\ndatatable(January_salary)\n\n\n\n\n\nHere we can see the salary increase each January 1st from 2007 to 2024. To stay about average with the wage increase, I will assume that I get a raise every 5 years. The first raise will be $5,000, the second will be $7,000, and the final one will be $12,000. This will create the following salaries, per year:\n\n2007 - 2011: $60,000\n2012 - 2016: $65,000\n2017 - 2021: $72,000\n2022 - 2024: $84,000\n\nThough these raises seem random, they are selected to put us near the New York salary average by 5-year increments.\nNow, let’s see how much money we put into the pension plan over our time as an employee.\n\nyears1_to_5 &lt;- (.045 * 60000) * 5\nyears6_to_10 &lt;- (.045 * 65000) * 5\nyears11_15 &lt;- (.045 * 72000) * 5 \nyears15_to_17 &lt;- (.0575 * 84000) * 3\n\ntotal_paid &lt;- (years1_to_5 + years6_to_10 + years11_15 + years15_to_17)\n\nprint(total_paid)\n\n[1] 58815\n\n\nThus, over our career, we would have paid $58,815 worth of pension to the state. Now, let’s see which option of retirement would be better for us to take.\n\nTRS:\nSince 2007 to 2024 is only 17 years, we fall under the first bracket with the following equation: 1.75% * FAS * N, if N is equal to 20 years.\nSince we made $84,000 for our final three years, 2022, 2023, and 2024, our final average salary would be $84,000.\n\nmonth_one_TRS &lt;- (.0167 * 84000 * 17) / 12 \n\nprint(month_one_TRS)\n\n[1] 1987.3\n\n\nThus, our first month of retirement with this plan will leave us with $1,987.30.\n\n\nORP\nFor this plan, it is recommended that you only take 4% out a year. Thus, once we get our total, we will divide it by four.\n\n\nClick here to see how the math was calculated\n\n\noverall_ORP &lt;- ((60000 * .045 + 60000 * .08) * 5) + ((65000 * .045 + 65000 * .08) * 2) + ((65000 * .045 + 60000 * .1) * 3) +  (((72000 * .045 + 72000 * .10) * 5) + ((84000 * .045 + 84000 * .1) * 2))\n\nAge43_to_49 &lt;- ((60000 * .045 + 60000 * .08) * 5) + ((65000 * .045 + 65000 * .08) * 1)\n\nUS_Equities_1 &lt;- Age43_to_49 * .54 * 1.004 \nInt_Equities_1 &lt;- Age43_to_49 * .36 * .996\nbond_1 &lt;- Age43_to_49 * .1 * 1.77\n\ntotal_1 &lt;- US_Equities_1 + Int_Equities_1 + bond_1 \n\nAge_50_to_60 &lt;- ((65000 * .045 + 65000 * .08) * 1) + ((65000 * .045 + 60000 * .1) * 3) +  (((72000 * .045 + 72000 * .10) * 5) + ((84000 * .045 + 84000 * .1) * 2))\n\nUS_Equities_2 &lt;- Age_50_to_60  * .47 * 1.004 \nInt_Equities_2 &lt;- Age_50_to_60  * .32 * .996\nbond_2 &lt;- Age_50_to_60  * .21 * 1.77\n\ntotal_2 &lt;- US_Equities_2 + Int_Equities_2 + bond_2 \n\ntotal_ORP &lt;- total_1 + total_2 \n\nmonth_one_orp &lt;- (total_ORP * .04)/12\n\n\n\nprint(month_one_orp)\n\n[1] 595.7364\n\n\nTo find the total amount, I used the average of each return to calculate the total built up overtime. Thus, the first monthly takeaway will be $595.74.\n\n\nLong-Term Average Analysis\n\nWe will assume that we began working at 43, retired at 60, and pass away at 90 years old. That is 30 years of retirement.\n\nTRS:\nLet’s see how much money we will get before we pass, if we were to use the TRS method.\n\ntotal_pay_out &lt;- (.0167 * 84000 * 17) * 30\n\nprint(total_pay_out)\n\n[1] 715428\n\n\nThus, the total life-time pay out will be $715,428.\n\n\nORP:\nLet’s see how much money we will get before we pass, if we were to use the ORP method.\n\n\nClick here to see how the math was calculated\n\n\nV1 &lt;- ((178720.9 - (178720.9 * .04)) * 1.07) \nV2 &lt;- ((V1 - (V1 * .04)) * 1.07) \nV3 &lt;- ((V2 - (V2 * .04)) * 1.07) \nV4 &lt;- ((V3 - (V3 * .04)) * 1.07) \nV5 &lt;- ((V4 - (V4 * .04)) * 1.07) \nV6 &lt;- ((V5 - (V5 * .04)) * 1.07) \nV7 &lt;- ((V6 - (V6 * .04)) * 1.07) \nV8 &lt;- ((V7 - (V7 * .04)) * 1.07) \nV9 &lt;- ((V8 - (V8 * .04)) * 1.07) \nV10 &lt;- ((V9 - (V9 * .04)) * 1.07) \nV11 &lt;- ((V10 - (V10 * .04)) * 1.07) \nV12 &lt;- ((V11 - (V11 * .04)) * 1.07) \nV13 &lt;- ((V12 - (V12 * .04)) * 1.07) \nV14 &lt;- ((V13 - (V13 * .04)) * 1.07) \nV15 &lt;- ((V14 - (V14 * .04)) * 1.07) \nV16 &lt;- ((V15 - (V15 * .04)) * 1.07) \nV17 &lt;- ((V16 - (V16 * .04)) * 1.07) \nV18 &lt;- ((V17 - (V17 * .04)) * 1.07) \nV19 &lt;- ((V18 - (V18 * .04)) * 1.07) \nV20 &lt;- ((V19 - (V19 * .04)) * 1.07) \nV21 &lt;- ((V20 - (V20 * .04)) * 1.07) \nV22 &lt;- ((V21 - (V21 * .04)) * 1.07) \nV23 &lt;- ((V22 - (V22 * .04)) * 1.07) \nV24 &lt;- ((V23 - (V23 * .04)) * 1.07) \nV25 &lt;- ((V24 - (V24 * .04)) * 1.07) \nV26 &lt;- ((V25 - (V25 * .04)) * 1.07) \nV27 &lt;- ((V26 - (V26 * .04)) * 1.07) \nV28 &lt;- ((V27 - (V27 * .04)) * 1.07) \nV29 &lt;- ((V28 - (V28 * .04)) * 1.07) \nV30 &lt;- ((V29 - (V29 * .04)) * 1.07)\n\n\norp_calculation &lt;- data.frame(\n  Retirement_Year = c(1:30),\n  Value = c(V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, V29, V30))\n\n\n\ndatatable(orp_calculation)\n\n\n\n\n\nIn this ORP calculation, I do not run out of funds. Since I have an increase about 7% a year due to a good stock market, my money only goes down about $30,000. In this calculation, the monthly average ranges from $500 to $1000, which is much less than the TRS plan. This is due to the differing stock market returns and the only 4% per year take home.\nThus, after passing away, you will still have $147,408.60 to give to your family.\n\n\nMonte Carlo Analysis\n\nBelow is the Monte Carlo Analysis.\n\n\nClick here to see how the Monte Carlo was calculated\n\n\nmonte_carlo &lt;- function(num_simulations, \n                        starting_balance, \n                        monthly_trs_pension, \n                        fixed_withdrawal_rate, \n                        inflation_rate, \n                        market_return_data) {\n  results &lt;- tibble(simulation_id = numeric(), \n                    month = numeric(), \n                    trs_income = numeric(),\n                    orp_income = numeric(), \n                    orp_balance = numeric())\n  \n  for (sim in 1:num_simulations) {\n    set.seed(sim)\n    sampled_returns &lt;- tibble(\n      us_equity_return = sample(market_return_data$us_equity_return, \n                                size = nrow(market_return_data), \n                                replace = TRUE))\n    trs_income_stream &lt;- simulate_trs(\n      monthly_pension = monthly_trs_pension,\n      retirement_years = retirement_years,\n      inflation_rate = inflation_rate)\n    orp_simulation &lt;- simulate_orp(\n      account_balance = starting_balance,\n      retirement_years = retirement_years,\n      fixed_withdrawal_rate = fixed_withdrawal_rate,\n      market_return_rate = sampled_returns$us_equity_return)\n    \n    orp_income_stream &lt;- orp_simulation$withdrawal\n    orp_balance_stream &lt;- orp_simulation$balance\n    \n    results &lt;- results |&gt;\n      bind_rows(tibble(simulation_id = sim,\n                       month = 1:(retirement_years * 12),\n                       trs_income = trs_income_stream,\n                       orp_income = orp_income_stream,\n                       orp_balance = orp_balance_stream))}\n  return(results)}\n\nsimulate_trs &lt;- function(monthly_pension, \n                         retirement_years, \n                         inflation_rate) {\n  pension &lt;- numeric(retirement_years * 12)\n  for (i in 1:length(pension)) {\n    inflation_adjustment &lt;- ifelse(i %% 12 == 1, \n                                   (1 + inflation_rate) - 1, 0)\n    if (i == 1) {pension[i] &lt;- monthly_pension} \n    else {pension[i] &lt;- pension[i - 1] * (1 + inflation_adjustment)}}\n  return(pension)}\n\nsimulate_orp &lt;- function(account_balance, \n                         retirement_years, \n                         fixed_withdrawal_rate, \n                         market_return_rate) {\n  withdrawal &lt;- numeric(retirement_years * 12)\n  balance &lt;- numeric(retirement_years * 12)\n  for (i in 1:(retirement_years * 12)) {\n    market_return &lt;- market_return_rate[i %% length(market_return_rate) + 1]\n    account_balance &lt;- account_balance * (1 + market_return)\n    withdrawal_amount &lt;- account_balance * fixed_withdrawal_rate / 12\n    withdrawal[i] &lt;- min(account_balance, withdrawal_amount)\n    account_balance &lt;- account_balance - withdrawal[i]\n    balance[i] &lt;- account_balance}\n  return(list(withdrawal = withdrawal, balance = balance))}\n\nset.seed(2024)  \nnum_simulations &lt;- 200\nstarting_balance &lt;- 179000\nmonthly_trs_pension &lt;- 1987\nfixed_withdrawal_rate &lt;- 0.04\ninflation_rate &lt;- 0.15\nretirement_years &lt;- 30\n\nmarket_return_data &lt;- tibble(\n  us_equity_return = rnorm(360, mean = 0.05 / 12, sd = 0.06))\n\nsimulation_results &lt;- monte_carlo (\n  num_simulations = num_simulations,\n  starting_balance = starting_balance,\n  monthly_trs_pension = monthly_trs_pension,\n  fixed_withdrawal_rate = fixed_withdrawal_rate,\n  inflation_rate = inflation_rate,\n  market_return_data = market_return_data)\n\n\norp_depletion_probability &lt;- simulation_results %&gt;%\n  filter(orp_balance == 0) %&gt;%\n  summarize(prob = n_distinct(simulation_id) / num_simulations) %&gt;%\n  pull(prob)\n\ncat(\"Probability of ORP funds depletion: \", round(orp_depletion_probability * 100, 2), \"%\\n\")\n\nProbability of ORP funds depletion:  0 %\n\norp_better_than_trs_probability &lt;- simulation_results %&gt;%\n  group_by(simulation_id) %&gt;%\n  summarize(orp_better = mean(orp_income &gt; trs_income)) %&gt;%\n  summarize(prob = mean(orp_better &gt; 0.5)) %&gt;%\n  pull(prob) \n\ncat(\"Probability ORP income exceeds TRS income: \", round(orp_better_than_trs_probability * 100, 2), \"%\\n\")\n\nProbability ORP income exceeds TRS income:  0 %\n\n\n\nAfter completing this analysis, we can see that the probability of the ORP funds depleting before death is 0%. This matches up with our analysis from earlier, as we saw that by age 90, we still had about $140,000 left in our account.\nWe also can see that the probability of an ORP employee having a higher monthly income than a TRS employee is only 2.5%, which is quite low.\nI would recommend more than a 4% withdrawal rate. Sticking with the 4%, there will be a lot of money left in your account after death and you will only be living on about $500 - $900 a month, which is unrealistic, especially in relation to the inflation rate increase.\n\nData-Driven Decision Recommendation\n\nBased on the Data, I believe that it is better to go with the TRS option for retirement. Overall, you will get more bang for your buck than the ORP, and can inevitably save money for your family after you pass. In this time, you get almost $550,00 more dollars by just taking the fixed monthly amount. However, if you do choose to go with the ORP plan, there is very little risk, and you have the ability to make a lot of money if the stock market has major successes. Both options are viable.\nAlso, I did my analysis for an employee of less than 20 years. I believe if you compute this for an employee that worked longer, the results might differ. Nevertheless, I still believe that the TRS option is better, as it is the more stable one of the two."
  }
]